Introduction
Hello my name is Andrej and i've been training deep neural networks for a bit more than a decade and in this lecture i'd like to show you 
what neural network training looks like under the hood. So in particular, we are going to start with a blank jupyter
notebook and by the end of this lecture we will define and train in neural net and you'll get to see everything that
goes on under the hood and exactly sort of how that works on an intuitive level now specifically what i would like to do
is i would like to take you through building of micrograd now micrograd is.

Micrograd Overview
this library that i released on github about two years ago but at the time i only uploaded the source code and you'd
have to go in by yourself and really figure out how it works so in this lecture i will take you
through it step by step and kind of comment on all the pieces of it so what is micrograd and why is it interesting
good um micrograd is basically an autograd engine autograd is short for automatic
gradient and really what it does is it implements backpropagation now backpropagation is this algorithm that
allows you to efficiently evaluate the gradient of some kind of a loss function with
respect to the weights of a neural network and what that allows us to do then is we can iteratively tune the
weights of that neural network to minimize the loss function and therefore improve the accuracy of the network so
back propagation would be at the mathematical core of any modern deep neural network library like say pytorch
or jaxx so the functionality of microgrant is i think best illustrated by an example so
if we just scroll down here you'll see that micrograph basically allows you to build out mathematical
expressions and um here what we are doing is we have an expression that we're building out where you have two inputs a and b
and you'll see that a and b are negative four and two but we are wrapping those
values into this value object that we are going to build out as part of micrograd so this value object will wrap the
numbers themselves and then we are going to build out a mathematical expression here where a and b are transformed into c d and
eventually e f and g and i'm showing some of the functions some of the functionality of micrograph
and the operations that it supports so you can add two value objects you can multiply them you can raise them to a
constant power you can offset by one negate squash at zero
square divide by constant divide by it etc and so we're building out an expression
graph with with these two inputs a and b and we're creating an output value of g
and micrograd will in the background build out this entire mathematical expression so it will for example know
that c is also a value c was a result of an addition operation and the
child nodes of c are a and b because the and will maintain pointers to a and b
value objects so we'll basically know exactly how all of this is laid out and then not only can we do what we call
the forward pass where we actually look at the value of g of course that's pretty straightforward we will access
that using the dot data attribute and so the output of the forward pass the value
of g is 24.7 it turns out but the big deal is that we can also take this g
value object and we can call that backward and this will basically uh initialize back propagation at the node g
and what backpropagation is going to do is it's going to start at g and it's going to go backwards through that
expression graph and it's going to recursively apply the chain rule from calculus and what that allows us to do then is
we're going to evaluate basically the derivative of g with respect to all the internal nodes
like e d and c but also with respect to the inputs a and b
and then we can actually query this derivative of g with respect to a for example that's a dot grad in this case
it happens to be 138 and the derivative of g with respect to b which also happens to be here 645
and this derivative we'll see soon is very important information because it's telling us how a and b are affecting g
through this mathematical expression so in particular a dot grad is 138 so if we slightly
nudge a and make it slightly larger 138 is telling us that g will grow and
the slope of that growth is going to be 138 and the slope of growth of b is going to be 645. so that's going to tell us about
how g will respond if a and b get tweaked a tiny amount in a positive direction
okay now you might be confused about what this expression is that we built out
here and this expression by the way is completely meaningless i just made it up i'm just flexing about the kinds of
operations that are supported by micrograd what we actually really care about are neural networks but it turns out that
neural networks are just mathematical expressions just like this one but actually slightly bit less crazy even
neural networks are just a mathematical expression they take the input data as an input and they take the weights of a
neural network as an input and it's a mathematical expression and the output are your predictions of your neural net
or the loss function we'll see this in a bit but basically neural networks just happen to be a certain class of
mathematical expressions but back propagation is actually significantly more general it doesn't
actually care about neural networks at all it only tells us about arbitrary mathematical expressions and then we
happen to use that machinery for training of neural networks now one more note i would like to make at this stage
is that as you see here micrograd is a scalar valued auto grant engine so it's working on the you know level of
individual scalars like negative four and two and we're taking neural nets and we're breaking them down all the way to
these atoms of individual scalars and all the little pluses and times and it's just excessive and so obviously you
would never be doing any of this in production it's really just put down for pedagogical reasons because it allows us
to not have to deal with these n-dimensional tensors that you would use in modern deep neural network library so
this is really done so that you understand and refactor out back propagation and chain rule and
understanding of neurologic training and then if you actually want to train bigger networks you have to be using
these tensors but none of the math changes this is done purely for efficiency we are basically taking scale
value all the scale values we're packaging them up into tensors which are just arrays of these scalars and then because
we have these large arrays we're making operations on those large arrays that allows us to take advantage of the
parallelism in a computer and all those operations can be done in parallel and then the whole thing runs faster but
really none of the math changes and that's done purely for efficiency so i don't think that it's pedagogically useful to be dealing with tensors from
scratch uh and i think and that's why i fundamentally wrote micrograd because you can understand how things work uh at
the fundamental level and then you can speed it up later okay so here's the fun part my claim is that micrograd is what
you need to train your networks and everything else is just efficiency so you'd think that micrograd would be a
very complex piece of code and that turns out to not be the case so if we just go to micrograd
and you'll see that there's only two files here in micrograd this is the actual engine it doesn't know anything
about neural nuts and this is the entire neural nets library on top of micrograd so engine and nn.pi
so the actual backpropagation autograd engine that gives you the power of neural
networks is literally 100 lines of code of like very simple
python which we'll understand by the end of this lecture and then nn.pi
this neural network library built on top of the autograd engine um is like a joke it's like
we have to define what is a neuron and then we have to define what is the layer of neurons and then we define what is a
multi-layer perceptron which is just a sequence of layers of neurons and so it's just a total joke
so basically there's a lot of power that comes from only 150 lines of code
and that's all you need to understand to understand neural network training and everything else is just efficiency and
of course there's a lot to efficiency but fundamentally that's all that's happening okay so now let's dive right
derivative of a simple function with one input
in and implement micrograph step by step the first thing i'd like to do is i'd like to make sure that you have a very good understanding intuitively of what a
derivative is and exactly what information it gives you so let's start with some basic imports that i copy
paste in every Jupyter notebook always and let's define a function a scalar
valued function f of x as follows so i just make this up randomly i just
want to scale a valid function that takes a single scalar x and returns a single scalar y
and we can call this function of course so we can pass in say 3.0 and get 20 back
now we can also plot this function to get a sense of its shape you can tell from the mathematical expression that this is probably a parabola it's a
quadratic and so if we just uh create a set of um
um scale values that we can feed in using for example a range from negative five to five in steps of 0.25
so this is so axis is just from negative 5 to 5 not including 5 in steps of 0.25
and we can actually call this function on this numpy array as well so we get a set of y's if we call f on axis
and these y's are basically also applying a function on every one of
these elements independently and we can plot this using matplotlib so plt.plot x's and y's and we get a nice
parabola so previously here we fed in 3.0 somewhere here and we received 20
back which is here the y coordinate so now i'd like to think through what is the derivative
of this function at any single input point x right so what is the derivative at different points x of this function now
if you remember back to your calculus class you've probably derived derivatives so we take this mathematical expression 3x squared minus 4x plus 5
and you would write out on a piece of paper and you would you know apply the product rule and all the other rules and derive the mathematical expression of
the great derivative of the original function and then you could plug in different texts and see what the derivative is
we're not going to actually do that because no one in neural networks actually writes out the expression for
the neural net it would be a massive expression um it would be you know thousands tens of thousands of terms no
one actually derives the derivative of course and so we're not going to take this kind of like a symbolic approach
instead what i'd like to do is i'd like to look at the definition of derivative and just make sure that we really understand what derivative is measuring
what it's telling you about the function and so if we just look up derivative
we see that okay so this is not a very good definition of derivative this is a definition of what it means to be differentiable
but if you remember from your calculus it is the limit as h goes to zero of f of x plus h minus f of x over h so
basically what it's saying is if you slightly bump up you're at some point x
that you're interested in or a and if you slightly bump up you know you slightly increase it by
small number h how does the function respond with what sensitivity does it respond what is the
slope at that point does the function go up or does it go down and by how much and that's the slope of that function
the the slope of that response at that point and so we can basically evaluate
the derivative here numerically by taking a very small h of course the definition would ask us to take h to
zero we're just going to pick a very small h 0.001 and let's say we're interested in point 3.0 so we can look at f of x of course
as 20 and now f of x plus h so if we slightly nudge x in a positive
direction how is the function going to respond and just looking at this do you expect do you expect f of x plus h to be
slightly greater than 20 or do you expect to be slightly lower than 20
and since this 3 is here and this is 20 if we slightly go positively the function will respond positively so
you'd expect this to be slightly greater than 20. and now by how much it's telling you the
sort of the the strength of that slope right the the size of the slope so f of x plus h minus
f of x this is how much the function responded in the positive direction and we have to
normalize by the run so we have the rise over run to get the slope so this of course is just a
numerical approximation of the slope because we have to make age very very small to converge to the exact amount
now if i'm doing too many zeros at some point i'm gonna get an incorrect answer
because we're using floating point arithmetic and the representations of all these numbers in computer memory is
finite and at some point we get into trouble so we can converse towards the right answer with this approach
but basically um at 3 the slope is 14. and you can see that by taking 3x
squared minus 4x plus 5 and differentiating it in our head so 3x squared would be
6 x minus 4 and then we plug in x equals 3 so that's 18 minus 4 is 14. so this is correct
so that's at 3. now how about the slope at say negative 3
would you expect would you expect for the slope now telling the exact value is really hard but what is the sign of that slope
so at negative three if we slightly go in the positive direction at x the function would
actually go down and so that tells you that the slope would be negative so we'll get a slight number below
below 20. and so if we take the slope we expect something negative negative 22. okay
and at some point here of course the slope would be zero now for this specific function i looked it up
previously and it's at point two over three so at roughly two over three
uh that's somewhere here um this derivative be zero so basically at that precise point
yeah at that precise point if we nudge in a positive direction the function doesn't respond this stays the same almost and
so that's why the slope is zero okay now let's look at a bit more complex case so we're going to start you know
derivative of a function with multiple inputs
complexifying a bit so now we have a function here
with output variable d that is a function of three scalar inputs a b and c
so a b and c are some specific values three inputs into our expression graph and a single output d
and so if we just print d we get four and now what i have to do is i'd like to
again look at the derivatives of d with respect to a b and c and uh think through uh again just the
intuition of what this derivative is telling us so in order to evaluate this derivative we're going to get a bit hacky here
we're going to again have a very small value of h and then we're going to fix the inputs
at some values that we're interested in so these are the this is the point abc
at which we're going to be evaluating the the derivative of d with respect to all a b and c at that point
so there are the inputs and now we have d1 is that expression and then we're going to for example look
at the derivative of d with respect to a so we'll take a and we'll bump it by h and then we'll get d2 to be the exact
same function and now we're going to print um you know f1
d1 is d1 d2 is d2 and print slope
so the derivative or slope here will be um of course
d2 minus d1 divide h so d2 minus d1 is how much the function
increased uh when we bumped the uh the specific input that we're interested
in by a tiny amount and this is then normalized by h
to get the slope so um
yeah so this so if i just run this we're going to print
d1 which we know is four
now d2 will be bumped a will be bumped by h so let's just think through
a little bit uh what d2 will be uh printed out here
in particular d1 will be four will d2 be a number slightly greater
than four or slightly lower than four and that's going to tell us the sl the the sign of the derivative
so we're bumping a by h
b as minus three c is ten so you can just intuitively think through this derivative and what it's
doing a will be slightly more positive and but b is a negative number
so if a is slightly more positive because b is negative three
we're actually going to be adding less to d so you'd actually expect that the value
of the function will go down so let's just see this
yeah and so we went from 4 to 3.9996 and that tells you that the slope will
be negative and then uh will be a negative number because we went down
and then the exact number of slope will be exact amount of slope is negative 3.
and you can also convince yourself that negative 3 is the right answer mathematically and analytically because
if you have a times b plus c and you are you know you have calculus then differentiating a times b plus c with
respect to a gives you just b and indeed the value of b is negative 3 which is the derivative that we have so
you can tell that that's correct so now if we do this with b so if we bump b by a little bit in a
positive direction we'd get different slopes so what is the influence of b on the output d
so if we bump b by a tiny amount in a positive direction then because a is positive
we'll be adding more to d right so um and now what is the what is the
sensitivity what is the slope of that addition and it might not surprise you that this should be
2 and y is a 2 because d of d by db differentiating with respect to b
would be would give us a and the value of a is two so that's also working well
and then if c gets bumped a tiny amount in h by h then of course a times b is unaffected
and now c becomes slightly bit higher what does that do to the function it makes it slightly bit higher because
we're simply adding c and it makes it slightly bit higher by the exact same amount that we added to c
and so that tells you that the slope is one that will be the
the rate at which d will increase as we scale c
okay so we now have some intuitive sense of what this derivative is telling you about the function and we'd like to move to neural networks now as i mentioned
starting the core Value object of micrograd and its visualization
neural networks will be pretty massive expressions mathematical expressions so we need some data structures that maintain these expressions and that's
what we're going to start to build out now so we're going to build out this value object that i
showed you in the readme page of micrograd so let me copy paste a skeleton of the
first very simple value object so class value takes a single
scalar value that it wraps and keeps track of and that's it so
we can for example do value of 2.0 and then we can get we can look at its content and
python will internally use the wrapper function to uh return
uh this string oops like that so this is a value object with data
equals two that we're creating here now we'd like to do is like we'd like to be able to
have not just like two values but we'd like to do a bluffy right we'd like to add them
so currently you would get an error because python doesn't know how to add two value objects so we have to tell it
so here's addition so you have to basically use these
special double underscore methods in python to define these operators for these objects so if we call um
the uh if we use this plus operator python will internally call a dot add of
b that's what will happen internally and so b will be the other and
self will be a and so we see that what we're going to return is a new value object and it's
just it's going to be wrapping the plus of their data
but remember now because data is the actual like numbered python number so this operator here is just the typical
floating point plus addition now it's not an addition of value objects and will return a new value so now a
plus b should work and it should print value of negative one because that's two plus minus three
there we go okay let's now implement multiply just so we can recreate this expression
here so multiply i think it won't surprise you will be fairly similar
so instead of add we're going to be using mul and then here of course we want to do times
and so now we can create a c value object which will be 10.0 and now we should be able to do a times b well
let's just do a times b first um [Music] that's value of negative six now
and by the way i skipped over this a little bit suppose that i didn't have the wrapper function here then it's just that you'll get some kind
of an ugly expression so what wrapper is doing is it's providing us a way to print out like a nicer looking
expression in python uh so we don't just have something cryptic we actually are you know it's
value of negative six so this gives us a times and then this we should now be able to
add c to it because we've defined and told the python how to do mul and add and so this will call this will
basically be equivalent to a dot small of b
and then this new value object will be dot add of c and so let's see if that worked
yep so that worked well that gave us four which is what we expect from before and i believe we can just call them
manually as well there we go so yeah okay so now what we are missing is the
connective tissue of this expression as i mentioned we want to keep these expression graphs so we need to know and
keep pointers about what values produce what other values so here for example we are going to
introduce a new variable which we'll call children and by default it will be an empty tuple and then we're actually going to keep a
slightly different variable in the class which we'll call underscore prev which will be the set of children
this is how i done i did it in the original micrograd looking at my code here i can't remember exactly the reason
i believe it was efficiency but this underscore children will be a tuple for convenience but then when we actually
maintain it in the class it will be just this set yeah i believe for efficiency
um so now when we are creating a value like this with a constructor children will be
empty and prep will be the empty set but when we're creating a value through addition or multiplication we're going
to feed in the children of this value which in this case is self and other
so those are the children here so now we can do d dot prev
and we'll see that the children of the we now know are this value of negative 6
and value of 10 and this of course is the value resulting from a times b and the c value which is 10.
now the last piece of information we don't know so we know that the children of every single value but we don't know
what operation created this value so we need one more element here let's call it underscore pop
and by default this is the empty set for leaves and then we'll just maintain it here
and now the operation will be just a simple string and in the case of addition it's plus in the case of
multiplication is times so now we not just have d dot pref we also have a
d dot up and we know that d was produced by an addition of those two values and so now
we have the full mathematical expression uh and we're building out this data structure and we know exactly how each value came to be
by word expression and from what other values now because these expressions are about
to get quite a bit larger we'd like a way to nicely visualize these expressions that we're building out so
for that i'm going to copy paste a bunch of slightly scary code that's going to visualize this these expression graphs
for us so here's the code and i'll explain it in a bit but first let me just show you what this code does
basically what it does is it creates a new function drawdot that we can call on some root node
and then it's going to visualize it so if we call drawdot on d which is this final value here that is a
times b plus c it creates something like this so this is d
and you see that this is a times b creating an integrated value plus c gives us this output node d
so that's dried out of d and i'm not going to go through this in complete detail you can take a look at
graphless and its api uh graphis is a open source graph visualization software
and what we're doing here is we're building out this graph and graphis api and
you can basically see that trace is this helper function that enumerates all of the nodes and edges in the graph
so that just builds a set of all the nodes and edges and then we iterate for all the nodes and we create special node
objects for them in using dot node
and then we also create edges using dot dot edge and the only thing that's like slightly tricky here is you'll notice that i
basically add these fake nodes which are these operation nodes so for example this node here is just like a plus node
and i create these special op nodes here
and i connect them accordingly so these nodes of course are not actual
nodes in the original graph they're not actually a value object the only value objects here are the things
in squares those are actual value objects or representations thereof and these op nodes are just created in this
drawdot routine so that it looks nice let's also add labels to these graphs
just so we know what variables are where so let's create a special underscore label
um or let's just do label equals empty by default and save it in
each node and then here we're going to do label as a
label is the label a c
and then let's create a special um e equals a times b
and e dot label will be e it's kind of naughty and e will be e plus c
and a d dot label will be d okay so nothing really changes i just
added this new e function a new e variable and then here when we are
printing this i'm going to print the label here so this will be a percent s
bar and this will be end.label
and so now we have the label on the left here so it says a b creating e and then e plus c
creates d just like we have it here and finally let's make this expression just one layer deeper
so d will not be the final output node instead after d we are going to create a
new value object called f we're going to start running out of variables soon f will be negative
2.0 and its label will of course just be f and then l capital l will be the output
of our graph and l will be p times f okay so l will be negative eight is the
output so now we don't just draw a d we draw l
okay and somehow the label of l was undefined oops all that label has
to be explicitly sort of given to it there we go so l is the output
so let's quickly recap what we've done so far we are able to build out mathematical expressions using only plus and times so
far they are scalar valued along the way and we can do this forward pass
and build out a mathematical expression so we have multiple inputs here a b c and f
going into a mathematical expression that produces a single output l and this here is visualizing the forward
pass so the output of the forward pass is negative eight that's the value
now what we'd like to do next is we'd like to run back propagation and in back propagation we are going to
start here at the end and we're going to reverse and calculate the gradient along along
all these intermediate values and really what we're computing for every single value here
um we're going to compute the derivative of that node with respect to l
so the derivative of l with respect to l is just uh one
and then we're going to derive what is the derivative of l with respect to f with respect to d with respect to c with
respect to e with respect to b and with respect to a and in the neural network setting you'd
be very interested in the derivative of basically this loss function l with respect to the weights of a neural
network and here of course we have just these variables a b c and f but some of these will eventually
represent the weights of a neural net and so we'll need to know how those weights are impacting
the loss function so we'll be interested basically in the derivative of the output with respect to some of its leaf
nodes and those leaf nodes will be the weights of the neural net and the other leaf nodes of course will be the data itself but usually we will
not want or use the derivative of the loss function with respect to data because the data is fixed but the
weights will be iterated on using the gradient information so next we are going to create a variable inside
the value class that maintains the derivative of l with respect to that
value and we will call this variable grad so there's a data and there's a
self.grad and initially it will be zero and remember that zero is basically means no
effect so at initialization we're assuming that every value does not impact does not affect the out the
output right because if the gradient is zero that means that changing this variable is not changing the loss function
so by default we assume that the gradient is zero and then now that we have grad and it's 0.0
we are going to be able to visualize it here after data so here grad is 0.4 f
and this will be in that graph and now we are going to be showing both the data and the grad
initialized at zero and we are just about getting ready to calculate the back propagation
and of course this grad again as i mentioned is representing the derivative of the output in this case l with respect to this value so
with respect to so this is the derivative of l with respect to f with respect to d and so on so let's now fill
manual backpropagation example #1: simple expression
in those gradients and actually do back propagation manually so let's start filling in these gradients and start all
the way at the end as i mentioned here first we are interested to fill in this gradient here so what is the derivative
of l with respect to l in other words if i change l by a tiny amount of h
how much does l change it changes by h so it's proportional and
therefore derivative will be one we can of course measure these or estimate these numerical gradients
numerically just like we've seen before so if i take this expression and i create a def lol function here
and put this here now the reason i'm creating a gating function hello here is because i don't want to pollute or mess
up the global scope here this is just kind of like a little staging area and as you know in python all of these will
be local variables to this function so i'm not changing any of the global scope here
so here l1 will be l and then copy pasting this expression we're going to add a small amount h in for example a
right and this would be measuring the derivative of l with respect to a so here this will be l2
and then we want to print this derivative so print l2 minus l1 which is how much l changed
and then normalize it by h so this is the rise over run and we have to be careful because l is a
value node so we actually want its data um so that these are floats dividing by h
and this should print the derivative of l with respect to a because a is the one that we bumped a little bit by h
so what is the derivative of l with respect to a it's six okay and obviously if we change l by h
then that would be here effectively this looks really awkward but changing l by h you see the derivative here is 1. um
that's kind of like the base case of what we are doing here so basically we cannot come up here and
we can manually set l.grad to one this is our manual back propagation
l dot grad is one and let's redraw and we'll see that we filled in grad as
1 for l we're now going to continue the back propagation so let's here look at the derivatives of l with respect to d and f
let's do a d first so what we are interested in if i create a markdown on here is we'd like to know
basically we have that l is d times f and we'd like to know what is uh d
l by d d what is that and if you know your calculus uh l is d
times f so what is d l by d d it would be f and if you don't believe me we can also
just derive it because the proof would be fairly straightforward uh we go to the
definition of the derivative which is f of x plus h minus f of x divide h
as a limit limit of h goes to zero of this kind of expression so when we have l is d times f
then increasing d by h would give us the output of b plus h
times f that's basically f of x plus h right
minus d times f and then divide h and symbolically
expanding out here we would have basically d times f plus h times f minus
t times f divide h and then you see how the df minus df cancels so you're left with h times f
divide h which is f so in the limit as h goes to zero of
you know derivative definition we just get f in the case of
d times f so symmetrically dl by d
f will just be d so what we have is that f dot grad
we see now is just the value of d which is 4.
and we see that d dot grad is just uh the value of f
and so the value of f is negative two so we'll set those manually
let me erase this markdown node and then let's redraw what we have
okay and let's just make sure that these were correct so we seem to think that dl by
dd is negative two so let's double check um let me erase this plus h from before
and now we want the derivative with respect to f so let's just come here when i create f and let's do a plus h here and this
should print the derivative of l with respect to f so we expect to see four
yeah and this is four up to floating point funkiness and then dl by dd
should be f which is negative two grad is negative two
so if we again come here and we change d
d dot data plus equals h right here so we expect so we've added a little h
and then we see how l changed and we expect to print uh negative two
there we go so we've numerically verified what we're doing here is what kind of like an
inline gradient check gradient check is when we are deriving this like back propagation
and getting the derivative with respect to all the intermediate results and then numerical gradient is just you know
estimating it using small step size now we're getting to the crux of backpropagation so this will be the most
important node to understand because if you understand the gradient for this node you understand all of back
propagation and all of training of neural nets basically so we need to derive dl by bc
in other words the derivative of l with respect to c because we've computed all these other gradients already
now we're coming here and we're continuing the back propagation manually so we want dl by dc and then we'll also
derive dl by de now here's the problem how do we derive dl
by dc we actually know the derivative l with respect to d so we know how l assessed
it to d but how is l sensitive to c so if we wiggle c how does that impact l
through d so we know dl by dc
and we also here know how c impacts d and so just very intuitively if you know the impact that c is having on d and the
impact that d is having on l then you should be able to somehow put that information together to figure out
how c impacts l and indeed this is what we can actually do so in particular we know just
concentrating on d first let's look at how what is the derivative basically of d with respect to c so in other words
what is dd by dc so here we know that d is c times c plus
e that's what we know and now we're interested in dd by dc if you just know your calculus again and
you remember that differentiating c plus e with respect to c you know that that gives you
1.0 and we can also go back to the basics and derive this because again we can go
to our f of x plus h minus f of x divide by h that's the definition of a derivative as
h goes to zero and so here focusing on c and its effect on d
we can basically do the f of x plus h will be c is incremented by h plus e
that's the first evaluation of our function minus c plus e
and then divide h and so what is this uh just expanding this out this will be
c plus h plus e minus c minus e divide h and then you see here how c
minus c cancels e minus e cancels we're left with h over h which is 1.0
and so by symmetry also d d by d
e will be 1.0 as well so basically the derivative of a sum
expression is very simple and and this is the local derivative so i call this the local derivative because we have the
final output value all the way at the end of this graph and we're now like a small node here and this is a little plus node
and it the little plus node doesn't know anything about the rest of the graph that it's embedded in all it knows is
that it did a plus it took a c and an e added them and created d and this plus note also knows the local
influence of c on d or rather rather the derivative of d with respect to c and it
also knows the derivative of d with respect to e but that's not what we want that's just a local derivative what we actually
want is d l by d c and l could l is here just one step away but in a general case
this little plus note is could be embedded in like a massive graph so
again we know how l impacts d and now we know how c and e impact d how do we put
that information together to write dl by dc and the answer of course is the chain rule in calculus
and so um i pulled up a chain rule here from kapedia
and i'm going to go through this very briefly so chain rule wikipedia sometimes can be very
confusing and calculus can can be very confusing like this is the way i
learned chain rule and it was very confusing like what is happening it's just complicated so i like this expression
much better if a variable z depends on a variable y which itself depends on the variable x
then z depends on x as well obviously through the intermediate variable y in this case the chain rule is expressed
as if you want dz by dx then you take the dz by dy and you
multiply it by d y by dx so the chain rule fundamentally is telling you
how we chain these uh derivatives together
correctly so to differentiate through a function composition we have to apply a multiplication
of those derivatives so that's really what chain rule is telling us
and there's a nice little intuitive explanation here which i also think is kind of cute the chain rule says that
knowing the instantaneous rate of change of z with respect to y and y relative to x allows one to calculate the instantaneous rate of change of z
relative to x as a product of those two rates of change simply the product of those two
so here's a good one if a car travels twice as fast as bicycle and the bicycle is four times as
fast as walking man then the car travels two times four eight times as fast as demand
and so this makes it very clear that the correct thing to do sort of is to multiply
so cars twice as fast as bicycle and bicycle is four times as fast as man
so the car will be eight times as fast as the man and so we can take these
intermediate rates of change if you will and multiply them together and that justifies the
chain rule intuitively so have a look at chain rule about here really what it means for us is there's a very simple
recipe for deriving what we want which is dl by dc and what we have so far
is we know want and we know
what is the impact of d on l so we know d l by
d d the derivative of l with respect to d d we know that that's negative two and now because of this local
reasoning that we've done here we know dd by d c
so how does c impact d and in particular this is a plus node so the
local derivative is simply 1.0 it's very simple and so the chain rule tells us that dl by dc
going through this intermediate variable will just be simply d l by
d times
dd by dc that's chain rule so this is identical to what's happening
here except z is rl y is our d and x is rc
so we literally just have to multiply these and because
these local derivatives like dd by dc are just one we basically just copy over dl by dd
because this is just times one so what does it do so because dl by dd is negative two what is dl by dc
well it's the local gradient 1.0 times dl by dd which is negative two
so literally what a plus node does you can look at it that way is it literally just routes the gradient
because the plus nodes local derivatives are just one and so in the chain rule one times
dl by dd is um is uh is just dl by dd and so that
derivative just gets routed to both c and to e in this case
so basically um we have that that grad or let's start with c since that's the
one we looked at is negative two times one
negative two and in the same way by symmetry e that grad will be negative two that's the
claim so we can set those we can redraw
and you see how we just assign negative to negative two so this backpropagating signal which is carrying the information
of like what is the derivative of l with respect to all the intermediate nodes we can imagine it almost like flowing
backwards through the graph and a plus node will simply distribute the derivative to all the leaf nodes sorry
to all the children nodes of it so this is the claim and now let's verify it so let me remove the plus h
here from before and now instead what we're going to do is we're going to increment c so c dot
data will be credited by h and when i run this we expect to see negative 2
negative 2. and then of course for e so e dot data plus equals h and we
expect to see negative 2. simple
so those are the derivatives of these internal nodes and now we're going to recurse our way
backwards again and we're again going to apply the chain rule so here we go our second
application of chain rule and we will apply it all the way through the graph we just happen to only have one more node remaining
we have that d l by d e as we have just calculated is negative two so we know that
so we know the derivative of l with respect to e and now we want dl
by da right and the chain rule is telling us that that's just dl by de
negative 2 times the local gradient so what is the local gradient basically d e
by d a we have to look at that so i'm a little times node
inside a massive graph and i only know that i did a times b and i produced an e
so now what is d e by d a and d e by d b that's the only thing that i sort of
know about that's my local gradient so because we have that e's a times b we're
asking what is d e by d a and of course we just did that here we
had a times so i'm not going to rederive it but if you want to differentiate this
with respect to a you'll just get b right the value of b which in this case is negative 3.0
so basically we have that dl by da well let me just do it right here we
have that a dot grad and we are applying chain rule here is d l by d e which we see here is
negative two times what is d e by d a
it's the value of b which is negative 3.
that's it and then we have b grad is again dl by
de which is negative 2 just the same way times what is d e by d
um db is the value of a which is 2.2.0
as the value of a so these are our claimed derivatives
let's redraw and we see here that a dot grad turns out to be 6 because
that is negative 2 times negative 3 and b dot grad is negative 4
times sorry is negative 2 times 2 which is negative 4. so those are our claims let's delete
this and let's verify them we have
a here a dot data plus equals h so the claim is that
a dot grad is six let's verify six
and we have beta data plus equals h so nudging b by h
and looking at what happens we claim it's negative four and indeed it's negative four plus minus
again float oddness um and uh
that's it this that was the manual back propagation
uh all the way from here to all the leaf nodes and we've done it piece by piece and really all we've done is as you saw
we iterated through all the nodes one by one and locally applied the chain rule we always know what is the derivative of
l with respect to this little output and then we look at how this output was produced this output was produced
through some operation and we have the pointers to the children nodes of this operation
and so in this little operation we know what the local derivatives are and we just multiply them onto the derivative
always so we just go through and recursively multiply on the local derivatives and
that's what back propagation is is just a recursive application of chain rule backwards through the computation graph
preview of a single optimization step
let's see this power in action just very briefly what we're going to do is we're going to
nudge our inputs to try to make l go up so in particular what we're doing is we
want a.data we're going to change it and if we want l to go up that means we just have to go in the direction of the
gradient so a should increase in the direction of gradient by like some small step amount
this is the step size and we don't just want this for ba but also for b
also for c also for f those are
leaf nodes which we usually have control over and if we nudge in direction of the
gradient we expect a positive influence on l so we expect l to go up
positively so it should become less negative it should go up to say negative you know
six or something like that uh it's hard to tell exactly and we'd have to rewrite the forward pass so let
me just um do that here um
this would be the forward pass f would be unchanged this is effectively the forward pass and now if we print l.data
we expect because we nudged all the values all the inputs in the rational gradient we expected a less negative l
we expect it to go up so maybe it's negative six or so let's see what happens
okay negative seven and uh this is basically one step of an
optimization that we'll end up running and really does gradient just give us some power because we know how to
influence the final outcome and this will be extremely useful for training knowledge as well as you'll see
manual backpropagation example #2: a neuron
so now i would like to do one more uh example of manual backpropagation using
a bit more complex and uh useful example we are going to back propagate through a
neuron so we want to eventually build up neural networks and in the simplest case these
are multilateral perceptrons as they're called so this is a two layer neural net and it's got these hidden layers made up
of neurons and these neurons are fully connected to each other now biologically neurons are very complicated devices but we have very
simple mathematical models of them and so this is a very simple mathematical model of a neuron you have
some inputs axis and then you have these synapses that have weights on them so
the w's are weights and then the synapse interacts with the input to
this neuron multiplicatively so what flows to the cell body of this neuron is w times x
but there's multiple inputs so there's many w times x's flowing into the cell body
the cell body then has also like some bias so this is kind of like the inert innate sort of trigger happiness
of this neuron so this bias can make it a bit more trigger happy or a bit less trigger happy regardless of the input
but basically we're taking all the w times x of all the inputs adding the bias and
then we take it through an activation function and this activation function is usually some kind of a squashing function
like a sigmoid or 10h or something like that so as an example we're going to use the 10h in this
example numpy has a np.10h so
we can call it on a range and we can plot it this is the 10h function and you see
that the inputs as they come in get squashed on the y coordinate here so
um right at zero we're going to get exactly zero and then as you go more positive in
the input then you'll see that the function will only go up to one and then plateau out
and so if you pass in very positive inputs we're gonna cap it smoothly at one and on the negative side we're gonna
cap it smoothly to negative one so that's 10h and that's the squashing function or an
activation function and what comes out of this neuron is just the activation function applied to the dot product of
the weights and the inputs so let's write one out
um i'm going to copy paste because
i don't want to type too much but okay so here we have the inputs x1 x2 so this is a two-dimensional
neuron so two inputs are going to come in these are thought out as the weights of this neuron
weights w1 w2 and these weights again are the synaptic strengths for each
input and this is the bias of the neuron b
and now we want to do is according to this model we need to multiply x1 times
w1 and x2 times w2 and then we need to add bias on top of
it and it gets a little messy here but all we are trying to do is x1 w1 plus x2 w2
plus b and these are multiply here except i'm doing it in small steps so
that we actually have pointers to all these intermediate nodes so we have x1 w1 variable x times x2 w2 variable and
i'm also labeling them so n is now the cell body raw
raw activation without the activation function for now
and this should be enough to basically plot it so draw dot of n
gives us x1 times w1 x2 times w2 being added
then the bias gets added on top of this and this n is this sum
so we're now going to take it through an activation function and let's say we use the 10h
so that we produce the output so what we'd like to do here is we'd like to do the output and i'll call it o
is um n dot 10h okay but we haven't yet written the 10h
now the reason that we need to implement another 10h function here is that tanh is a
hyperbolic function and we've only so far implemented a plus and the times and you can't make a 10h out of just pluses
and times you also need exponentiation so 10h is this kind of a formula here
you can use either one of these and you see that there's exponentiation involved which we have not implemented yet for
our low value node here so we're not going to be able to produce 10h yet and we have to go back up and implement something like it
now one option here is we could actually implement um
exponentiation right and we could return the x of a value instead of a 10h of a value
because if we had x then we have everything else that we need so um because we know how to add and we know
how to um we know how to add and we know how to multiply so we'd be able to create 10h
if we knew how to x but for the purposes of this example i specifically wanted to
show you that we don't necessarily need to have the most atomic pieces
in um in this value object we can actually like create functions at arbitrary
points of abstraction they can be complicated functions but they can be also very very simple functions like a plus and it's totally up to us the only
thing that matters is that we know how to differentiate through any one function so we take some inputs and we
make an output the only thing that matters it can be arbitrarily complex function as long as you know how to
create the local derivative if you know the local derivative of how the inputs impact the output then that's all you
need so we're going to cluster up all of this expression and we're not going to break it down to its atomic
pieces we're just going to directly implement tanh so let's do that depth nh
and then out will be a value of and we need this expression here so
um let me actually copy paste
let's grab n which is a cell.theta and then this i believe is the tan h
math.x of two no n
n minus one over two n plus one maybe i can call this x
just so that it matches exactly okay and now this will be t
and uh children of this node there's just one child and i'm wrapping it in a tuple so this
is a tuple of one object just self and here the name of this operation will be 10h
and we're going to return that okay
so now valley should be implementing 10h and now we can scroll all the way down here
and we can actually do n.10 h and that's going to return the tanhd output of n
and now we should be able to draw it out of o not of n so let's see how that worked
there we go n went through 10 h to produce this output
so now tan h is a sort of our little micro grad supported node
here as an operation and as long as we know the derivative of
10h then we'll be able to back propagate through it now let's see this 10h in action currently it's not squashing too
much because the input to it is pretty low so if the bias was increased to say
eight then we'll see that what's flowing into the 10h now is
two and 10h is squashing it to 0.96 so we're already hitting the tail of this 10h and
it will sort of smoothly go up to 1 and then plateau out over there okay so now i'm going to do something slightly strange i'm going to change
this bias from 8 to this number 6.88 etc
and i'm going to do this for specific reasons because we're about to start back propagation
and i want to make sure that our numbers come out nice they're not like very crazy numbers they're nice numbers that
we can sort of understand in our head let me also add a pose label o is short for output here
so that's zero okay so 0.88 flows into 10 h comes out 0.7 so on
so now we're going to do back propagation and we're going to fill in all the gradients so what is the derivative o with respect
to all the inputs here and of course in the typical neural network setting what we really
care about the most is the derivative of these neurons on the weights specifically the w2 and w1 because those
are the weights that we're going to be changing part of the optimization and the other thing that we have to remember is here we have only a single
neuron but in the neural natives typically have many neurons and they're connected so this is only like a one small neuron
a piece of a much bigger puzzle and eventually there's a loss function that sort of measures the accuracy of the neural net and we're back propagating
with respect to that accuracy and trying to increase it so let's start off by propagation here
in the end what is the derivative of o with respect to o the base case sort of we know
always is that the gradient is just 1.0 so let me fill it in
and then let me split out the drawing function
here and then here cell
clear this output here okay so now when we draw o we'll see that oh
that grad is one so now we're going to back propagate through the tan h so to back propagate through 10h we need
to know the local derivative of 10h so if we have that
o is 10 h of n then what is d o by d n
now what you could do is you could come here and you could take this expression and you could do your calculus derivative taking
um and that would work but we can also just scroll down wikipedia here into a section that hopefully tells us
that derivative uh d by dx of 10 h of x is any of these i like this one 1 minus 10
h square of x so this is 1 minus 10 h of x squared
so basically what this is saying is that d o by d n is
1 minus 10 h of n squared
and we already have 10 h of n that's just o so it's one minus o squared
so o is the output here so the output is this number
data is this number and then
what this is saying is that do by dn is 1 minus this squared so
one minus of that data squared is 0.5 conveniently
so the local derivative of this 10 h operation here is 0.5
and so that would be d o by d n so we can fill in that in that grad
is 0.5 we'll just fill in
so this is exactly 0.5 one half so now we're going to continue the back propagation
this is 0.5 and this is a plus node so how is backprop going to what is that
going to do here and if you remember our previous example a plus is just a distributor of gradient
so this gradient will simply flow to both of these equally and that's because the local derivative of this operation
is one for every one of its nodes so 1 times 0.5 is 0.5
so therefore we know that this node here which we called this
its grad is just 0.5 and we know that b dot grad is also 0.5
so let's set those and let's draw so 0.5
continuing we have another plus 0.5 again we'll just distribute it so 0.5 will flow to both of these
so we can set theirs
x2w2 as well that grad is 0.5 and let's redraw pluses are my favorite
uh operations to back propagate through because it's very simple so now it's flowing into these
expressions is 0.5 and so really again keep in mind what the derivative is telling us at every point in time along
here this is saying that if we want the output of this neuron to increase
then the influence on these expressions is positive on the output both of them are
positive contribution to the output
so now back propagating to x2 and w2 first this is a times node so we know that the
local derivative is you know the other term so if we want to calculate x2.grad
then can you think through what it's going to be
so x2.grad will be w2.data times this x2w2
by grad right and w2.grad will be
x2 that data times x2w2.grad
right so that's the local piece of chain rule
let's set them and let's redraw so here we see that the gradient on our weight 2 is 0 because x2 data was 0
right but x2 will have the gradient 0.5 because data here was 1. and so what's interesting here right is
because the input x2 was 0 then because of the way the times works
of course this gradient will be zero and think about intuitively why that is derivative always tells us the influence
of this on the final output if i wiggle w2 how is the output changing
it's not changing because we're multiplying by zero so because it's not changing there's no derivative and zero is the correct
answer because we're squashing it at zero and let's do it here point five should
come here and flow through this times and so we'll have that x1.grad is
can you think through a little bit what what this should be
the local derivative of times with respect to x1 is going to be w1
so w1 is data times x1 w1 dot grad
and w1.grad will be x1.data times x1 w2 w1 with graph
let's see what those came out to be so this is 0.5 so this would be negative 1.5 and this would be 1.
and we've back propagated through this expression these are the actual final derivatives so if we want this neuron's
output to increase we know that what's necessary is that
w2 we have no gradient w2 doesn't actually matter to this neuron right now but this neuron this weight should uh go
up so if this weight goes up then this neuron's output would have gone up and
proportionally because the gradient is one okay so doing the back propagation manually is obviously ridiculous so we
implementing the backward function for each operation
are now going to put an end to this suffering and we're going to see how we can implement uh the backward pass a bit
more automatically we're not going to be doing all of it manually out here it's now pretty obvious to us by example
how these pluses and times are back property ingredients so let's go up to the value
object and we're going to start codifying what we've seen in the examples below
so we're going to do this by storing a special cell dot backward
and underscore backward and this will be a function which is going to do that little piece of chain rule at each
little node that compute that took inputs and produced output uh we're going to store
how we are going to chain the the outputs gradient into the inputs gradients
so by default this will be a function that uh doesn't do anything
so um and you can also see that here in the value in micrograb
so with this backward function by default doesn't do anything
this is an empty function and that would be sort of the case for example for a leaf node for leaf node there's nothing to do
but now if when we're creating these out values these out values are an addition
of self and other and so we will want to sell set
outs backward to be the function that propagates the gradient
so let's define what should happen
and we're going to store it in a closure let's define what should happen when we call outs grad
for in addition our job is to take outs grad and propagate it into self's
grad and other grad so basically we want to sell self.grad to something
and we want to set others.grad to something okay
and the way we saw below how chain rule works we want to take the local derivative times
the sort of global derivative i should call it which is the derivative of the final output of the expression with respect to
out's data with respect to out so
the local derivative of self in an addition is 1.0 so it's just 1.0 times
outs grad that's the chain rule and others.grad will be 1.0 times
outgrad and what you basically what you're seeing here is that outscrad will simply be copied onto selfs grad
and others grad as we saw happens for an addition operation so we're going to later call this
function to propagate the gradient having done an addition let's now do multiplication we're going
to also define that backward and we're going to set its backward to
be backward and we want to chain outgrad into
self.grad and others.grad
and this will be a little piece of chain rule for multiplication so we'll have so what should this be
can you think through
so what is the local derivative here the local derivative was others.data
and then oops others.data and the times of that grad that's channel
and here we have self.data times of that grad that's what we've been doing
and finally here for 10 h left backward
and then we want to set out backwards to be just backward
and here we need to back propagate we have out that grad and we want to chain it into self.grad
and salt.grad will be the local derivative of this operation that we've done here which is 10h
and so we saw that the local the gradient is 1 minus the tan h of x squared which here is t
that's the local derivative because that's t is the output of this 10 h so 1 minus t squared is the local derivative
and then gradient um has to be multiplied because of the chain rule so outgrad is chained through the local
gradient into salt.grad and that should be basically it so we're
going to redefine our value node we're going to swing all the way down here
and we're going to redefine our expression make sure that all the grads are zero
okay but now we don't have to do this manually anymore we are going to basically be calling the
dot backward in the right order so first we want to call os
dot backwards
so o was the outcome of 10h right so calling all that those who's
backward will be this function this is what it will do
now we have to be careful because there's a times out.grad
and out.grad remember is initialized to zero
so here we see grad zero so as a base case we need to set both.grad to 1.0
to initialize this with 1
and then once this is 1 we can call oda backward and what that should do is it should
propagate this grad through 10h so the local derivative times
the global derivative which is initialized at one so this should
um a dope
so i thought about redoing it but i figured i should just leave the error in here because it's pretty funny why is
anti-object not callable uh it's because i screwed up we're trying to save these
functions so this is correct this here we don't want to call the function
because that returns none these functions return none we just want to store the function so let me redefine the value object
and then we're going to come back in redefine the expression draw a dot everything is great o dot grad is one
o dot grad is one and now now this should work of course
okay so all that backward should this grant should now be 0.5 if we redraw and if everything went correctly
0.5 yay okay so now we need to call ns.grad
and it's not awkward sorry ends backward so that seems to have worked
so instead backward routed the gradient to both of these so this is looking great
now we could of course called uh called b grad beat up backwards sorry
what's gonna happen well b doesn't have it backward b is backward
because b is a leaf node b's backward is by initialization the empty function
so nothing would happen but we can call call it on it but when we call
this one it's backward
then we expect this 0.5 to get further routed right so there we go 0.5.5
and then finally we want to call it here on x2 w2
and on x1 w1
do both of those and there we go so we get 0 0.5 negative 1.5 and 1
exactly as we did before but now we've done it through calling that backward um
sort of manually so we have the lamp one last piece to get rid of which is us calling
implementing the backward function for a whole expression graph
underscore backward manually so let's think through what we are actually doing um
we've laid out a mathematical expression and now we're trying to go backwards through that expression
um so going backwards through the expression just means that we never want to call a dot backward for any node
before we've done a sort of um everything after it
so we have to do everything after it before we're ever going to call that backward on any one node we have to get all of its full dependencies everything
that it depends on has to propagate to it before we can continue back propagation so this ordering of
graphs can be achieved using something called topological sort so topological sort
is basically a laying out of a graph such that all the edges go only from left to right basically
so here we have a graph it's a directory a cyclic graph a dag and this is two different topological
orders of it i believe where basically you'll see that it's laying out of the notes such that all the edges go only
one way from left to right and implementing topological sort you can look in wikipedia and so on i'm not
going to go through it in detail but basically this is what builds a
topological graph we maintain a set of visited nodes and
then we are going through starting at some root node
which for us is o that's where we want to start the topological sort and starting at o we go through all of
its children and we need to lay them out from left to right and basically this starts at o
if it's not visited then it marks it as visited and then it iterates through all of its children
and calls build topological on them and then uh after it's gone through all
the children it adds itself so basically this node that we're going to call it on
like say o is only going to add itself to the topo list after all of the
children have been processed and that's how this function is guaranteeing that you're only going to be in the list
once all your children are in the list and that's the invariant that is being maintained so if we built upon o and
then inspect this list we're going to see that it ordered our
value objects and the last one is the value of 0.707 which is the
output so this is o and then this is n and then all the other nodes get laid
out before it so that builds the topological graph and really what we're doing now is we're
just calling dot underscore backward on all of the nodes in a topological order
so if we just reset the gradients they're all zero what did we do we started by
setting o dot grad to b1 that's the base case
then we built the topological order and then we went for node
in reversed of topo now
in in the reverse order because this list goes from you know we need to go through it in reversed order
so starting at o note that backward and this should be
it there we go those are the correct derivatives
finally we are going to hide this functionality so i'm going to copy this and we're going to hide it
inside the valley class because we don't want to have all that code lying around so instead of an underscore backward
we're now going to define an actual backward so that's backward without the underscore
and that's going to do all the stuff that we just arrived so let me just clean this up a little bit so
we're first going to build a topological graph
starting at self so build topo of self
will populate the topological order into the topo list which is a local variable
then we set self.grad to be one and then for each node in the reversed
list so starting at us and going to all the children underscore backward
and that should be it so save
come down here redefine [Music] okay all the grands are zero
and now what we can do is oh that backward without the underscore and
there we go and that's uh that's back propagation
place for one neuron now we shouldn't be too happy with ourselves actually because we have a bad
fixing a backprop bug when one node is used multiple times
bug um and we have not surfaced the bug because of some specific conditions that we are we have to think about right now
so here's the simplest case that shows the bug say i create a single node a
and then i create a b that is a plus a and then i called backward
so what's going to happen is a is 3 and then a b is a plus a so there's two
arrows on top of each other here then we can see that b is of course the
forward pass works b is just a plus a which is six but the gradient here is not actually
correct that we calculate it automatically and that's because
um of course uh just doing calculus in your head the
derivative of b with respect to a should be uh two
one plus one it's not one intuitively what's happening here right so b is the result of a plus a and then
we call backward on it so let's go up and see what that does
um b is a result of addition so out as b and then when we called backward what
happened is self.grad was set to one and then other that grad was set to one
but because we're doing a plus a self and other are actually the exact same object
so we are overriding the gradient we are setting it to one and then we are setting it again to one and that's why
it stays at one so that's a problem there's another way to see this in a
little bit more complicated expression
so here we have a and b and then uh d will be the multiplication
of the two and e will be the addition of the two and then we multiply e times d to get f and
then we called fda backward and these gradients if you check will be incorrect
so fundamentally what's happening here again is basically we're going to see an issue
anytime we use a variable more than once until now in these expressions above every variable is used exactly once so
we didn't see the issue but here if a variable is used more than once what's going to happen during backward pass we're backpropagating from
f to e to d so far so good but now equals it backward and it deposits its
gradients to a and b but then we come back to d and call backward and it overwrites
those gradients at a and b so that's obviously a problem
and the solution here if you look at the multivariate case of the chain rule
and its generalization there the solution there is basically that we have to accumulate these gradients these
gradients add and so instead of setting those gradients
we can simply do plus equals we need to accumulate those gradients plus equals plus equals
plus equals plus equals and this will be okay remember because
we are initializing them at zero so they start at zero and then any
contribution that flows backwards will simply add
so now if we redefine this one because the plus equals this now works
because a.grad started at zero and we called beta backward we deposit one and
then we deposit one again and now this is two which is correct and here this will also work and we'll
get correct gradients because when we call eta backward we will deposit the gradients from this branch and then we get to back into
detail backward it will deposit its own gradients and then those gradients simply add on top of each other and so
we just accumulate those gradients and that fixes the issue okay now before we move on let me actually do a bit of
cleanup here and delete some of these some of this intermediate work so
we're not gonna need any of this now that we've derived all of it um we are going to keep this because i want
to come back to it delete the 10h delete our morning example
delete the step delete this keep the code that draws
and then delete this example and leave behind only the definition of value
breaking up a tanh, exercising with more operations
and now let's come back to this non-linearity here that we implemented the tanh now i told you that we could
have broken down 10h into its explicit atoms in terms of other expressions if
we had the x function so if you remember tan h is defined like this and we chose to develop tan h as a single function
and we can do that because we know its derivative and we can back propagate through it but we can also break down tan h into
and express it as a function of x and i would like to do that now because i want to prove to you that you get all the
same results and all those ingredients but also because it forces us to implement a few more expressions it
forces us to do exponentiation addition subtraction division and things like that and i think it's a good exercise to
go through a few more of these okay so let's scroll up to the definition of value
and here one thing that we currently can't do is we can do like a value of say 2.0
but we can't do you know here for example we want to add constant one and we can't do something like this
and we can't do it because it says object has no attribute data that's because a plus one comes right here to
add and then other is the integer one and then here python is trying to access
one.data and that's not a thing and that's because basically one is not a value object and we only have addition
for value objects so as a matter of convenience so that we can create expressions like this and make them make
sense we can simply do something like this basically
we let other alone if other is an instance of value but if it's not an instance of value we're going to assume
that it's a number like an integer float and we're going to simply wrap it in in value and then other will just become
value of other and then other will have a data attribute and this should work so if i just say this predefined value then
this should work there we go okay now let's do the exact same thing for multiply because we can't
do something like this again for the exact same reason so we just have to go to mole and if other is
not a value then let's wrap it in value let's redefine value and now this works
now here's a kind of unfortunate and not obvious part a times two works we saw that but two times a is that gonna work
you'd expect it to right but actually it will not and the reason it won't is because python doesn't know
like when when you do a times two basically um so a times two python will
go and it will basically do something like a dot mul of two that's basically what it will
call but to it 2 times a is the same as 2 dot mol of a
and it doesn't 2 can't multiply value and so it's really confused about that
so instead what happens is in python the way this works is you are free to define something called the r mold
and our mole is kind of like a fallback so if python can't do 2 times a it will check if um
if by any chance a knows how to multiply two and that will be called into our mole
so because python can't do two times a it will check is there an our mole in value and because there is it will now
call that and what we'll do here is we will swap the order of the operands so basically
two times a will redirect to armel and our mole will basically call a times two and that's how that will work
so redefining now with armor two times a becomes four okay now looking at the
other elements that we still need we need to know how to exponentiate and how to divide so let's first the explanation to the exponentiation part we're going
to introduce a single function x here and x is going to mirror 10h in the
sense that it's a simple single function that transforms a single scalar value and outputs a single scalar value
so we pop out the python number we use math.x to exponentiate it create a new value object
everything that we've seen before the tricky part of course is how do you propagate through e to the x
and so here you can potentially pause the video and think about what should go here
okay so basically we need to know what is the local derivative of e to the x so
d by d x of e to the x is famously just e to the x and we've already just calculated e to the x and it's inside
out that data so we can do up that data times and out that grad that's the chain rule
so we're just chaining on to the current running grad and this is what the expression looks like it looks a little confusing but
this is what it is and that's the exponentiation so redefining we should now be able to
call a.x and hopefully the backward pass works as well okay and the last thing we'd like
to do of course is we'd like to be able to divide now i actually will implement something slightly more powerful than division
because division is just a special case of something a bit more powerful so in particular just by rearranging
if we have some kind of a b equals value of 4.0 here we'd like to basically be able to do a divide b and we'd like
this to be able to give us 0.5 now division actually can be reshuffled
as follows if we have a divide b that's actually the same as a multiplying one over b
and that's the same as a multiplying b to the power of negative one and so what i'd like to do instead is i
basically like to implement the operation of x to the k for some constant uh k so it's an integer or a
float um and we would like to be able to differentiate this and then as a special case uh negative one will be division
and so i'm doing that just because uh it's more general and um yeah you might as well do it that way so basically what
i'm saying is we can redefine uh division which we will put here somewhere
yeah we can put it here somewhere what i'm saying is that we can redefine division so self-divide other
can actually be rewritten as self times other to the power of negative one and now
a value raised to the power of negative one we have now defined that so here's
so we need to implement the pow function where am i going to put the power function maybe here somewhere
this is the skeleton for it so this function will be called when we try to raise a value to some power and
other will be that power now i'd like to make sure that other is only an int or a float usually other is
some kind of a different value object but here other will be forced to be an end or a float otherwise the math
won't work for for or try to achieve in the specific case that would be a different derivative expression if we wanted other
to be a value so here we create the output value which is just uh you know this data raised to
the power of other and other here could be for example negative one that's what we are hoping to achieve
and then uh this is the backwards stub and this is the fun part which is what is the uh chain rule expression here for
back for um back propagating through the power function where the power is to the power
of some kind of a constant so this is the exercise and maybe pause the video here and see if you can figure it out yourself as to what we should put
here
okay so you can actually go here and look at derivative rules as an example and we
see lots of derivatives that you can hopefully know from calculus in particular what we're looking for is the power rule
because that's telling us that if we're trying to take d by dx of x to the n which is what we're doing here
then that is just n times x to the n minus 1 right okay
so that's telling us about the local derivative of this power operation
so all we want here basically n is now other and self.data is x
and so this now becomes other which is n times
self.data which is now a python in torah float it's not a valley object we're accessing
the data attribute raised to the power of other minus one or n
minus one i can put brackets around this but this doesn't matter because
power takes precedence over multiply and python so that would have been okay and that's the local derivative only but
now we have to chain it and we change just simply by multiplying by output grad that's chain rule
and this should technically work and we're going to find out soon but now
if we do this this should now work and we get 0.5 so the forward pass works
but does the backward pass work and i realize that we actually also have to know how to subtract so
right now a minus b will not work to make it work we need one more
piece of code here and basically this is the
subtraction and the way we're going to implement subtraction is we're going to implement it by addition of a negation
and then to implement negation we're gonna multiply by negative one so just again using the stuff we've already built and just um expressing it in terms
of what we have and a minus b is now working okay so now let's scroll again to this expression here for this neuron
and let's just compute the backward pass here once we've defined o and let's draw it
so here's the gradients for all these leaf nodes for this two-dimensional neuron that has a 10h that we've seen
before so now what i'd like to do is i'd like to break up this 10h into this expression here
so let me copy paste this here and now instead of we'll preserve the label
and we will change how we define o so in particular we're going to implement this formula here
so we need e to the 2x minus 1 over e to the x plus 1. so e to the 2x we need to take 2 times n and we
need to exponentiate it that's e to the two x and then because we're using it twice let's create an intermediate
variable e and then define o as e plus one over
e minus one over e plus one e minus one over e plus one
and that should be it and then we should be able to draw that of o so now before i run this what do we
expect to see number one we're expecting to see a much longer graph here because we've broken up 10h
into a bunch of other operations but those operations are mathematically equivalent and so what we're expecting
to see is number one the same result here so the forward pass works and number two because of that
mathematical equivalence we expect to see the same backward pass and the same gradients on these leaf nodes so these
gradients should be identical so let's run this so number one let's verify that instead
of a single 10h node we have now x and we have plus we have times negative one
uh this is the division and we end up with the same forward pass here and then the gradients we have to be
careful because they're in slightly different order potentially the gradients for w2x2 should be 0 and 0.5
w2 and x2 are 0 and 0.5 and w1 x1 are 1 and negative 1.5
1 and negative 1.5 so that means that both our forward passes and backward passes were correct
because this turned out to be equivalent to 10h before and so the reason i wanted to go through
this exercise is number one we got to practice a few more operations and uh writing more backwards passes and number
two i wanted to illustrate the point that the um the level at which you implement your
operations is totally up to you you can implement backward passes for tiny expressions like a single individual
plus or a single times or you can implement them for say 10h
which is a kind of a potentially you can see it as a composite operation because it's made up of all these more atomic
operations but really all of this is kind of like a fake concept all that matters is we have some kind of inputs
and some kind of an output and this output is a function of the inputs in some way and as long as you can do forward pass and the backward pass of
that little operation it doesn't matter what that operation is and how composite it is
if you can write the local gradients you can chain the gradient and you can continue back propagation so the design
of what those functions are is completely up to you so now i would like to show you how you
doing the same thing but in PyTorch: comparison
can do the exact same thing by using a modern deep neural network library like for example pytorch which i've roughly
modeled micrograd by and so pytorch is something you would use in production and i'll show you how you can
do the exact same thing but in pytorch api so i'm just going to copy paste it in and walk you through it a little bit
this is what it looks like so we're going to import pi torch and then we need to define these
value objects like we have here now micrograd is a scalar valued
engine so we only have scalar values like 2.0 but in pi torch everything is
based around tensors and like i mentioned tensors are just n-dimensional arrays of scalars
so that's why things get a little bit more complicated here i just need a scalar value to tensor a tensor with
just a single element but by default when you work with pytorch you would use um
more complicated tensors like this so if i import pytorch
then i can create tensors like this and this tensor for example is a two by three array
of scalar scalars in a single compact representation so we
can check its shape we see that it's a two by three array and so on so this is usually what you would work
with um in the actual libraries so here i'm creating a tensor that has only a single element
2.0 and then i'm casting it to be double
because python is by default using double precision for its floating point numbers so i'd like everything to be
identical by default the data type of these tensors will be float32 so it's
only using a single precision float so i'm casting it to double so that we have float64 just like in
python so i'm casting to double and then we get something similar to value of two the
next thing i have to do is because these are leaf nodes by default pytorch assumes that they do not require gradients so i need to explicitly say
that all of these nodes require gradients okay so this is going to construct scalar valued one element tensors
make sure that fighters knows that they require gradients now by default these are set to false by the way because of
efficiency reasons because usually you would not want gradients for leaf nodes like the inputs to the network and this
is just trying to be efficient in the most common cases so once we've defined all of our values
in python we can perform arithmetic just like we can here in microgradlend so this will just work and then there's a
torch.10h also and when we get back is a tensor again and we can
just like in micrograd it's got a data attribute and it's got grant attributes so these tensor objects just like in
micrograd have a dot data and a dot grad and the only difference here is that we need
to call it that item because otherwise um pi torch
that item basically takes a single tensor of one element and it just returns that element stripping out
the tensor so let me just run this and hopefully we are going to get this is going to print
the forward pass which is 0.707 and this will be the gradients which
hopefully are 0.5 0 negative 1.5 and 1. so if we just run this
there we go 0.7 so the forward pass agrees and then point five zero negative one point five
and one so pi torch agrees with us and just to show you here basically o
here's a tensor with a single element and it's a double and we can call that item on it to just
get the single number out so that's what item does and o is a tensor object like i mentioned and it's
got a backward function just like we've implemented and then all of these also have a dot graph so like x2 for example in the grad
and it's a tensor and we can pop out the individual number with that actin
so basically torches torch can do what we did in micrograph is a special case when your
tensors are all single element tensors but the big deal with pytorch is that everything is significantly more
efficient because we are working with these tensor objects and we can do lots of operations in parallel on all of
these tensors but otherwise what we've built very much agrees with the api of pytorch
building out a neural net library (multi-layer perceptron) in micrograd
okay so now that we have some machinery to build out pretty complicated mathematical expressions we can also start building out neural nets and as i
mentioned neural nets are just a specific class of mathematical expressions so we're going to start building out a
neural net piece by piece and eventually we'll build out a two-layer multi-layer layer perceptron as it's called and i'll
show you exactly what that means let's start with a single individual neuron we've implemented one here but
here i'm going to implement one that also subscribes to the pytorch api in how it designs its neural network
modules so just like we saw that we can like match the api of pytorch on the auto grad side we're going to try
to do that on the neural network modules so here's class neuron and just for the sake of efficiency i'm
going to copy paste some sections that are relatively straightforward so the constructor will take
number of inputs to this neuron which is how many inputs come to a neuron so this
one for example has three inputs and then it's going to create a weight there is some random number between
negative one and one for every one of those inputs and a bias that controls the overall
trigger happiness of this neuron and then we're going to implement a def underscore underscore call
of self and x some input x and really what we don't do here is w times x plus b
where w times x here is a dot product specifically now if you haven't seen
call let me just return 0.0 here for now the way this works now is we can have an x
which is say like 2.0 3.0 then we can initialize a neuron that is two-dimensional
because these are two numbers and then we can feed those two numbers into that neuron to get an output
and so when you use this notation n of x python will use call
so currently call just return 0.0
now we'd like to actually do the forward pass of this neuron instead so we're going to do here first is we
need to basically multiply all of the elements of w with all of the elements of x pairwise we need to multiply them
so the first thing we're going to do is we're going to zip up celta w and x and in python zip takes two iterators
and it creates a new iterator that iterates over the tuples of the corresponding entries
so for example just to show you we can print this list and still return 0.0 here
sorry so we see that these w's are paired up
with the x's w with x
and now what we want to do is
for w i x i in we want to multiply w times
w wi times x i and then we want to sum all of that together to come up with an activation
and add also subnet b on top so that's the raw activation and then of course we need to pass that through a
non-linearity so what we're going to be returning is act.10h and here's out
so now we see that we are getting some outputs and we get a different output from a neuron each time because we are
initializing different weights and by and biases and then to be a bit more efficient here actually sum by the way takes a second
optional parameter which is the start and by default the start is zero so
these elements of this sum will be added on top of zero to begin with but actually we can just start with cell dot
b and then we just have an expression like this
and then the generator expression here must be parenthesized in python there we go
yep so now we can forward a single neuron next up we're going to define a layer of neurons so here we have a
schematic for a mlb so we see that these mlps each layer
this is one layer has actually a number of neurons and they're not connected to each other but all of them are fully connected to the input
so what is a layer of neurons it's just it's just a set of neurons evaluated independently
so in the interest of time i'm going to do something fairly straightforward here
it's um literally a layer is just a list of neurons
and then how many neurons do we have we take that as an input argument here how many neurons do you want in your layer
number of outputs in this layer and so we just initialize completely independent neurons with this given
dimensionality and when we call on it we just independently evaluate them so now instead of a neuron
we can make a layer of neurons they are two-dimensional neurons and let's have three of them and now we see that we have three
independent evaluations of three different neurons right okay finally let's complete this picture
and define an entire multi-layer perceptron or mlp and as we can see here in an mlp these
layers just feed into each other sequentially so let's come here and i'm just going to copy the code here in interest of time
so an mlp is very similar we're taking the number of inputs as before but now instead of taking a
single n out which is number of neurons in a single layer we're going to take a list of an outs and this list defines
the sizes of all the layers that we want in our mlp so here we just put them all together and then iterate over consecutive pairs
of these sizes and create layer objects for them and then in the call function we are just calling them sequentially so that's
an mlp really and let's actually re-implement this picture so we want three input neurons
and then two layers of four and an output unit so we want
a three-dimensional input say this is an example input we want three inputs into
two layers of four and one output and this of course is an mlp
and there we go that's a forward pass of an mlp to make this a little bit nicer you see how we have just a single element but
it's wrapped in a list because layer always returns lists circle for convenience
return outs at zero if len out is exactly a single element else return fullest
and this will allow us to just get a single value out at the last layer that only has a single neuron
and finally we should be able to draw dot of n of x and as you might imagine
these expressions are now getting relatively involved so this is an entire mlp that we're
defining now
all the way until a single output okay and so obviously you would never
differentiate on pen and paper these expressions but with micrograd we will be able to back propagate all the way
through this and back propagate into these weights of all these neurons so
let's see how that works okay so let's create ourselves a very simple example data set here
creating a tiny dataset, writing the loss function
so this data set has four examples and so we have four possible inputs into the neural net
and we have four desired targets so we'd like the neural net to assign
or output 1.0 when it's fed this example negative one when it's fed these examples and one when it's fed this
example so it's a very simple binary classifier neural net basically that we would like here
now let's think what the neural net currently thinks about these four examples we can just get their predictions
um basically we can just call n of x for x in axis and then we can
print so these are the outputs of the neural net on those four examples
so the first one is 0.91 but we'd like it to be one so we should push this one
higher this one we want to be higher this one says 0.88 and we want this to
be negative one this is 0.8 we want it to be negative one and this one is 0.8 we want it to be one
so how do we make the neural net and how do we tune the weights to better predict the desired targets
and the trick used in deep learning to achieve this is to calculate a single number that somehow
measures the total performance of your neural net and we call this single number the loss
so the loss first is is a single number that we're going to define that basically measures how
well the neural net is performing right now we have the intuitive sense that it's not performing very well because we're not very much close to this
so the loss will be high and we'll want to minimize the loss so in particular in this case what we're
going to do is we're going to implement the mean squared error loss so this is doing is we're going to
basically iterate um for y ground truth
and y output in zip of um wise and white red so we're going to
pair up the ground truths with the predictions and this zip iterates over tuples of
them and for each y ground truth and y output we're going
to subtract them and square them
so let's first see what these losses are these are individual loss components and so basically for each
one of the four we are taking the prediction and the ground truth we are subtracting them and
squaring them so because this one is so close to its target 0.91
is almost one subtracting them gives a very small number
so here we would get like a negative point one and then squaring it just makes sure
that regardless of whether we are more negative or more positive we always get a positive
number instead of squaring we should we could also take for example the absolute value we need to discard the sign
and so you see that the expression is ranged so that you only get zero exactly when y out is equal to y ground truth
when those two are equal so your prediction is exactly the target you are going to get zero and if your prediction is not the target
you are going to get some other number so here for example we are way off and so that's why the loss is quite high
and the more off we are the greater the loss will be so we don't want high loss we want low
loss and so the final loss here will be just the sum
of all of these numbers so you see that this should be zero roughly plus zero roughly
but plus seven so loss should be about seven
here and now we want to minimize the loss we want the loss to be low
because if loss is low then every one of the predictions is equal to its target
so the loss the lowest it can be is zero and the greater it is the worse off the
neural net is predicting so now of course if we do lost that backward
something magical happened when i hit enter and the magical thing of course that happened is that we can look at
end.layers.neuron and that layers at say like the the first layer that neurons at zero
because remember that mlp has the layers which is a list and each layer has a neurons which is a
list and that gives us an individual neuron and then it's got some weights and so we can for example look at the
weights at zero um
oops it's not called weights it's called w and that's a value but now this value
also has a groud because of the backward pass and so we see that because this gradient
here on this particular weight of this particular neuron of this particular layer is negative we see that its influence on the loss is
also negative so slightly increasing this particular weight of this neuron of this layer would make the loss go down
and we actually have this information for every single one of our neurons and all their parameters actually it's worth looking at also the draw dot loss by the
way so previously we looked at the draw dot of a single neural neuron forward pass and that was already a large expression
but what is this expression we actually forwarded every one of those four examples and
then we have the loss on top of them with the mean squared error and so this is a really massive graph
because this graph that we've built up now oh my gosh this graph that we've built
up now which is kind of excessive it's excessive because it has four forward passes of a neural net for every one of
the examples and then it has the loss on top and it ends with the value of the loss which was 7.12
and this loss will now back propagate through all the four forward passes all the way through just every single
intermediate value of the neural net all the way back to of course the parameters of the weights which are the
input so these weight parameters here are inputs to this neural net
and these numbers here these scalars are inputs to the neural net so if we went around here
we'll probably find some of these examples this 1.0 potentially maybe this 1.0 or you know
some of the others and you'll see that they all have gradients as well the thing is these gradients on the
input data are not that useful to us and that's because the input data seems
to be not changeable it's it's a given to the problem and so it's a fixed input we're not going to be changing it or
messing with it even though we do have gradients for it but some of these gradients here
will be for the neural network parameters the ws and the bs and those we of course we want to change
okay so now we're going to want some convenience code to gather up all of the parameters of the neural net so that we
collecting all of the parameters of the neural net
can operate on all of them simultaneously and every one of them we will nudge a tiny amount
based on the gradient information so let's collect the parameters of the neural net all in one array
so let's create a parameters of self that just returns celta w which is a list
concatenated with a list of self.b so this will just return a list
list plus list just you know gives you a list so that's parameters of neuron and i'm
calling it this way because also pi torch has a parameters on every single and in module
and uh it does exactly what we're doing here it just returns the parameter tensors for us as the
parameter scalars now layer is also a module so it will have parameters
itself and basically what we want to do here is something like this like
params is here and then for neuron in salt out neurons
we want to get neuron.parameters and we want to params.extend
right so these are the parameters of this neuron and then we want to put them on top of params so params dot extend
of peace and then we want to return brands so this is way too much code so actually
there's a way to simplify this which is return
p for neuron in self neurons
for p in neuron dot parameters
so it's a single list comprehension in python you can sort of nest them like this and you can um
then create uh the desired array so this is these are identical
we can take this out and then let's do the same here
def parameters self and return a parameter for layer in self dot layers
for p in layer dot parameters
and that should be good now let me pop out this so
we don't re-initialize our network because we need to re-initialize our
okay so unfortunately we will have to probably re-initialize the network because we just add functionality
because this class of course we i want to get all the and that parameters but that's not going to work because this is
the old class okay so unfortunately we do have to reinitialize the network which will
change some of the numbers but let me do that so that we pick up the new api we can now do in the
parameters and these are all the weights and biases inside the entire neural net
so in total this mlp has 41 parameters
and now we'll be able to change them if we recalculate the loss here we see
doing gradient descent optimization manually, training the network
that unfortunately we have slightly different predictions and slightly different laws
but that's okay okay so we see that this neurons gradient is slightly negative we can
also look at its data right now which is 0.85 so this is the current
value of this neuron and this is its gradient on the loss so what we want to do now is we want to
iterate for every p in n dot parameters so for all the 41 parameters in this neural net
we actually want to change p data slightly according to the gradient information
okay so dot dot to do here but this will be basically a tiny update
in this gradient descent scheme in gradient descent we are thinking of the
gradient as a vector pointing in the direction of increased
loss and so in gradient descent we are modifying
p data by a small step size in the direction of the gradient so the step size as an
example could be like a very small number like 0.01 is the step size times p dot grad
right but we have to think through some of the signs here so uh
in particular working with this specific example here we see that if we just left it like this
then this neuron's value would be currently increased by a tiny amount of the gradient
the grain is negative so this value of this neuron would go slightly down it would become like 0.8 you know four or
something like that but if this neuron's value goes lower
that would actually increase the loss that's because
the derivative of this neuron is negative so increasing this makes the loss go down so
increasing it is what we want to do instead of decreasing it so basically what we're missing here is we're
actually missing a negative sign and again this other interpretation and that's because we want to minimize
the loss we don't want to maximize the loss we want to decrease it and the other interpretation as i mentioned is you can think of the
gradient vector so basically just the vector of all the gradients as pointing in the direction of
increasing the loss but then we want to decrease it so we actually want to go in the opposite direction
and so you can convince yourself that this sort of plug does the right thing here with the negative because we want to minimize the loss
so if we nudge all the parameters by tiny amount
then we'll see that this data will have changed a little bit so now this neuron
is a tiny amount greater value so 0.854 went to 0.857
and that's a good thing because slightly increasing this neuron uh
data makes the loss go down according to the gradient and so the correct thing has happened sign wise
and so now what we would expect of course is that because we've changed all these parameters we expect that the loss
should have gone down a bit so we want to re-evaluate the loss let me basically
this is just a data definition that hasn't changed but the forward pass here of the network we can recalculate
and actually let me do it outside here so that we can compare the two loss values so here if i recalculate the loss
we'd expect the new loss now to be slightly lower than this number so hopefully what we're getting now is a
tiny bit lower than 4.84 4.36 okay and remember the way we've arranged
this is that low loss means that our predictions are matching the targets so our predictions now are probably
slightly closer to the targets and now all we have to do is we
have to iterate this process so again um we've done the forward pass and this is the loss
now we can lost that backward let me take these out and we can do a step size
and now we should have a slightly lower loss 4.36 goes to 3.9
and okay so we've done the forward pass here's the backward pass nudge
and now the loss is 3.66 3.47
and you get the idea we just continue doing this and this is uh gradient descent we're just iteratively doing
forward pass backward pass update forward pass backward pass update and the neural net is improving its
predictions so here if we look at why pred now
like red we see that um this value should be getting closer to
one so this value should be getting more positive these should be getting more negative and this one should be also getting more positive so if we just
iterate this a few more times actually we may be able to afford go to
go a bit faster let's try a slightly higher learning rate
oops okay there we go so now we're at 0.31 if you go too fast by the way if you try
to make it too big of a step you may actually overstep
it's overconfidence because again remember we don't actually know exactly about the loss function the loss function has all kinds of structure and
we only know about the very local dependence of all these parameters on the loss but if we step too far
we may step into you know a part of the loss that is completely different and that can destabilize training and
make your loss actually blow up even so the loss is now 0.04 so actually the
predictions should be really quite close let's take a look so you see how this is almost one
almost negative one almost one we can continue going uh so
yep backward update oops there we go so we went way too fast
and um we actually overstepped so we got two uh too eager where are we
now oops okay seven e negative nine so this is very very low loss
and the predictions are basically perfect so somehow we
basically we were doing way too big updates and we briefly exploded but then somehow we ended up getting into a really good spot so usually this
learning rate and the tuning of it is a subtle art you want to set your learning rate if it's too low you're going to
take way too long to converge but if it's too high the whole thing gets unstable and you might actually even
explode the loss depending on your loss function so finding the step size to be just
right it's it's a pretty subtle art sometimes when you're using sort of vanilla gradient descent
but we happen to get into a good spot we can look at n-dot parameters
so this is the setting of weights and biases that makes our network
predict the desired targets very very close and
basically we've successfully trained neural net okay let's make this a tiny bit more respectable and implement an actual
training loop and what that looks like so this is the data definition that stays this is the forward pass
um so for uh k in range you know we're going to
take a bunch of steps first you do the forward pass
we validate the loss let's re-initialize the neural net from scratch
and here's the data and we first do before pass then we do
the backward pass
and then we do an update that's gradient descent
and then we should be able to iterate this and we should be able to print the current step the current loss um let's just print the
sort of number of the loss and that should be it
and then the learning rate 0.01 is a little too small 0.1 we saw is like a little bit dangerously too high let's go
somewhere in between and we'll optimize this for not 10 steps but let's go for say 20
steps let me erase all of this junk
and uh let's run the optimization and you see how we've actually converged
slower in a more controlled manner and got to a loss that is very low
so i expect white bread to be quite good there we go
um and that's it okay so this is kind of embarrassing but
we actually have a really terrible bug in here and it's a subtle bug and it's a
very common bug and i can't believe i've done it for the 20th time in my life
especially on camera and i could have reshot the whole thing but i think it's pretty funny and you know you get to
appreciate a bit what um working with neural nets maybe is like sometimes
we are guilty of come bug i've actually tweeted
the most common neural net mistakes a long time ago now uh and
i'm not really gonna explain any of these except for we are guilty of number three you forgot to
zero grad before that backward what is that
basically what's happening and it's a subtle bug and i'm not sure if you saw it is that all of these
weights here have a dot data and a dot grad and that grad starts at zero
and then we do backward and we fill in the gradients and then we do an update on the data but
we don't flush the grad it stays there so when we do the second
forward pass and we do backward again remember that all the backward operations do a plus equals on the grad
and so these gradients just add up and they never get reset to zero
so basically we didn't zero grad so here's how we zero grad before
backward we need to iterate over all the parameters and we need to make sure that p dot grad
is set to zero we need to reset it to zero just like it is in the constructor
so remember all the way here for all these value nodes grad is reset to zero and then all these backward passes do a
plus equals from that grad but we need to make sure that we reset these graphs to zero so that
when we do backward all of them start at zero and the actual backward pass accumulates um
the loss derivatives into the grads so this is zero grad in pytorch
and uh we will slightly get we'll get a slightly different optimization let's reset the neural net
the data is the same this is now i think correct and we get a much more
you know we get a much more slower descent we still end up with pretty good results
and we can continue this a bit more to get down lower and lower
and lower yeah so the only reason that the previous
thing worked it's extremely buggy um the only reason that worked is that
this is a very very simple problem and it's very easy for this neural net to fit this data
and so the grads ended up accumulating and it effectively gave us a massive step size and it made us converge
extremely fast but basically now we have to do more steps to get to very low values of loss
and get wipe red to be really good we can try to step a bit greater
yeah we're gonna get closer and closer to one minus one and one so
working with neural nets is sometimes tricky because uh
you may have lots of bugs in the code and uh your network might actually work just like ours worked
but chances are is that if we had a more complex problem then actually this bug would have made us not optimize the loss
very well and we were only able to get away with it because the problem is very simple
summary of what we learned, how to go towards modern neural nets
so let's now bring everything together and summarize what we learned what are neural nets neural nets are
these mathematical expressions fairly simple mathematical expressions in the case of multi-layer perceptron
that take input as the data and they take input the weights and the parameters of the
neural net mathematical expression for the forward pass followed by a loss function and the loss function tries to
measure the accuracy of the predictions and usually the loss will be low when your predictions are matching your
targets or where the network is basically behaving well so we we manipulate the loss function so that
when the loss is low the network is doing what you want it to do on your problem
and then we backward the loss use backpropagation to get the gradient and then we know how to tune all the
parameters to decrease the loss locally but then we have to iterate that process many times in what's called the gradient
descent so we simply follow the gradient information and that minimizes the loss
and the loss is arranged so that when the loss is minimized the network is doing what you want it to do
and yeah so we just have a blob of neural stuff and we can make it do arbitrary things and that's what gives
neural nets their power um it's you know this is a very tiny network with 41 parameters
but you can build significantly more complicated neural nets with billions at this point almost trillions of
parameters and it's a massive blob of neural tissue simulated neural tissue
roughly speaking and you can make it do extremely complex problems and these neurons then have all
kinds of very fascinating emergent properties in when you try to make them do
significantly hard problems as in the case of gpt for example we have massive amounts of text from the
internet and we're trying to get a neural net to predict to take like a few words and try to predict the next word
in a sequence that's the learning problem and it turns out that when you train this on all of internet the neural net
actually has like really remarkable emergent properties but that neural net would have hundreds of billions of parameters
but it works on fundamentally the exact same principles the neural net of course will be a bit more complex but otherwise the
value in the gradient is there and would be identical and the gradient descent would be there and would be
basically identical but people usually use slightly different updates this is a very simple stochastic gradient descent
update um and the loss function would not be mean squared error they would be using something called the cross-entropy loss
for predicting the next token so there's a few more details but fundamentally the neural network setup and neural network
training is identical and pervasive and now you understand intuitively
how that works under the hood in the beginning of this video i told you that by the end of it you would understand everything in micrograd and then we'd
walkthrough of the full code of micrograd on github
slowly build it up let me briefly prove that to you so i'm going to step through all the code that is in micrograd as of today
actually potentially some of the code will change by the time you watch this video because i intend to continue developing micrograd
but let's look at what we have so far at least init.pi is empty when you go to engine.pi that has the value
everything here you should mostly recognize so we have the data.grad attributes we have the backward function
uh we have the previous set of children and the operation that produced this value we have addition multiplication and
raising to a scalar power we have the relu non-linearity which is slightly different type of nonlinearity
than 10h that we used in this video both of them are non-linearities and notably 10h is not actually present in
micrograd as of right now but i intend to add it later with the backward which is identical and
then all of these other operations which are built up on top of operations here so values should be very recognizable
except for the non-linearity used in this video um there's no massive difference between relu and 10h and sigmoid and these other
non-linearities they're all roughly equivalent and can be used in mlps so i use 10h because it's a bit smoother and
because it's a little bit more complicated than relu and therefore it's stressed a little bit more the
local gradients and working with those derivatives which i thought would be useful and then that pi is the neural networks
library as i mentioned so you should recognize identical implementation of neuron layer and mlp
notably or not so much we have a class module here there is a parent class of all these modules i did
that because there's an nn.module class in pytorch and so this exactly matches that api and end.module and pytorch has
also a zero grad which i've refactored out here so that's the end of micrograd really
then there's a test which you'll see basically creates two chunks of code one in micrograd and
one in pi torch and we'll make sure that the forward and the backward pass agree identically for a slightly less complicated
expression a slightly more complicated expression everything agrees so we agree with pytorch on all
of these operations and finally there's a demo.ipymb here and it's a bit more complicated binary
classification demo than the one i covered in this lecture so we only had a tiny data set of four examples um here
we have a bit more complicated example with lots of blue points and lots of red points and we're trying to again build a
binary classifier to distinguish uh two dimensional points as red or blue it's a bit more complicated mlp here
with it's a bigger mlp the loss is a bit more complicated because it supports batches
so because our dataset was so tiny we always did a forward pass on the entire data set of four examples but when your
data set is like a million examples what we usually do in practice is we chair we basically pick out some random subset we
call that a batch and then we only process the batch forward backward and update so we don't have to forward the
entire training set so this supports batching because there's a lot more examples here
we do a forward pass the loss is slightly more different this is a max margin loss that i implement here
the one that we used was the mean squared error loss because it's the simplest one there's also the binary cross entropy
loss all of them can be used for binary classification and don't make too much of a difference in the simple examples
that we looked at so far there's something called l2 regularization used here this has to do
with generalization of the neural net and controls the overfitting in machine learning setting but i did not cover
these concepts and concepts in this video potentially later and the training loop you should recognize so forward backward with zero
grad and update and so on you'll notice that in the update here the learning rate is
scaled as a function of number of iterations and it shrinks and this is something called learning
rate decay so in the beginning you have a high learning rate and as the network sort of stabilizes near the end you
bring down the learning rate to get some of the fine details in the end and in the end we see the decision
surface of the neural net and we see that it learns to separate out the red and the blue area based on the data
points so that's the slightly more complicated example and then we'll demo that hyper ymb that you're free to go over
but yeah as of today that is micrograd i also wanted to show you a little bit of real stuff so that you get to see how
real stuff: diving into PyTorch, finding their backward pass for tanh
this is actually implemented in production grade library like by torch uh so in particular i wanted to show i
wanted to find and show you the backward pass for 10h in pytorch so here in
micrograd we see that the backward password 10h is one minus t square where t is the output of the tanh of x
times of that grad which is the chain rule so we're looking for something that looks like this now
i went to pytorch um which has an open source github codebase and uh i looked
through a lot of its code and honestly i i i spent about 15 minutes and i couldn't find 10h
and that's because these libraries unfortunately they grow in size and entropy and if you just search for 10h
you get apparently 2 800 results and 400 and 406 files so i don't know what these
files are doing honestly and why there are so many mentions of
10h but unfortunately these libraries are quite complex they're meant to be used not really inspected um
eventually i did stumble on someone who tries to change the 10 h backward
code for some reason and someone here pointed to the cpu kernel and the kuda kernel for 10 inch
backward so this so basically depends on if you're using pi torch on a cpu device or
on a gpu which these are different devices and i haven't covered this but this is the 10 h backwards kernel
for uh cpu and the reason it's so large is that
number one this is like if you're using a complex type which we haven't even talked about if you're using a specific data type of b-float 16 which we haven't
talked about and then if you're not then this is the kernel and deep here we see something
that resembles our backward pass so they have a times one minus b square uh so this b
b here must be the output of the 10h and this is the health.grad so here we found it
uh deep inside pi torch from this location for some reason inside binaryops kernel when 10h
is not actually a binary op and then this is the gpu kernel
we're not complex we're here and here we go with one line of code
so we did find it but basically unfortunately these codepieces are very large and
micrograd is very very simple but if you actually want to use real stuff uh finding the code for it you'll actually
find that difficult i also wanted to show you a little example here where pytorch is showing
you how can you can register a new type of function that you want to add to pytorch as a lego building block
so here if you want to for example add a gender polynomial 3
here's how you could do it you will register it as a class that subclasses storage.org that function
and then you have to tell pytorch how to forward your new function and how to backward through it
so as long as you can do the forward pass of this little function piece that you want to add and as long as you know the the local derivative the local
gradients which are implemented in the backward pi torch will be able to back propagate through your function and then
you can use this as a lego block in a larger lego castle of all the different lego blocks that pytorch already has
and so that's the only thing you have to tell pytorch and everything would just work and you can register new types of functions
in this way following this example and that is everything that i wanted to cover in this lecture so i hope you enjoyed building out
conclusion
micrograd with me i hope you find it interesting insightful and
yeah i will post a lot of the links that are related to this video in the video description below i will also
probably post a link to a discussion forum or discussion group where you can ask questions related to this video and then
i can answer or someone else can answer your questions and i may also do a follow-up video that answers some of the
most common questions but for now that's it i hope you enjoyed it if you did then please like and
subscribe so that youtube knows to feature this video to more people and that's it for now i'll see you later
outtakes :)
now here's the problem we know dl by
wait what is the problem and that's everything i wanted to cover in this lecture
so i hope you enjoyed us building up microcraft micro crab
okay now let's do the exact same thing for multiply because we can't do something like a times two
oops i know what happened there

intro
hi everyone hope you're well and next up what i'd like to do is i'd like to build out make more
like micrograd before it make more is a repository that i have on my github webpage
you can look at it but just like with micrograd i'm going to build it out step by step and i'm going to spell everything out so we're
going to build it out slowly and together now what is make more make more as the name suggests
makes more of things that you give it so here's an example names.txt is an example dataset to make
more and when you look at names.txt you'll find that it's a very large data set of
names so here's lots of different types of names in fact i believe there are 32 000 names
that i've sort of found randomly on the government website and if you train make more on this data
set it will learn to make more of things like this
and in particular in this case that will mean more things that sound name-like
but are actually unique names and maybe if you have a baby and you're trying to assign name maybe you're looking for a cool new sounding unique
name make more might help you so here are some example generations from the neural network
once we train it on our data set so here's some example unique names that it will generate
dontel irot zhendi and so on and so all these are sound
name like but they're not of course names so under the hood make more is a
character level language model so what that means is that it is treating every single line here as an example and
within each example it's treating them all as sequences of individual characters so r e e s e is this example
and that's the sequence of characters and that's the level on which we are building out make more and what it means
to be a character level language model then is that it's just uh sort of modeling those sequences of characters
and it knows how to predict the next character in the sequence now we're actually going to implement a
large number of character level language models in terms of the neural networks that are involved in predicting the next
character in a sequence so very simple bi-gram and back of work models multilingual perceptrons recurrent
neural networks all the way to modern transformers in fact the transformer that we will build will be basically the
equivalent transformer to gpt2 if you have heard of gpt uh so that's kind of a big deal it's a modern network and by
the end of the series you will actually understand how that works um on the level of characters now to give you a
sense of the extensions here uh after characters we will probably spend some time on the word level so that we can
generate documents of words not just little you know segments of characters but we can generate entire large much
larger documents and then we're probably going to go into images and image text
networks such as dolly stable diffusion and so on but for now we have to start
here character level language modeling let's go so like before we are starting with a completely blank Jupyter notebook page
reading and exploring the dataset
the first thing is i would like to basically load up the dataset names.txt so we're going to open up names.txt for
reading and we're going to read in everything into a massive string
and then because it's a massive string we'd only like the individual words and put them in the list so let's call split lines
on that string to get all of our words as a python list of strings
so basically we can look at for example the first 10 words and we have that it's a list of emma
olivia eva and so on and if we look at the top of the page here that is indeed
what we see um so that's good this list actually makes me feel that
this is probably sorted by frequency but okay so
these are the words now we'd like to actually like learn a little bit more about this data set let's look at the
total number of words we expect this to be roughly 32 000 and then what is the for example
shortest word so min of length of each word for w inwards
so the shortest word will be length two and max of one w for w in words so the
longest word will be 15 characters so let's now think through our very first language model
as i mentioned a character level language model is predicting the next character in a sequence given already
some concrete sequence of characters before it now we have to realize here is that every single word here like isabella is
actually quite a few examples packed in to that single word because what is an existence of a word
like isabella in the data set telling us really it's saying that the character i is a very likely
character to come first in the sequence of a name the character s is likely to come
after i the character a is likely to come after is
the character b is very likely to come after isa and so on all the way to a following isabel
and then there's one more example actually packed in here and that is that after there's isabella
the word is very likely to end so that's one more sort of explicit piece of information that we have here
that we have to be careful with and so there's a lot backed into a single individual word in terms of the
statistical structure of what's likely to follow in these character sequences and then of course we don't have just an
individual word we actually have 32 000 of these and so there's a lot of structure here to model
now in the beginning what i'd like to start with is i'd like to start with building a bi-gram language model
now in the bigram language model we're always working with just two characters at a time
so we're only looking at one character that we are given and we're trying to predict the next character in the
sequence so um what characters are likely to follow are what characters are likely to
follow a and so on and we're just modeling that kind of a little local structure and we're forgetting the fact that we
may have a lot more information we're always just looking at the previous character to predict the next one so
it's a very simple and weak language model but i think it's a great place to start so now let's begin by looking at these
exploring the bigrams in the dataset
bi-grams in our data set and what they look like and these bi-grams again are just two characters in a row
so for w in words each w here is an individual word a string
we want to iterate uh for we're going to iterate this word
with consecutive characters so two characters at a time sliding it through the word now a interesting nice way cute
way to do this in python by the way is doing something like this for character one character two in zip off
w and w at one one column
print character one character two and let's not do all the words let's just do the first three words and i'm
going to show you in a second how this works but for now basically as an example let's just do the very first word alone
emma you see how we have a emma and this will just print e m m m a
and the reason this works is because w is the string emma w at one column is
the string mma and zip takes two iterators and it pairs them up
and then creates an iterator over the tuples of their consecutive entries and if any one of these lists is shorter
than the other then it will just halt and return so basically that's why we return em mmm
ma but then because this iterator second one here runs out of elements zip just
ends and that's why we only get these tuples so pretty cute so these are the consecutive elements in
the first word now we have to be careful because we actually have more information here than just these three
examples as i mentioned we know that e is the is very likely to come first and
we know that a in this case is coming last so one way to do this is basically we're
going to create a special array here all characters
and um we're going to hallucinate a special start token here
i'm going to call it like special start so this is a list of one element
plus w and then plus a special end character
and the reason i'm wrapping the list of w here is because w is a string emma list of w will just have the individual
characters in the list and then doing this again now but not iterating
over w's but over the characters will give us something like this
so e is likely so this is a bigram of the start character and e and this is a bigram of the
a and the special end character and now we can look at for example what this looks like for
olivia or eva and indeed we can actually potentially do this for the entire data
set but we won't print that that's going to be too much but these are the individual character diagrams and we can print them
counting bigrams in a python dictionary
now in order to learn the statistics about which characters are likely to follow other characters the simplest way
in the bigram language models is to simply do it by counting so we're basically just going to count
how often any one of these combinations occurs in the training set in these words
so we're going to need some kind of a dictionary that's going to maintain some counts for every one of these diagrams
so let's use a dictionary b and this will map these bi-grams so
bi-gram is a tuple of character one character two and then b at bi-gram
will be b dot get of bi-gram which is basically the same as b at bigram
but in the case that bigram is not in the dictionary b we would like to by default return to zero
plus one so this will basically add up all the bigrams and count how often they occur
let's get rid of printing or rather let's keep the printing and let's just
inspect what b is in this case and we see that many bi-grams occur just
a single time this one allegedly occurred three times so a was an ending character three times
and that's true for all of these words all of emma olivia and eva and with a
so that's why this occurred three times now let's do it for all the words
oops i should not have printed i'm going to erase that
let's kill this let's just run and now b will have the statistics of
the entire data set so these are the counts across all the words of the individual pie grams
and we could for example look at some of the most common ones and least common ones
this kind of grows in python but the way to do this the simplest way i like is we just use b dot items
b dot items returns the tuples of key value in this case the keys are
the character diagrams and the values are the counts and so then what we want to do is we
want to do sorted of this
but by default sort is on the first
on the first item of a tuple but we want to sort by the values which are the second element of a tuple that is the
key value so we want to use the key equals lambda
that takes the key value and returns the key value at the one not at zero but
at one which is the count so we want to sort by the count of these elements
and actually we wanted to go backwards so here we have is the bi-gram q and r occurs only a single
time dz occurred only a single time and when we sort this the other way around
we're going to see the most likely bigrams so we see that n was very often an ending character
many many times and apparently n almost always follows an a and that's a very likely combination as
well so this is kind of the individual counts
that we achieve over the entire data set now it's actually going to be significantly more convenient for us to
counting bigrams in a 2D torch tensor ("training the model")
keep this information in a two-dimensional array instead of a python dictionary
so we're going to store this information in a 2d array and
the rows are going to be the first character of the bigram and the columns are going to be the second character and
each entry in this two-dimensional array will tell us how often that first character files the second character in
the data set so in particular the array representation that we're going to use or the library is that of pytorch
and pytorch is a deep learning neural network framework but part of it is also this torch.tensor
which allows us to create multi-dimensional arrays and manipulate them very efficiently so
let's import pytorch which you can do by import torch and then we can create
arrays so let's create a array of zeros and we give it a
size of this array let's create a three by five array as an example and
this is a three by five array of zeros and by default you'll notice a.d type
which is short for data type is float32 so these are single precision floating point numbers
because we are going to represent counts let's actually use d type as torch dot and 32
so these are 32-bit integers so now you see that we have integer data
inside this tensor now tensors allow us to really manipulate all the individual entries
and do it very efficiently so for example if we want to change this bit we have to index into the tensor and in
particular here this is the first row and the
because it's zero indexed so this is row index one and column index zero one two
three so a at one comma three we can set that to one
and then a we'll have a 1 over there we can of course also do things like
this so now a will be 2 over there or 3.
and also we can for example say a 0 0 is 5 and then a will have a 5 over here
so that's how we can index into the arrays now of course the array that we are interested in is much much bigger so
for our purposes we have 26 letters of the alphabet and then we have two special characters
s and e so uh we want 26 plus 2 or 28 by 28
array and let's call it the capital n because it's going to represent sort of the counts
let me erase this stuff so that's the array that starts at zeros 28 by 28
and now let's copy paste this here but instead of having a dictionary b
which we're going to erase we now have an n now the problem here is that we have
these characters which are strings but we have to now um basically index into a
um array and we have to index using integers so we need some kind of a lookup table from characters to integers
so let's construct such a character array and the way we're going to do this is we're going to take all the words which
is a list of strings we're going to concatenate all of it into a massive string so this is just simply the entire data set as a single
string we're going to pass this to the set constructor which takes this massive
string and throws out duplicates because sets do not allow duplicates
so set of this will just be the set of all the lowercase characters
and there should be a total of 26 of them and now we actually don't want a set we
want a list but we don't want a list sorted in some weird arbitrary way we want it to be
sorted from a to z so sorted list
so those are our characters now what we want is this lookup table as
i mentioned so let's create a special s2i i will call it
um s is string or character and this will be an s2i mapping
for is in enumerate of these characters
so enumerate basically gives us this iterator over the integer index and the
actual element of the list and then we are mapping the character to the integer
so s2i is a mapping from a to 0 b to 1 etc all the way from z to 25
and that's going to be useful here but we actually also have to specifically set that s will be 26
and s to i at e will be 27 right because z was 25.
so those are the lookups and now we can come here and we can map both character 1 and character 2 to
their integers so this will be s2i at character 1 and ix2 will be s2i of character 2.
and now we should be able to do this line but using our array so n at
x1 ix2 this is the two-dimensional array indexing i've shown you before and honestly just plus equals one
because everything starts at zero so this should work
and give us a large 28 by 28 array of all these counts so
if we print n this is the array but of course it looks ugly so let's erase this ugly mess and
visualizing the bigram tensor
let's try to visualize it a bit more nicer so for that we're going to use a library
called matplotlib so matplotlib allows us to create figures so we can do things like plt
item show of the counter array so this is the 28x28 array
and this is structure but even this i would say is still pretty ugly so we're going to try to create a much
nicer visualization of it and i wrote a bunch of code for that the first thing we're going to need is
we're going to need to invert this array here this dictionary so s2i
is mapping from s to i and in i2s we're going to reverse this dictionary so iterator of all the items
and just reverse that array so i2s maps inversely from 0 to a 1 to b etc
so we'll need that and then here's the code that i came up with to try to make this a little bit
nicer we create a figure we plot
n and then we do and then we visualize a bunch of things later let me just run it so you get a sense of what this is
okay so you see here that we have the array spaced out
and every one of these is basically like b follows g zero times
b follows h 41 times um so a follows j 175 times
and so what you can see that i'm doing here is first i show that entire array
and then i iterate over all the individual little cells here and i create a character string here
which is the inverse mapping i2s of the integer i and the integer j so those are
the bi-grams in a character representation and then i plot just the diagram text
and then i plot the number of times that this bigram occurs now the reason that there's a dot item
here is because when you index into these arrays these are torch tensors
you see that we still get a tensor back so the type of this thing you'd think it
would be just an integer 149 but it's actually a torch.tensor and so if you do dot item then it will pop out
that in individual integer so it will just be 149.
so that's what's happening there and these are just some options to make it look nice so what is the structure of this array
we have all these counts and we see that some of them occur often and some of them do not occur often now if you scrutinize this carefully you
deleting spurious (S) and (E) tokens in favor of a single . token
will notice that we're not actually being very clever that's because when you come over here you'll notice that for example we have
an entire row of completely zeros and that's because the end character is never possibly going to be the first
character of a bi-gram because we're always placing these end tokens all at the end of the diagram
similarly we have entire columns zeros here because the s
character will never possibly be the second element of a bigram because we always start with s and we end with e
and we only have the words in between so we have an entire column of zeros an entire row of zeros and in this little
two by two matrix here as well the only one that can possibly happen is if s directly follows e
that can be non-zero if we have a word that has no letters so in that case
there's no letters in the word it's an empty word and we just have s follows e but the other ones are just not possible
and so we're basically wasting space and not only that but the s and the e are getting very crowded here
i was using these brackets because there's convention and natural language processing to use these kinds of brackets to denote special tokens
but we're going to use something else so let's fix all this and make it prettier
we're not actually going to have two special tokens we're only going to have one special token so
we're going to have n by n array of 27 by 27 instead
instead of having two we will just have one and i will call it a dot
okay let me swing this over here
now one more thing that i would like to do is i would actually like to make this special character half position zero
and i would like to offset all the other letters off i find that a little bit more pleasing
so we need a plus one here so that the first character which is a will start at
one so s2i will now be a starts at one and dot is 0
and i2s of course we're not changing this because i2s just creates a reverse mapping and this will work fine so 1 is
a 2 is b 0 is dot so we've reversed that here
we have a dot and a dot this should work fine
make sure i start at zeros count and then here we don't go up to 28 we go
up to 27 and this should just work
okay so we see that dot never happened it's at zero because we don't have empty words
then this row here now is just uh very simply the um counts for all the first letters so
uh j starts a word h starts a word i starts a word etc and then these are all
the ending characters and in between we have the structure of what characters follow each other
so this is the counts array of our entire data set so this array actually has all
sampling from the model
the information necessary for us to actually sample from this bigram uh character level language model
and um roughly speaking what we're going to do is we're just going to start following these probabilities and these
counts and we're going to start sampling from the from the model so in the beginning of course
we start with the dot the start token dot so to sample the first character of a
name we're looking at this row here so we see that we have the counts and
those concepts terminally are telling us how often any one of these characters is to start a word
so if we take this n and we grab the first row
we can do that by using just indexing as zero and then using this notation column for
the rest of that row so n zero colon is indexing into the zeroth
row and then it's grabbing all the columns and so this will give us a one-dimensional array
of the first row so zero four four ten you know zero four four ten one three oh
six one five four two etc it's just the first row the shape of this is 27 it's just the row of 27
and the other way that you can do this also is you just you don't need to actually give this you just grab the zeroth row like this
this is equivalent now these are the counts and now what we'd like to do is we'd
like to basically um sample from this since these are the raw counts we actually have to convert this to
probabilities so we create a probability vector
so we'll take n of zero and we'll actually convert this to float
first okay so these integers are converted to float floating point numbers and the reason
we're creating floats is because we're about to normalize these counts so to create a probability distribution
here we want to divide we basically want to do p p p divide p
that sum and now we get a vector of smaller
numbers and these are now probabilities so of course because we divided by the sum the sum of p now is 1.
so this is a nice proper probability distribution it sums to 1 and this is giving us the probability for any single
character to be the first character of a word so now we can try to sample from this
distribution to sample from these distributions we're going to use storch.multinomial which i've pulled up
here so torch.multinomial returns uh
samples from the multinomial probability distribution which is a complicated way of saying you give me probabilities and
i will give you integers which are sampled according to the property distribution
so this is the signature of the method and to make everything deterministic we're going to use a generator object in
pytorch so this makes everything deterministic so when you run this on your computer
you're going to the exact get the exact same results that i'm getting here on my computer so let me show you how this works
here's the deterministic way of creating a torch generator object
seeding it with some number that we can agree on so that seeds a generator gets gives us
an object g and then we can pass that g to a function that creates um
here random numbers twerk.rand creates random numbers three of them
and it's using this generator object to as a source of randomness
so without normalizing it i can just print
this is sort of like numbers between 0 and 1 that are random according to this thing and whenever i run it again
i'm always going to get the same result because i keep using the same generator object which i'm seeing here
and then if i divide to normalize i'm going to get a nice
probability distribution of just three elements and then we can use torsion multinomial
to draw samples from it so this is what that looks like tertiary multinomial we'll take the
torch tensor of probability distributions then we can ask for a number of samples
let's say 20. replacement equals true means that when we draw an element
we will uh we can draw it and then we can put it back into the list of eligible indices to draw again
and we have to specify replacement as true because by default uh for some reason it's false
and i think you know it's just something to be careful with and the generator is passed in here so
we're going to always get deterministic results the same results so if i run these two
we're going to get a bunch of samples from this distribution now you'll notice here that the
probability for the first element in this tensor is 60
so in these 20 samples we'd expect 60 of them to be zero
we'd expect thirty percent of them to be one and because the the element index two
has only ten percent probability very few of these samples should be two and indeed we only have a small number of
twos and we can sample as many as we'd like and the more we sample the more
these numbers should um roughly have the distribution here so we should have lots of zeros
half as many um ones and we should have um three times
as few oh sorry s few ones and three times as few uh
twos so you see that we have very few twos we have some ones and most of them are zero
so that's what torsion multinomial is doing for us here
we are interested in this row we've created this p here
and now we can sample from it so if we use the same seed
and then we sample from this distribution let's just get one sample
then we see that the sample is say 13. so this will be the index
and let's you see how it's a tensor that wraps 13 we again have to use that item
to pop out that integer and now index would be just the number 13.
and of course the um we can do we can map the i2s of ix to figure out
exactly which character we're sampling here we're sampling m so we're saying that the first character
is in our generation and just looking at the road here
m was drawn and you we can see that m actually starts a large number of words uh m
started 2 500 words out of 32 000 words so almost
a bit less than 10 percent of the words start with them so this was actually a fairly likely character to draw
um so that would be the first character of our work and now we can continue to sample more characters because now we
know that m started m is already sampled so now to draw the next character we
will come back here and we will look for the row that starts with m
so you see m and we have a row here so we see that m dot is
516 m a is this many and b is this many etc so these are the counts for the next
row and that's the next character that we are going to now generate so i think we are ready to actually just
write out the loop because i think you're starting to get a sense of how this is going to go the um
we always begin at index 0 because that's the start token
and then while true we're going to grab the row corresponding to index
that we're currently on so that's p so that's n array at ix
converted to float is rp then we normalize
this p to sum to one i accidentally ran the infinite loop we
normalize p to something one then we need this generator object
now we're going to initialize up here and we're going to draw a single sample from this distribution
and then this is going to tell us what index is going to be next
if the index sampled is 0 then that's now the end token
so we will break otherwise we are going to print
s2i of ix i2s
and uh that's pretty much it we're just uh this should work okay more
so that's that's the name that we've sampled we started with m the next step was o then r and then dot
and this dot we it here as well so
let's now do this a few times so let's actually create an
out list here and instead of printing we're going to
append so out that append this character
and then here let's just print it at the end so let's just join up all the outs and we're just going to print more okay
now we're always getting the same result because of the generator so if we want to do this a few times we
can go for i in range 10 we can sample 10 names
and we can just do that 10 times and these are the names that we're getting out
let's do 20.
i'll be honest with you this doesn't look right so i started a few minutes to convince myself that it actually is right
the reason these samples are so terrible is that bigram language model is actually look just like really
terrible we can generate a few more here and you can see that they're kind of like their name like a little bit like
yanu o'reilly etc but they're just like totally messed up um
and i mean the reason that this is so bad like we're generating h as a name but you have to think through
it from the model's eyes it doesn't know that this h is the very first h all it
knows is that h was previously and now how likely is h the last character well
it's somewhat likely and so it just makes it last character it doesn't know that there were other things before it or there
were not other things before it and so that's why it's generating all these like nonsense names
another way to do this is to convince yourself that this is actually doing something reasonable even
though it's so terrible is these little piece here are 27 right
like 27. so how about if we did something like this
instead of p having any structure whatsoever how about if p was just torch dot once
of 27 by default this is a float 32 so this is fine divide 27
so what i'm doing here is this is the uniform distribution which will make everything equally likely
and we can sample from that so let's see if that does any better okay so it's
this is what you have from a model that is completely untrained where everything is equally likely so it's obviously
garbage and then if we have a trained model which is trained on just bi-grams
this is what we get so you can see that it is more name-like it is actually working it's just um
my gram is so terrible and we have to do better now next i would like to fix an inefficiency that we have going on here
efficiency! vectorized normalization of the rows, tensor broadcasting
because what we're doing here is we're always fetching a row of n from the counts matrix up ahead
and then we're always doing the same things we're converting to float and we're dividing and we're doing this every single iteration of this loop and
we just keep renormalizing these rows over and over again and it's extremely inefficient and wasteful so what i'd like to do is i'd like to actually
prepare a matrix capital p that will just have the probabilities in it so in
other words it's going to be the same as the capital n matrix here of counts but every single row will have the row of
probabilities uh that is normalized to 1 indicating the probability distribution for the next character given the
character before it um as defined by which row we're in so basically what we'd like to do is
we'd like to just do it up front here and then we would like to just use that row here so here we would like to just
do p equals p of ix instead okay
the other reason i want to do this is not just for efficiency but also i would like us to practice these n-dimensional tensors and i'd like
us to practice their manipulation and especially something that's called broadcasting that we'll go into in a second
we're actually going to have to become very good at these tensor manipulations because if we're going to build out all
the way to transformers we're going to be doing some pretty complicated um array operations for efficiency and we
need to really understand that and be very good at it so intuitively what we want to do is we
first want to grab the floating point copy of n and i'm mimicking the line here
basically and then we want to divide all the rows so that they sum to 1.
so we'd like to do something like this p divide p dot sum but
now we have to be careful because p dot sum actually produces a sum
sorry equals and that float copy p dot sum produces a um
sums up all of the counts of this entire matrix n and gives us a single number of just the summation of everything so
that's not the way we want to define divide we want to simultaneously and in parallel divide all the rows
by their respective sums so what we have to do now is we have to
go into documentation for torch.sum and we can scroll down here to a definition that is relevant to us which
is where we don't only provide an input array that we want to sum but we also provide the dimension along which we
want to sum and in particular we want to sum up over rows
right now one more argument that i want you to pay attention to here is the keep them
is false if keep them is true then the output tensor is of the same size as input
except of course the dimension along which is summed which will become just one but if you pass in keep them as false
then this dimension is squeezed out and so torch.sum not only does the sum and collapses dimension to be of size one
but in addition it does what's called a squeeze where it squeezes out it squeezes out that dimension
so basically what we want here is we instead want to do p dot sum of some axis
and in particular notice that p dot shape is 27 by 27 so when we sum up across axis zero then
we would be taking the zeroth dimension and we would be summing across it so when keep them as true
then this thing will not only give us the counts across um
along the columns but notice that basically the shape of this is 1 by 27 we just get a row vector
and the reason we get a row vector here again is because we passed in zero dimension so this zero dimension becomes
one and we've done a sum and we get a row and so basically we've done the sum
this way vertically and arrived at just a single 1 by 27 vector of counts
what happens when you take out keep them is that we just get 27. so it squeezes
out that dimension and we just get a one-dimensional vector of size 27.
now we don't actually want one by 27 row vector because that gives
us the counts or the sums across the columns
we actually want to sum the other way along dimension one and you'll see that the shape of this is 27 by one so it's a
column vector it's a 27 by one vector of counts
okay and that's because what's happened here is that we're going horizontally and this 27 by 27 matrix becomes a 27 by 1
array now you'll notice by the way that um the actual numbers
of these counts are identical and that's because this special array of counts here comes from bi-gram
statistics and actually it just so happens by chance or because of the way this array is
constructed that the sums along the columns or along the rows horizontally or vertically is identical
but actually what we want to do in this case is we want to sum across the rows
horizontally so what we want here is p that sum of one with keep in true
27 by one column vector and now what we want to do is we want to divide by that
now we have to be careful here again is it possible to take what's a um p dot shape you see here 27
by 27 is it possible to take a 27 by 27 array and divide it by what is a 27 by 1
array is that an operation that you can do and whether or not you can perform this
operation is determined by what's called broadcasting rules so if you just search broadcasting semantics in torch
you'll notice that there's a special definition for what's called broadcasting that uh for whether or not um these two uh arrays
can be combined in a binary operation like division so the first condition is each tensor
has at least one dimension which is the case for us and then when iterating over the dimension sizes starting at the trailing
dimension the dimension sizes must either be equal one of them is one or one of them does not exist
okay so let's do that we need to align the two arrays and their shapes which is
very easy because both of these shapes have two elements so they're aligned then we iterate over from the from the
right and going to the left each dimension must be either equal one
of them is a one or one of them does not exist so in this case they're not equal but one of them is a one so this is fine
and then this dimension they're both equal so uh this is fine so all the dimensions are fine and
therefore the this operation is broadcastable so that means that this operation is allowed
and what is it that these arrays do when you divide 27 by 27 by 27 by one
what it does is that it takes this dimension one and it stretches it out it copies it to match
27 here in this case so in our case it takes this column vector which is 27 by 1
and it copies it 27 times to make these both be 27 by 27 internally you
can think of it that way and so it copies those counts and then it does an element-wise division
which is what we want because these counts we want to divide by them on every single one of these columns in
this matrix so this actually we expect will normalize every single row
and we can check that this is true by taking the first row for example and taking its sum we expect this to be
1. because it's not normalized and then we expect this now because if
we actually correctly normalize all the rows we expect to get the exact same result here so let's run this
it's the exact same result this is correct so now i would like to scare you a little bit
uh you actually have to like i basically encourage you very strongly to read through broadcasting semantics
and i encourage you to treat this with respect and it's not something to play fast and loose with it's something to
really respect really understand and look up maybe some tutorials for broadcasting and practice it and be careful with it because you can very
quickly run into books let me show you what i mean you see how here we have p dot sum of
one keep them as true the shape of this is 27 by one let me take out this line just so we have the n
and then we can see the counts we can see that this is a all the counts across all the
rows and it's a 27 by one column vector right now suppose that i tried to do the
following but i erase keep them just true here what does that do if keep them is not
true it's false then remember according to documentation it gets rid of this dimension one it squeezes it out so
basically we just get all the same counts the same result except the shape of it is not 27 by 1 it is just 27 the
one disappears but all the counts are the same so you'd think that this divide that
would uh would work first of all can we even uh write this and will it is it even is it even
expected to run is it broadcastable let's determine if this result is broadcastable p.summit one is shape
is 27. this is 27 by 27. so 27 by 27
broadcasting into 27. so now rules of broadcasting number one align
all the dimensions on the right done now iteration over all the dimensions starting from the right going to the
left all the dimensions must either be equal one of them must be one or one that does
not exist so here they are all equal here the dimension does not exist so internally what broadcasting will do
is it will create a one here and then we see that one of them is a one and
this will get copied and this will run this will broadcast okay so you'd expect this
to work because we we are
this broadcast and this we can divide this now if i run this you'd expect it to work but
it doesn't uh you actually get garbage you get a wrong dissolve because this is actually a bug
this keep them equals true makes it work
this is a bug in both cases we are doing the correct counts we are summing up
across the rows but keep them is saving us and making it work so in this case
i'd like to encourage you to potentially like pause this video at this point and try to think about why this is buggy and
why the keep dim was necessary here okay so the reason to do
for this is i'm trying to hint it here when i was sort of giving you a bit of a hint on how this works
this 27 vector internally inside the broadcasting this becomes a 1 by 27
and 1 by 27 is a row vector right and now we are dividing 27 by 27 by 1 by
27 and torch will replicate this dimension so basically
uh it will take it will take this row vector and it will copy it
vertically now 27 times so the 27 by 27 lies exactly and element wise divides
and so basically what's happening here is we're actually normalizing the columns
instead of normalizing the rows so you can check that what's happening
here is that p at zero which is the first row of p dot sum is not one it's seven
it is the first column as an example that sums to one
so to summarize where does the issue come from the issue comes from the silent adding of a dimension here because in
broadcasting rules you align on the right and go from right to left and if dimension doesn't exist you create it
so that's where the problem happens we still did the counts correctly we did the counts across the rows and we got
the the counts on the right here as a column vector but because the keep things was true this this uh this
dimension was discarded and now we just have a vector of 27. and because of broadcasting the way it works this
vector of 27 suddenly becomes a row vector and then this row vector gets replicated vertically and that every single point
we are dividing by the by the count in the opposite direction
so uh so this thing just uh doesn't work this needs to be keep things equal true in
this case so then then we have that p at zero is normalized
and conversely the first column you'd expect to potentially not be normalized and this is what makes it work
so pretty subtle and uh hopefully this helps to scare you that you should have
a respect for broadcasting be careful check your work uh and uh understand how it works under the hood and make sure
that it's broadcasting in the direction that you like otherwise you're going to introduce very subtle bugs very hard to
find bugs and uh just be careful one more note on efficiency we don't want to be doing this here because this creates
a completely new tensor that we store into p we prefer to use in place operations if possible
so this would be an in-place operation it has the potential to be faster it doesn't create new memory
under the hood and then let's erase this we don't need it and let's
also um just do fewer just so i'm not wasting space
loss function (the negative log likelihood of the data under our model)
okay so we're actually in a pretty good spot now we trained a bigram language model and we trained it really just by counting uh
how frequently any pairing occurs and then normalizing so that we get a nice property distribution
so really these elements of this array p are really the parameters of our biogram language model giving us and summarizing
the statistics of these bigrams so we train the model and then we know how to sample from a model we just
iteratively uh sample the next character and feed it in each time and get a next
character now what i'd like to do is i'd like to somehow evaluate the quality of this model we'd like to somehow summarize the
quality of this model into a single number how good is it at predicting the training set
and as an example so in the training set we can evaluate now the training loss
and this training loss is telling us about sort of the quality of this model in a single number just like we saw in
micrograd so let's try to think through the quality of the model and how we would evaluate it
basically what we're going to do is we're going to copy paste this code that we previously used for counting
okay and let me just print these diagrams first we're gonna use f strings and i'm gonna print character one
followed by character two these are the diagrams and then i don't wanna do it for all the words just do the first three words so here we have emma olivia
and ava bigrams now what we'd like to do is we'd like to basically look at the probability that
the model assigns to every one of these diagrams so in other words we can look at the probability which is
summarized in the matrix b of i x 1 x 2 and then we can print it here
as probability and because these properties are way too large let me present
or call in 0.4 f to like truncate it a bit so what do we have here right we're
looking at the probabilities that the model assigns to every one of these bigrams in the dataset and so we can see some of them are four
percent three percent etc just to have a measuring stick in our mind by the way um we have 27 possible
characters or tokens and if everything was equally likely then you'd expect all these probabilities
to be four percent roughly so anything above four percent means
that we've learned something useful from these bigram statistics and you see that roughly some of these are four percent
but some of them are as high as 40 percent 35 percent and so on so you see that the model actually assigned a pretty high
probability to whatever's in the training set and so that's a good thing um basically if you have a very good
model you'd expect that these probabilities should be near one because that means that your model is correctly
predicting what's going to come next especially on the training set where you where you trained your model
so now we'd like to think about how can we summarize these probabilities into a single number that measures the quality
of this model now when you look at the literature into maximum likelihood estimation and
statistical modeling and so on you'll see that what's typically used here is something called the likelihood
and the likelihood is the product of all of these probabilities and so the product of all these
probabilities is the likelihood and it's really telling us about the probability of the entire data set assigned uh
assigned by the model that we've trained and that is a measure of quality so the product of these
should be as high as possible when you are training the model and when you have a good model your pro your
product of these probabilities should be very high um now because the product of these probabilities is an unwieldy thing to
work with you can see that all of them are between zero and one so your product of these probabilities will be a very tiny number
um so for convenience what people work with usually is not the likelihood but they work with what's called the log
likelihood so the product of these is the likelihood to get the log likelihood we just have
to take the log of the probability and so the log of the probability here i have the log of x from zero to one
the log is a you see here monotonic transformation of the probability where if you pass in one
you get zero so probability one gets your log probability of zero and then as you go lower and lower
probability the log will grow more and more negative until all the way to negative infinity at zero
so here we have a log prob which is really just a torch.log of probability let's print it out to get a sense of
what that looks like log prob also 0.4 f
okay so as you can see when we plug in numbers that are very close some of our
higher numbers we get closer and closer to zero and then if we plug in very bad probabilities we get more and more
negative number that's bad so and the reason we work with this is for
a large extent convenience right because we have mathematically that if you have some product a times b times c
of all these probabilities right the likelihood is the product of all these probabilities
then the log of these is just log of a plus
log of b plus log of c if you remember your logs
from your high school or undergrad and so on so we have that basically
the likelihood of the product probabilities the log likelihood is just the sum of the logs of the individual
probabilities so log likelihood
starts at zero and then log likelihood here we can just accumulate simply
and in the end we can print this
print the log likelihood f strings
maybe you're familiar with this so log likelihood is negative 38.
okay now we actually want um
so how high can log likelihood get it can go to zero so when all the
probabilities are one log likelihood will be zero and then when all the probabilities are lower this will grow
more and more negative now we don't actually like this because what we'd like is a loss function and a
loss function has the semantics that low is good because we're trying to minimize the
loss so we actually need to invert this and that's what gives us something called the negative log likelihood
negative log likelihood is just negative of the log likelihood
these are f strings by the way if you'd like to look this up negative log likelihood equals
so negative log likelihood now is just negative of it and so the negative log block load is a very nice loss function
because um the lowest it can get is zero and the higher it is the worse off the
predictions are that you're making and then one more modification to this that sometimes people do is that for
convenience uh they actually like to normalize by they like to make it an average instead of a sum
and so uh here let's just keep some counts as well so n plus equals one
starts at zero and then here um we can have sort of like a normalized log likelihood
um if we just normalize it by the count then we will sort of get the average
log likelihood so this would be usually our loss function here is what this we would this is what we would use
uh so our loss function for the training set assigned by the model is 2.4 that's the quality of this model
and the lower it is the better off we are and the higher it is the worse off we are and
the job of our you know training is to find the parameters that minimize the
negative log likelihood loss and that would be like a high quality
model okay so to summarize i actually wrote it out here so our goal is to maximize likelihood
which is the product of all the probabilities assigned by the model and we want to maximize this likelihood
with respect to the model parameters and in our case the model parameters here are defined in the table these numbers
the probabilities are the model parameters sort of in our program language models so far but you
have to keep in mind that here we are storing everything in a table format the probabilities but what's coming up as a
brief preview is that these numbers will not be kept explicitly but these numbers will be calculated by a neural network
so that's coming up and we want to change and tune the parameters of these neural networks we
want to change these parameters to maximize the likelihood the product of the probabilities
now maximizing the likelihood is equivalent to maximizing the log likelihood because log is a monotonic
function here's the graph of log and basically all it is doing is it's
just scaling your um you can look at it as just a scaling of the loss function
and so the optimization problem here and here are actually equivalent because
this is just scaling you can look at it that way and so these are two identical optimization problems
um maximizing the log-likelihood is equivalent to minimizing the negative log likelihood and then in practice
people actually minimize the average negative log likelihood to get numbers like 2.4
and then this summarizes the quality of your model and we'd like to minimize it and make it as small as possible
and the lowest it can get is zero and the lower it is the better off your model is because
it's signing it's assigning high probabilities to your data now let's estimate the probability over the entire
training set just to make sure that we get something around 2.4 let's run this over the entire oops
let's take out the print segment as well okay 2.45 or the entire training set
now what i'd like to show you is that you can actually evaluate the probability for any word that you want like for example
if we just test a single word andre and bring back the print statement
then you see that andre is actually kind of like an unlikely word like on average we take
three log probability to represent it and roughly that's because ej apparently is very uncommon as an example
model smoothing with fake counts
now think through this um when i take andre and i append q and i
test the probability of it under q we actually get
infinity and that's because jq has a zero percent probability according to our model so
the log likelihood so the log of zero will be negative infinity we get infinite loss
so this is kind of undesirable right because we plugged in a string that could be like a somewhat reasonable name but basically what this is saying is
that this model is exactly zero percent likely to uh to predict this name
and our loss is infinity on this example and really what the reason for that is
that j is followed by q uh zero times
uh where's q jq is zero and so jq is uh zero percent likely
so it's actually kind of gross and people don't like this too much to fix this there's a very simple fix that people like to do to sort of like smooth
out your model a little bit and it's called model smoothing and roughly what's happening is that we will eight we will add some fake counts
so imagine adding a count of one to everything so we add a count of one
like this and then we recalculate the probabilities and that's model smoothing and you can
add as much as you like you can add five and it will give you a smoother model and the more you add here
the more uniform model you're going to have and the less you add the more peaked model you are going to
have of course so one is like a pretty decent count to add and that will ensure that there will be
no zeros in our probability matrix p and so this will of course change the generations a little bit in this case it
didn't but in principle it could but what that's going to do now is that nothing will be infinity unlikely
so now our model will predict some other probability and we see that jq now has a very small probability so the model
still finds it very surprising that this was a word or a bigram but we don't get negative infinity so it's kind of like a
nice fix that people like to apply sometimes and it's called model smoothing okay so we've now trained a respectable bi-gram character level
PART 2: the neural network approach: intro
language model and we saw that we both sort of trained the model by looking at
the counts of all the bigrams and normalizing the rows to get probability distributions
we saw that we can also then use those parameters of this model to perform
sampling of new words so we sample new names according to those distributions and we also saw that
we can evaluate the quality of this model and the quality of this model is summarized in a single number which is
the negative log likelihood and the lower this number is the better the model is because it is giving high probabilities
to the actual next characters in all the bi-grams in our training set so that's all well and good but we've
arrived at this model explicitly by doing something that felt sensible we were just performing counts and then we
were normalizing those counts now what i would like to do is i would like to take an alternative approach we
will end up in a very very similar position but the approach will look very different because i would like to cast
the problem of bi-gram character level language modeling into the neural network framework in the neural network framework we're
going to approach things slightly differently but again end up in a very similar spot i'll go into that later now
our neural network is going to be a still a background character level language model so it receives a single
character as an input then there's neural network with some weights or some parameters w
and it's going to output the probability distribution over the next character in a sequence it's going to make guesses as
to what is likely to follow this character that was input to the model
and then in addition to that we're going to be able to evaluate any setting of the parameters of the neural net because
we have the loss function the negative log likelihood so we're going to take a look at its probability distributions and we're going to use the
labels which are basically just the identity of the next character in that diagram the
second character so knowing what second character actually comes next in the bigram allows
us to then look at what how high of probability the model assigns to that character
and then we of course want the probability to be very high and that is another way of saying that the loss is low
so we're going to use gradient-based optimization then to tune the parameters of this network because we have the loss
function and we're going to minimize it so we're going to tune the weights so that the neural net is correctly
predicting the probabilities for the next character so let's get started the first thing i want to do is i want to compile the
creating the bigram dataset for the neural net
training set of this neural network right so create the training set
of all the bigrams okay and
here i'm going to copy paste this code because this code iterates over all the
programs so here we start with the words we iterate over all the bygrams and
previously as you recall we did the counts but now we're not going to do counts we're just creating a training
set now this training set will be made up of two lists
we have the inputs and the targets
the the labels and these bi-grams will denote x y those are the characters right
and so we're given the first character of the bi-gram and then we're trying to predict the next one both of these are going to be integers
so here we'll take x's that append is just x1 ystat append ix2
and then here we actually don't want lists of integers we will create tensors out of these so
axis is torch.tensor of axis and wise a storage.tensor of ys
and then we don't actually want to take all the words just yet because i want everything to be manageable
so let's just do the first word which is emma and then it's clear what these x's and
y's would be here let me print character 1 character 2 just so you see
what's going on here so the bigrams of these characters is
dot e e m m m a a dot so this single word as i mentioned has one two three
four five examples for our neural network there are five separate examples in emma
and those examples are summarized here when the input to the neural network is integer 0
the desired label is integer 5 which corresponds to e when the input to the
neural network is 5 we want its weights to be arranged so that 13 gets a very high probability
when 13 is put in we want 13 to have a high probability when 13 is put in we also want 1 to have
a high probability when one is input we want zero to have a very high probability so there are five
separate input examples to a neural nut in this data set
i wanted to add a tangent of a node of caution to be careful with a lot of the apis of some of these frameworks
you saw me silently use torch.tensor with a lowercase t and the output looked right
but you should be aware that there's actually two ways of constructing a tensor there's a torch.lowercase tensor
and there's also a torch.capital tensor class which you can also construct so you can actually call both you can
also do torch.capital tensor and you get a nexus and wise as well
so that's not confusing at all um there are threads on what is the difference between these two
and um unfortunately the docs are just like not clear on the difference and when you look at the the docs of lower case
tensor construct tensor with no autograd history by copying data it's just like it doesn't
it doesn't make sense so the actual difference as far as i can tell is explained eventually in this random thread that you can google
and really it comes down to i believe that um
what is this torch.tensor in first d-type the data type automatically while torch.tensor
just returns a float tensor i would recommend stick to torch.lowercase tensor
so um indeed we see that when i construct this with a capital t the data
type here of xs is float32 but towards that lowercase tensor
you see how it's now x dot d type is now integer
so um it's advised that you use lowercase t and you can read more about it if you
like in some of these threads but basically um i'm pointing out some of these things
because i want to caution you and i want you to re get used to reading a lot of documentation and reading through a lot
of q and a's and threads like this and you know some of the stuff is
unfortunately not easy and not very well documented and you have to be careful out there what we want here is integers
because that's what makes uh sense um and so lowercase tensor is what we are using
feeding integers into neural nets? one-hot encodings
okay now we want to think through how we're going to feed in these examples into a neural network
now it's not quite as straightforward as plugging it in because these examples right now are integers so there's like a
0 5 or 13 it gives us the index of the character and you can't just plug an integer index into a neural net
these neural nets right are sort of made up of these neurons and
these neurons have weights and as you saw in micrograd these weights act multiplicatively on the inputs w x plus
b there's 10 h's and so on and so it doesn't really make sense to make an input neuron take on integer values that
you feed in and then multiply on with weights so instead a common way of encoding integers is
what's called one hot encoding in one hot encoding we take an integer like 13 and we create
a vector that is all zeros except for the 13th dimension which we turn to a
one and then that vector can feed into a neural net now conveniently
uh pi torch actually has something called the one hot function inside torching and functional
it takes a tensor made up of integers um long is a is a as an integer
um and it also takes a number of classes um which is how large you want your uh
tensor uh your vector to be so here let's import
torch.n.functional sf this is a common way of importing it and then let's do f.1 hot
and we feed in the integers that we want to encode so we can actually feed in the entire array of x's
and we can tell it that num classes is 27. so it doesn't have to try to guess it it
may have guessed that it's only 13 and would give us an incorrect result
so this is the one hot let's call this x inc for x encoded
and then we see that x encoded that shape is 5 by 27
and uh we can also visualize it plt.i am show of x inc
to make it a little bit more clear because this is a little messy so we see that we've encoded all the five examples uh into vectors we have
five examples so we have five rows and each row here is now an example into a neural nut
and we see that the appropriate bit is turned on as a one and everything else is zero
so um here for example the zeroth bit is turned on the fifth bit is turned on
13th bits are turned on for both of these examples and then the first bit here is turned on
so that's how we can encode integers into vectors and then these
vectors can feed in to neural nets one more issue to be careful with here by the way is
let's look at the data type of encoding we always want to be careful with data types what would you expect x encoding's data
type to be when we're plugging numbers into neural nuts we don't want them to be integers we want them to be floating point
numbers that can take on various values but the d type here is actually 64-bit
integer and the reason for that i suspect is that one hot received a 64-bit integer
here and it returned the same data type and when you look at the signature of one hot it doesn't even take a d type a
desired data type of the output tensor and so we can't in a lot of functions in
torch we'd be able to do something like d type equal storage.float32 which is what we want but one heart does
not support that so instead we're going to want to cast this to float like this
so that these everything is the same everything looks the same but the d-type
is float32 and floats can feed into neural nets so now let's construct our
the "neural net": one linear layer of neurons implemented with matrix multiplication
first neuron this neuron will look at these input vectors
and as you remember from micrograd these neurons basically perform a very simple function w x plus b where w x is a dot
product right so we can achieve the same thing here let's first define the weights of this
neuron basically what are the initial weights at initialization for this neuron let's initialize them with torch.rendin
torch.rendin is um fills a tensor with random numbers
drawn from a normal distribution and a normal distribution has a probability density function like this
and so most of the numbers drawn from this distribution will be around 0 but some of them will be as high as
almost three and so on and very few numbers will be above three in magnitude
so we need to take a size as an input here and i'm going to use size as to be 27 by
one so 27 by one and then let's visualize w so
w is a column vector of 27 numbers and
these weights are then multiplied by the inputs so now to perform this multiplication we
can take x encoding and we can multiply it with w this is a matrix multiplication operator
in pi torch and the output of this operation is five by one
the reason is five by five is the following we took x encoding which is five by twenty seven and we multiplied it by
twenty seven by one and in matrix multiplication
you see that the output will become five by one because these 27
will multiply and add so basically what we're seeing here outs
out of this operation is we are seeing the five activations
of this neuron on these five inputs and we've evaluated all of them in
parallel we didn't feed in just a single input to the single neuron we fed in simultaneously all the five inputs into
the same neuron and in parallel patrol has evaluated the wx plus b but here is just the wx
there's no bias it has value w times x for all of them
independently now instead of a single neuron though i would like to have 27 neurons and i'll show you in a second
why i want 27 neurons so instead of having just a 1 here which is indicating this presence of one
single neuron we can use 27 and then when w is 27 by 27
this will in parallel evaluate all the 27 neurons on all the 5 inputs
giving us a much better much much bigger result so now what we've done is 5 by 27 multiplied 27 by 27
and the output of this is now 5 by 27 so we can see that the shape of this
is 5 by 27. so what is every element here telling us right
it's telling us for every one of 27 neurons that we created
what is the firing rate of those neurons on every one of those five examples
so the element for example 3 comma 13
is giving us the firing rate of the 13th neuron looking at the third input
and the way this was achieved is by a dot product between the third
input and the 13th column of this w matrix here
okay so using matrix multiplication we can very efficiently evaluate
the dot product between lots of input examples in a batch and lots of neurons where all those
neurons have weights in the columns of those w's and in matrix multiplication we're just doing those dot products and
in parallel just to show you that this is the case we can take x and we can take the third
row and we can take the w and take its 13th column
and then we can do x and get three elementwise multiply with w at 13.
and sum that up that's wx plus b well there's no plus b it's just wx dot
product and that's this number so you see that this is just being done efficiently by the matrix multiplication
operation for all the input examples and for all the output neurons of this first layer
transforming neural net outputs into probabilities: the softmax
okay so we fed our 27-dimensional inputs into a first layer of a neural net that
has 27 neurons right so we have 27 inputs and now we have 27 neurons these
neurons perform w times x they don't have a bias and they don't have a non-linearity like 10 h we're going to
leave them to be a linear layer in addition to that we're not going to have any other layers this is going to
be it it's just going to be the dumbest smallest simplest neural net which is just a single linear layer
and now i'd like to explain what i want those 27 outputs to be intuitively what we're trying to produce
here for every single input example is we're trying to produce some kind of a probability distribution for the next
character in a sequence and there's 27 of them but we have to come up with like precise
semantics for exactly how we're going to interpret these 27 numbers that these neurons take on
now intuitively you see here that these numbers are negative and some of them are positive etc
and that's because these are coming out of a neural net layer initialized with these
normal distribution parameters but what we want is we want something like we had here
like each row here told us the counts and then we normalized the counts to get probabilities and we want something
similar to come out of the neural net but what we just have right now is just some negative and positive numbers
now we want those numbers to somehow represent the probabilities for the next character but you see that probabilities they they
have a special structure they um they're positive numbers and they sum to one
and so that doesn't just come out of a neural net and then they can't be counts because these counts are positive and
counts are integers so counts are also not really a good thing to output from a neural net
so instead what the neural net is going to output and how we are going to interpret the um
the 27 numbers is that these 27 numbers are giving us log counts
basically um so instead of giving us counts directly like in this table they're giving us log
counts and to get the counts we're going to take the log counts and we're going to exponentiate them
now exponentiation takes the following form
it takes numbers that are negative or they are positive it takes the entire real line
and then if you plug in negative numbers you're going to get e to the x which is uh always below one
so you're getting numbers lower than one and if you plug in numbers greater than zero you're getting numbers greater than
one all the way growing to the infinity and this here grows to zero
so basically we're going to take these numbers here
and instead of them being positive and negative and all over the place we're
going to interpret them as log counts and then we're going to element wise exponentiate these numbers
exponentiating them now gives us something like this and you see that these numbers now because they went through an exponent
all the negative numbers turned into numbers below 1 like 0.338 and all the
positive numbers originally turned into even more positive numbers sort of greater than one
so like for example seven is some positive number over here
that is greater than zero but exponentiated outputs here
basically give us something that we can use and interpret as the equivalent of counts originally so you see these
counts here 112 7 51 1 etc the neural net is kind of now predicting
uh counts and these counts are positive numbers
they can never be below zero so that makes sense and uh they can now take on various values
depending on the settings of w so let me break this down
we're going to interpret these to be the log counts
in other words for this that is often used is so-called logits these are logits log counts
then these will be sort of the counts largest exponentiated and this is equivalent to the n matrix
sort of the n array that we used previously remember this was the n
this is the the array of counts and each row here are the counts for the
for the um next character sort of so those are the counts and now the
probabilities are just the counts um normalized and so um
i'm not going to find the same but basically i'm not going to scroll all over the place we've already done this we want to
counts that sum along the first dimension and we want to keep them as true
we've went over this and this is how we normalize the rows of our counts matrix
to get our probabilities props so now these are the probabilities
and these are the counts that we ask currently and now when i show the probabilities
you see that um every row here of course
will sum to 1 because they're normalized and the shape of this
is 5 by 27 and so really what we've achieved is for every one of our five examples
we now have a row that came out of a neural net and because of the transformations here
we made sure that this output of this neural net now are probabilities or we can interpret to be probabilities
so our wx here gave us logits and then we interpret those to be log
counts we exponentiate to get something that looks like counts and then we normalize those counts to
get a probability distribution and all of these are differentiable operations so what we've done now is we're taking
inputs we have differentiable operations that we can back propagate through and we're getting out probability
distributions so for example for the zeroth example that
fed in right which was um the zeroth example here was a one-half
vector of zero and um it basically corresponded to feeding in
this example here so we're feeding in a dot into a neural net and the way we fed the dot into a neural net is that we
first got its index then we one hot encoded it then it went into the neural net and out
came this distribution of probabilities and its shape
is 27 there's 27 numbers and we're going to interpret this as the neural nets assignment for how likely every one of
these characters um the 27 characters are to come next
and as we tune the weights w we're going to be of course getting different probabilities out for any
character that you input and so now the question is just can we optimize and find a good w
such that the probabilities coming out are pretty good and the way we measure pretty good is by the loss function okay
summary, preview to next steps, reference to micrograd
so i organized everything into a single summary so that hopefully it's a bit more clear so it starts here
with an input data set we have some inputs to the neural net and we have some labels for the correct
next character in a sequence these are integers here i'm using uh torch generators now
so that you see the same numbers that i see and i'm generating um
27 neurons weights and each neuron here receives 27 inputs
then here we're going to plug in all the input examples x's into a neural net so here this is a forward pass
first we have to encode all of the inputs into one hot representations so we have 27 classes we pass in these
integers and x inc becomes a array that is 5 by 27
zeros except for a few ones we then multiply this in the first layer of a neural net to get logits
exponentiate the logits to get fake counts sort of and normalize these counts to get
probabilities so we lock these last two lines by the way here are called the softmax
which i pulled up here soft max is a very often used layer in a neural net
that takes these z's which are logics exponentiates them
and divides and normalizes it's a way of taking outputs of a neural net layer and these
these outputs can be positive or negative and it outputs probability distributions
it outputs something that is always sums to one and are positive numbers just like probabilities
um so it's kind of like a normalization function if you want to think of it that way and you can put it on top of any other linear layer inside a neural net
and it basically makes a neural net output probabilities that's very often used and we used it as well here
so this is the forward pass and that's how we made a neural net output probability now
you'll notice that um all of these this entire forward pass is made up of
differentiable layers everything here we can back propagate through and we saw some of the
back propagation in micrograd this is just multiplication and addition all that's
happening here is just multiply and then add and we know how to backpropagate through them exponentiation we know how to
backpropagate through and then here we are summing and sum is is easily backpropagable as
well and division as well so everything here is differentiable operation
and we can back propagate through now we achieve these probabilities which
are 5 by 27 for every single example we have a vector of probabilities that's into one
and then here i wrote a bunch of stuff to sort of like break down uh the examples
so we have five examples making up emma right and there are five bigrams inside emma
so bigram example a bigram example1 is that e is the beginning character right
after dot and the indexes for these are zero and five so then we feed in a zero
that's the input of the neural net we get probabilities from the neural net that are 27 numbers
and then the label is 5 because e actually comes after dot so that's the label
and then we use this label 5 to index into the probability distribution here
so this index 5 here is 0 1 2 3 4 5. it's this
number here which is here so that's basically the probability
assigned by the neural net to the actual correct character you see that the network currently thinks that this next character that e
following dot is only one percent likely which is of course not very good right because this actually is a training
example and the network thinks this is currently very very unlikely but that's just because we didn't get very lucky in
generating a good setting of w so right now this network things it says unlikely and 0.01 is not a good outcome
so the log likelihood then is very negative and the negative log likelihood is very
positive and so four is a very high negative log likelihood and that means we're going to
have a high loss because what is the loss the loss is just the average negative log likelihood
so the second character is em and you see here that also the network thought that m following e is very
unlikely one percent the for m following m i thought it was
two percent and for a following m it actually thought it was seven percent likely so
just by chance this one actually has a pretty good probability and therefore pretty low negative log likelihood
and finally here it thought this was one percent likely so overall our average negative log
likelihood which is the loss the total loss that summarizes basically the how well this network
currently works at least on this one word not on the full data suggested one word is 3.76 which is actually very
fairly high loss this is not a very good setting of w's now here's what we can do
we're currently getting 3.76 we can actually come here and we can change our w we can resample it so let
me just add one to have a different seed and then we get a different w and then we can rerun this
and with this different c with this different setting of w's we now get 3.37
so this is a much better w right and that and it's better because the probabilities just happen to come out
higher for the for the characters that actually are next and so you can imagine actually just
resampling this you know we can try two so
okay this was not very good let's try one more we can try three
okay this was terrible setting because we have a very high loss so anyway i'm going to erase this
what i'm doing here which is just guess and check of randomly assigning parameters and seeing if the network is good that is uh amateur hour that's not
how you optimize a neural net the way you optimize your neural net is you start with some random guess and we're
going to commit to this one even though it's not very good but now the big deal is we have a loss function
so this loss is made up only of differentiable operations and we can minimize the loss
by tuning ws by computing the gradients of the loss with respect to
these w matrices and so then we can tune w to minimize the loss and find a good setting of w
using gradient based optimization so let's see how that will work now things are actually going to look almost
identical to what we had with micrograd so here i pulled up the lecture from micrograd
the notebook it's from this repository and when i scroll all the way to the end where we left off with micrograd we had
something very very similar we had a number of input examples in this case we had four input examples inside axis
and we had their targets these are targets just like here we have our axes now but
we have five of them and they're now integers instead of vectors but we're going to convert our integers
to vectors except our vectors will be 27 large instead of three large
and then here what we did is first we did a forward pass where we ran a neural net on all of the inputs
to get predictions our neural net at the time this nfx was a multi-layer perceptron
our neural net is going to look different because our neural net is just a single layer
single linear layer followed by a soft max so that's our neural net
and the loss here was the mean squared error so we simply subtracted the prediction from the ground truth and
squared it and summed it all up and that was the loss and loss was the single number that summarized the quality of
the neural net and when loss is low like almost zero that means the neural net is
predicting correctly so we had a single number that uh that summarized the uh the performance of the
neural net and everything here was differentiable and was stored in massive compute graph
and then we iterated over all the parameters we made sure that the gradients are set to zero and we called
lost up backward and lasted backward initiated back propagation at the final output node of
loss right so yeah remember these expressions we had loss all the way at the end we start
back propagation and we went all the way back and we made sure that we populated all the parameters dot grad
so that graph started at zero but back propagation filled it in and then in the update we iterated over
all the parameters and we simply did a parameter update where every single
element of our parameters was nudged in the opposite direction of the gradient
and so we're going to do the exact same thing here so i'm going to pull this up
on the side here so that we have it available and we're
actually going to do the exact same thing so this was the forward pass so where we did this
and probs is our wipe red so now we have to evaluate the loss but we're not using the mean squared error we're using the
vectorized loss
negative log likelihood because we are doing classification we're not doing regression as it's called
so here we want to calculate loss now the way we calculate it is it's just
this average negative log likelihood now this probs here
has a shape of 5 by 27 and so to get all the we basically want
to pluck out the probabilities at the correct indices here so in particular because the labels are
stored here in array wise basically what we're after is for the first example we're looking at
probability of five right at index five for the second example
at the the second row or row index one we are interested in the probability assigned to index 13.
at the second example we also have 13. at the third row we want one
and then the last row which is four we want zero so these are the probabilities we're interested in right
and you can see that they're not amazing as we saw above so these are the probabilities we want
but we want like a more efficient way to access these probabilities not just listing them out in a tuple
like this so it turns out that the way to do this in pytorch uh one of the ways at least is we can basically pass in
all of these sorry about that all of these um
integers in the vectors so the these ones you see how they're just 0 1
2 3 4 we can actually create that using mp not mp sorry torch dot range of 5
0 1 2 3 4. so we can index here with torch.range of 5
and here we index with ys and you see that that gives us exactly these numbers
so that plucks out the probabilities of that the neural network assigns to the correct next character
now we take those probabilities and we don't we actually look at the log probability so we want to dot log
and then we want to just average that up so take the mean of all of that
and then it's the negative average log likelihood that is the loss
so the loss here is 3.7 something and you see that this loss 3.76 3.76 is
exactly as we've obtained before but this is a vectorized form of that expression so
we get the same loss and the same loss we can consider service part of this forward pass
and we've achieved here now loss okay so we made our way all the way to loss we've defined the forward pass
backward and update, in PyTorch
we forwarded the network and the loss now we're ready to do the backward pass so backward pass
we want to first make sure that all the gradients are reset so they're at zero now in pytorch you can set the gradients
to be zero but you can also just set it to none and setting it to none is more efficient and pi torch will interpret
none as like a lack of a gradient and is the same as zeros so this is a way to set to zero the
gradient and now we do lost it backward
before we do lost that backward we need one more thing if you remember from micrograd pytorch actually requires
that we pass in requires grad is true so that when we tell
pythorge that we are interested in calculating gradients for this leaf tensor by default this is false
so let me recalculate with that and then set to none and lost that backward
now something magical happened when lasted backward was run because pytorch just like micrograd when
we did the forward pass here it keeps track of all the operations under the hood it builds a full
computational graph just like the graphs we've produced in micrograd those graphs exist
inside pi torch and so it knows all the dependencies and all the mathematical operations of
everything and when you then calculate the loss we can call a dot backward on it
and that backward then fills in the gradients of all the intermediates
all the way back to w's which are the parameters of our neural net so now we
can do w grad and we see that it has structure there's stuff inside it
and these gradients every single element here so w dot shape is 27 by 27
w grad shape is the same 27 by 27 and every element of w that grad
is telling us the influence of that weight on the loss function
so for example this number all the way here if this element the zero zero element of
w because the gradient is positive is telling us that this has a positive
influence in the loss slightly nudging w slightly taking w 0 0
and adding a small h to it would increase the loss
mildly because this gradient is positive some of these gradients are also negative
so that's telling us about the gradient information and we can use this gradient information to update the weights of
this neural network so let's now do the update it's going to be very similar to what we had in micrograd we need no loop
over all the parameters because we only have one parameter uh tensor and that is w so we simply do w dot data plus equals
uh the we can actually copy this almost exactly negative 0.1 times w dot grad
and that would be the update to the tensor
so that updates the tensor and
because the tensor is updated we would expect that now the loss should decrease so
here if i print loss that item
it was 3.76 right so we've updated the w here so if i
recalculate forward pass loss now should be slightly lower so
3.76 goes to 3.74 and then
we can again set to set grad to none and backward update
and now the parameters changed again so if we recalculate the forward pass we expect a lower loss again 3.72
okay and this is again doing the we're now doing gradient descent
and when we achieve a low loss that will mean that the network is assigning high probabilities to the correctness
characters okay so i rearranged everything and i put it all together from scratch
putting everything together
so here is where we construct our data set of bigrams you see that we are still iterating only
on the first word emma i'm going to change that in a second i added a number that counts the number of
elements in x's so that we explicitly see that number of examples is five
because currently we're just working with emma and there's five backgrounds there and here i added a loop of exactly what
we had before so we had 10 iterations of grainy descent of forward pass backward pass and an update
and so running these two cells initialization and gradient descent gives us some improvement
on the loss function but now i want to use all the words
and there's not 5 but 228 000 bigrams now however this should require no
modification whatsoever everything should just run because all the code we wrote doesn't care if there's five migrants or 228 000 bigrams and with
everything we should just work so you see that this will just run but now we are optimizing over the
entire training set of all the bigrams and you see now that we are decreasing very slightly so actually we can
probably afford a larger learning rate and probably for even larger learning
rate
even 50 seems to work on this very very simple example right so let me re-initialize and let's run 100
iterations see what happens
okay we seem to be
coming up to some pretty good losses here 2.47 let me run 100 more
what is the number that we expect by the way in the loss we expect to get something around what we had originally
actually so all the way back if you remember in the beginning of this video when we
optimized uh just by counting our loss was roughly 2.47
after we had it smoothing but before smoothing we had roughly 2.45
likelihood sorry loss and so that's actually roughly the vicinity of what we expect to achieve
but before we achieved it by counting and here we are achieving the roughly the same result but with gradient based
optimization so we come to about 2.4 6 2.45 etc
and that makes sense because fundamentally we're not taking any additional information we're still just taking in the previous character and
trying to predict the next one but instead of doing it explicitly by counting and normalizing
we are doing it with gradient-based learning and it just so happens that the explicit approach happens to very well
optimize the loss function without any need for a gradient based optimization because the setup for bigram language
models are is so straightforward that's so simple we can just afford to estimate those probabilities directly and
maintain them in a table but the gradient-based approach is significantly more flexible
so we've actually gained a lot because what we can do now is
we can expand this approach and complexify the neural net so currently we're just taking a single character and
feeding into a neural net and the neural that's extremely simple but we're about to iterate on this substantially we're
going to be taking multiple previous characters and we're going to be feeding feeding them into increasingly more
complex neural nets but fundamentally out the output of the neural net will always just be logics
and those logits will go through the exact same transformation we are going to take them through a soft max
calculate the loss function and the negative log likelihood and do gradient based optimization and so actually
as we complexify the neural nets and work all the way up to transformers none of this will really fundamentally
change none of this will fundamentally change the only thing that will change is the way we do the forward pass where we
take in some previous characters and calculate logits for the next character in the sequence that will become more
complex and uh but we'll use the same machinery to optimize it and um
it's not obvious how we would have extended this bigram approach into the case where there are many more
characters at the input because eventually these tables would get way too large because there's way too many
combinations of what previous characters could be if you only have one previous character
we can just keep everything in a table that counts but if you have the last 10 characters that are input we can't
actually keep everything in the table anymore so this is fundamentally an unscalable approach and the neural network approach is significantly more
scalable and it's something that actually we can improve on over time so that's where we will be digging next i
wanted to point out two more things number one i want you to notice that
note 1: one-hot encoding really just selects a row of the next Linear layer's weight matrix
this x ink here this is made up of one hot vectors and
then those one hot vectors are multiplied by this w matrix and we think of this as multiple neurons
being forwarded in a fully connected manner but actually what's happening here is that for example
if you have a one hot vector here that has a one at say the fifth dimension
then because of the way the matrix multiplication works multiplying that one-half vector with w
actually ends up plucking out the fifth row of w log logits would become just the fifth
row of w and that's because of the way the matrix multiplication works
um so that's actually what ends up happening so but that's actually exactly what
happened before because remember all the way up here we have a bigram we took the first
character and then that first character indexed into a row of this array here
and that row gave us the probability distribution for the next character so the first character was used as a lookup
into a matrix here to get the probability distribution
well that's actually exactly what's happening here because we're taking the index we're encoding it as one hot and
multiplying it by w so logics literally becomes the
the appropriate row of w and that gets just as before exponentiated to create the counts
and then normalized and becomes probability so this w here is literally
the same as this array here but w remember is the log counts not the
counts so it's more precise to say that w exponentiated w dot x is this array
but this array was filled in by counting and by basically
populating the counts of bi-grams whereas in the gradient-based framework we initialize it randomly and then we
let the loss guide us to arrive at the exact same array
so this array exactly here is basically the array w at the end of
optimization except we arrived at it piece by piece by following the loss
and that's why we also obtain the same loss function at the end and the second note is if i come here
note 2: model smoothing as regularization loss
remember the smoothing where we added fake counts to our counts in order to
smooth out and make more uniform the distributions of these probabilities and that prevented us from assigning
zero probability to to any one bigram now if i increase the count here
what's happening to the probability as i increase the count probability
becomes more and more uniform right because these counts go only up to
like 900 or whatever so if i'm adding plus a million to every single number here you can see how
the row and its probability then when we divide is just going to become more and more close to exactly even probability
uniform distribution it turns out that the gradient based framework has an equivalent to smoothing
in particular think through these w's here
which we initialized randomly we could also think about initializing w's to be zero
if all the entries of w are zero then you'll see that logits will become
all zero and then exponentiating those logics becomes all one and then the probabilities turned out to
be exactly uniform so basically when w's are all equal to each other or say especially zero
then the probabilities come out completely uniform so trying to incentivize w to be near zero
is basically equivalent to label smoothing and the more you incentivize that in the loss function
the more smooth distribution you're going to achieve so this brings us to something that's called
regularization where we can actually augment the loss function to have a small component that we call a
regularization loss in particular what we're going to do is we can take w and we can for example
square all of its entries and then we can um whoops
sorry about that we can take all the entries of w and we can sum them
and because we're squaring uh there will be no signs anymore um negatives and positives all get squashed
to be positive numbers and then the way this works is you achieve zero loss if w is exactly or
zero but if w has non-zero numbers you accumulate loss and so we can actually take this and we
can add it on here so we can do something like loss plus
w square dot sum or let's actually instead of sum let's take a mean because otherwise the sum
gets too large so mean is like a little bit more manageable
and then we have a regularization loss here say 0.01 times or something like that you can choose
the regularization strength and then we can just optimize this and
now this optimization actually has two components not only is it trying to make all the probabilities work out but in
addition to that there's an additional component that simultaneously tries to make all w's be zero because if w's are
non-zero you feel a loss and so minimizing this the only way to achieve that is for w to be zero
and so you can think of this as adding like a spring force or like a gravity force that that pushes w to be zero so w
wants to be zero and the probabilities want to be uniform but they also simultaneously want to match up your
your probabilities as indicated by the data and so the strength of this regularization is exactly controlling
the amount of counts that you add here adding a lot more counts
here corresponds to increasing this number
because the more you increase it the more this part of the loss function dominates this part and the more these
these weights will un be unable to grow because as they grow they uh accumulate way too much loss
and so if this is strong enough then we are not able to overcome the
force of this loss and we will never and basically everything will be uniform predictions
so i thought that's kind of cool okay and lastly before we wrap up i wanted to show you how you would sample from this neural net model
sampling from the neural net
and i copy-pasted the sampling code from before where remember that we sampled five
times and all we did we start at zero we grabbed the current ix row of p and that
was our probability row from which we sampled the next index and just accumulated that and
break when zero and running this gave us these
results still have the p in memory so this is fine now
the speed doesn't come from the row of b instead it comes from this neural net
first we take ix and we encode it into a one hot row of x
inc this x inc multiplies rw which really just plucks out the row of
w corresponding to ix really that's what's happening and that gets our logits and then we
normalize those low jets exponentiate to get counts and then normalize to get uh the distribution and
then we can sample from the distribution so if i run this
kind of anticlimactic or climatic depending how you look at it but we get the exact same result
um and that's because this is in the identical model not only does it achieve the same loss
but as i mentioned these are identical models and this w is the log counts of
what we've estimated before but we came to this answer in a very different way and it's got a very different
interpretation but fundamentally this is basically the same model and gives the same samples here and so
that's kind of cool okay so we've actually covered a lot of ground we introduced the bigram character level
conclusion
language model we saw how we can train the model how we can sample from the model and how we can
evaluate the quality of the model using the negative log likelihood loss and then we actually trained the model
in two completely different ways that actually get the same result and the same model in the first way we just counted up the
frequency of all the bigrams and normalized in a second way we used the
negative log likelihood loss as a guide to optimizing the counts matrix
or the counts array so that the loss is minimized in the in a gradient-based framework and we saw that both of them
give the same result and that's it now the second one of these the
gradient-based framework is much more flexible and right now our neural network is super simple we're taking a
single previous character and we're taking it through a single linear layer to calculate the logits
this is about to complexify so in the follow-up videos we're going to be taking more and more of these characters
and we're going to be feeding them into a neural net but this neural net will still output the exact same thing the neural net will output logits
and these logits will still be normalized in the exact same way and all the loss and everything else and the gradient gradient-based framework
everything stays identical it's just that this neural net will now complexify all the way to transformers
so that's gonna be pretty awesome and i'm looking forward to it for now bye

Introduction
hi everyone today we are continuing our implementation of makemore now in the last lecture we implemented
the bigram language model and we implemented it both using counts and also using a super simple neural network
that had a single linear layer now this is the jupyter notebook that we built out last
lecture and we saw that the way we approached this is that we looked at only the single previous character and we
predicted the distribution for the character that would go next in the sequence and we did that by taking
counts and normalizing them into probabilities so that each row here sums to one
now this is all well and good if you only have one character of previous context and this works and it's approachable the
problem with this model of course is that the predictions from this model are not very good because you only take one
character of context so the model didn't produce very name like sounding things
now the problem with this approach though is that if we are to take more context into account when predicting the
next character in a sequence things quickly blow up and this table the size of this table grows and in fact it grows
exponentially with the length of the context because if we only take a single character at a time that's 27
possibilities of context but if we take two characters in the past and try to predict the third one
suddenly the number of rows in this matrix you can look at it that way is 27 times 27 so there's 729
possibilities for what could have come in the context if we take three characters as the
context suddenly we have 20 000 possibilities of context
and so there's just way too many rows of this matrix it's way too few counts
for each possibility and the whole thing just kind of explodes and doesn't work very well
Bengio et al. 2003 (MLP language model) paper walkthrough
so that's why today we're going to move on to this bullet point here and we're going to implement a multi-layer perceptron model to predict the next uh
character in a sequence and this modeling approach that we're going to adopt follows this paper
benguetal 2003 so i have the paper pulled up here now this isn't the very first paper that
proposed the use of multiglio perceptrons or neural networks to predict the next character or token in a
sequence but it's definitely one that is uh was very influential around that time it is very often cited to stand in for
this idea and i think it's a very nice write-up and so this is the paper that we're going to first look at and then
implement now this paper has 19 pages so we don't have time to go into the full detail of this paper but i
invite you to read it it's very readable interesting and has a lot of interesting ideas in it as well
in the introduction they describe the exact same problem i just described and then to address it they propose the
following model now keep in mind that we are building a character level language model so we're
working on the level of characters in this paper they have a vocabulary of 17 000 possible words and they instead
build a word level language model but we're going to still stick with the characters but we'll take the same modeling approach
now what they do is basically they propose to take every one of these words seventeen thousand words and they're
going to associate to each word a say thirty dimensional feature vector
so every word is now embedded into a thirty dimensional space you can think of it that way so we have
17 000 points or vectors in a 30 dimensional space and that's um you might imagine that's
very crowded that's a lot of points for a very small space now in the beginning these words are
initialized completely randomly so they're spread out at random but then we're going to tune these
embeddings of these words using back propagation so during the course of training of this neural network these points or vectors
are going to basically move around in this space and you might imagine that for example words that have very similar
meanings or that are indeed synonyms of each other might end up in a very similar part of the space and conversely
words that mean very different things would go somewhere else in a space now their modeling approach otherwise is
identical to ours they are using a multi-layer neural network to predict the next word given the previous words
and to train the neural network they are maximizing the log likelihood of the training data just like we did
so the modeling approach itself is identical now here they have a concrete example of this intuition
why does it work basically suppose that for example you are trying to predict a dog was running in a blank
now suppose that the exact phrase a dog was running in a has never occurred in a training data
and here you are at sort of test time later when the model is deployed somewhere and it's trying to make a sentence and
it's saying a dog was running in a blank and because it's never encountered this exact phrase in the training set you're
out of distribution as we say like you don't have fundamentally any reason to suspect
what might come next but this approach actually allows you to get around that because maybe you didn't
see the exact phrase a dog was running in a something but maybe you've seen similar phrases maybe you've seen the
phrase the dog was running in a blank and maybe your network has learned that a and the
are like frequently are interchangeable with each other and so maybe it took the embedding for a and the embedding for
the and it actually put them like nearby each other in the space and so you can transfer knowledge through that
embedding and you can generalize in that way similarly the network could know that cats and dogs are animals and they
co-occur in lots of very similar contexts and so even though you haven't seen this exact phrase
or if you haven't seen exactly walking or running you can through the embedding space
transfer knowledge and you can generalize to novel scenarios so let's now scroll down to the diagram
of the neural network they have a nice diagram here and in this example we are taking three
previous words and we are trying to predict the fourth word in a sequence
now these three previous words as i mentioned uh we have a vocabulary of 17 000 um possible words
so every one of these basically basically are the index of the incoming word
and because there are 17 000 words this is an integer between 0 and 16999
now there's also a lookup table that they call c this lookup table is a matrix that is 17
000 by say 30. and basically what we're doing here is we're treating this as a lookup table
and so every index is plucking out a row of this embedding matrix
so that each index is converted to the 30 dimensional vector that corresponds to the embedding vector for that word
so here we have the input layer of 30 neurons for three words making up 90
neurons in total and here they're saying that this matrix c is shared across all the words so
we're always indexing into the same matrix c over and over um for each one of these
words next up is the hidden layer of this neural network the size of this hidden neural layer of this neural net
is a hoppy parameter so we use the word hyperparameter when it's kind of like a design choice up to the designer of the
neural net and this can be as large as you'd like or as small as you'd like so for example the size could be a hundred
and we are going to go over multiple choices of the size of this hidden layer and we're going to evaluate how well
they work so say there were 100 neurons here all of them would be fully connected to the
90 words or 90 um numbers that make up these three words
so this is a fully connected layer then there's a 10 inch long linearity and then there's this output layer and
because there are 17 000 possible words that could come next this layer has 17 000 neurons
and all of them are fully connected to all of these neurons in the hidden layer
so there's a lot of parameters here because there's a lot of words so most computation is here this is the
expensive layer now there are 17 000 logits here so on top of there we have the softmax layer
which we've seen in our previous video as well so every one of these logits is exponentiated and then everything is
normalized to sum to 1 so that we have a nice probability distribution for the next word in the sequence
now of course during training we actually have the label we have the identity of the next word in a sequence
that word or its index is used to pluck out the
probability of that word and then we are maximizing the probability of that word
with respect to the parameters of this neural net so the parameters are the weights and biases of this output layer
the weights and biases of the hidden layer and the embedding lookup table c
and all of that is optimized using back propagation and these uh dashed arrows ignore those
uh that represents a variation of a neural nut that we are not going to explore in this video so that's the setup and now let's
implement it okay so i started a brand new notebook for this lecture we are importing pytorch and we are
(re-)building our training dataset
importing matplotlib so we can create figures then i am reading all the names into a
list of words like i did before and i'm showing the first eight right here keep in mind that we have a 32 000 in
total these are just the first eight and then here i'm building out the vocabulary of characters and all the
mappings from the characters as strings to integers and vice versa
now the first thing we want to do is we want to compile the data set for the neural network and i had to rewrite this code um i'll
show you in a second what it looks like so this is the code that i created for
the dataset creation so let me first run it and then i'll briefly explain how this works
so first we're going to define something called block size and this is basically the context length of how many
characters do we take to predict the next one so here in this example we're taking three characters to predict the
fourth one so we have a block size of three that's the size of the block that supports the prediction
then here i'm building out the x and y the x are the input to the neural net
and the y are the labels for each example inside x
then i'm airing over the first five words i'm doing first five just for efficiency while we are developing all
the code but then later we're going to come here and erase this so that we use the entire training set
so here i'm printing the word emma and here i'm basically showing the examples that we can generate the five
examples that we can generate out of the single um sort of word emma
so when we are given the context of just uh dot dot the first character in a sequence is e
in this context the label is m when the context is this the label is m
and so forth and so the way i build this out is first i start with a padded context of just zero tokens
then i iterate over all the characters i get the character in the sequence and i
basically build out the array y of this current character and the array x which stores the current running context
and then here see i print everything and here i um crop the context and enter the
new character in a sequence so this is kind of like a rolling window of context
now we can change the block size here to for example four and in that case we'll be predicting the
fifth character given the previous four or it can be five and then it would look like this
or it can be say ten and then it would look something like this we're taking ten characters to
predict the eleventh one and we're always padding with dots so let me bring this back to three
just so that we have what we have here in the paper and finally the data set right now looks
as follows from these five words we have created a data set of 32 examples
and each input of the neural net is three integers and we have a label that is also an integer
y so x looks like this these are the individual examples
and then y are the labels so given this
let's now write a neural network that takes these axes and predicts the y's first let's build the embedding lookup
implementing the embedding lookup table
table c so we have 27 possible characters and we're going to embed them in a lower
dimensional space in the paper they have 17 000 words and they bet them in uh spaces as small
dimensional as 30. so they cram 17 000 words into 30 dimensional space in our
case we have only 27 possible characters so let's grab them in something as small as to start with for example a
two-dimensional space so this lookup table will be random numbers and we'll have 27 rows and we'll have
two columns right so each 20 each one of 27 characters will have a two-dimensional
embedding so that's our matrix c of embeddings in
the beginning initialized randomly now before we embed all of the integers inside the input x using this lookup
table c let me actually just try to embed a single individual integer like say five
so we get a sense of how this works now one way this works of course is we can just take the c and we can index into
row five and that gives us a vector the fifth row of c
and um this is one way to do it the other way that i presented in the previous lecture is actually seemingly
different but actually identical so in the previous lecture what we did is we took these integers and we used the one
hot encoding to first encode them so f.1 hot we want to encode integer 5 and we want
to tell it that the number of classes is 27 so that's the 26 dimensional vector of all zeros except the fifth bit is
turned on now this actually doesn't work
the reason is that this input actually must be a doorstop tensor and i'm making some of these errors
intentionally just so you get to see some errors and how to fix them so this must be a tester not an int
fairly straightforward to fix we get a one hot vector the fifth dimension is one and the shape of this is 27.
and now notice that just as i briefly alluded to in the previous video if we take this one hot vector and we multiply
it by c then
what would you expect well number one first you'd expect an error
because expected scalar type long but found float so a little bit confusing but
the problem here is that one hot the data type of it is long it's a 64-bit integer but this is a
float tensor and so pytorch doesn't know how to multiply an int with a float and
that's why we had to explicitly cast this to a float so that we can multiply now the output actually here
is identical and that it's identical because of the way the matrix multiplication here works
we have the one hot um vector multiplying columns of c and because of all the zeros they
actually end up masking out everything in c except for the fifth row which is plucked out
and so we actually arrive at the same result and that tells you that here we can interpret this first
piece here this embedding of the integer we can either think of it as the integer indexing into a lookup table c but
equivalently we can also think of this little piece here as a first layer of this bigger neural net
this layer here has neurons that have no non-linearity there's no 10h they're just linear neurons and their weight
matrix is c and then we are encoding integers into one hot and feeding those into a neural
net and this first layer basically embeds them so those are two equivalent ways of doing the same thing we're just going to
index because it's much much faster and we're going to discard this interpretation of one hot inputs into
neural nets and we're just going to index integers and create and use embedding tables now embedding a single
integer like 5 is easy enough we can simply ask pytorch to retrieve the fifth row of c
or the row index five of c but how do we simultaneously embed all
of these 32 by three integers stored in array x luckily pytorch indexing is fairly
flexible and quite powerful so it doesn't just work to ask for a single element five like this
you can actually index using lists so for example we can get the rows five six and seven
and this will just work like this we can index with a list it doesn't just have to be a list it can
also be a actually a tensor of integers and we can index with that
so this is a integer tensor 567 and this will just work as well
in fact we can also for example repeat row 7 and retrieve it multiple times
and that same index will just get embedded multiple times here
so here we are indexing with a one-dimensional tensor of integers but it turns out that
you can also index with multi-dimensional tensors of integers here we have a two-dimensional in tensor
of integers so we can simply just do c at x and this just works
and the shape of this is 32 by 3 which is the original shape and
now for every one of those 32 by 3 integers we've retrieved the embedding vector here so basically we have that as an
example the 13th or example index 13
the second dimension is the integer 1 as an example and so
here if we do c of x which gives us that array and then we index into 13 by two
of that array then we we get the embedding here and you can verify that
c at one which is the integer at that location is indeed equal to this
you see they're equal so basically long story short pytorch indexing is awesome and to embed
simultaneously all of the integers in x we can simply do c of x and that is our embedding
and that just works now let's construct this layer here the hidden layer
implementing the hidden layer + internals of torch.Tensor: storage, views
so we have that w1 as i'll call it are these weights which we will initialize
randomly now the number of inputs to this layer is going to be three times two right because we have
two dimensional embeddings and we have three of them so the number of inputs is 6 and the number of neurons in this layer
is a variable up to us let's use 100 neurons as an example and then biases
will be also initialized randomly as an example and let's and we just need 100 of them
now the problem with this is we can't simply normally we would take the input in this case that's embedding and we'd
like to multiply it with these weights and then we would like to add the bias this is roughly what we want to do
but the problem here is that these embeddings are stacked up in the dimensions of this input tensor
so this will not work this matrix multiplication because this is a shape 32 by 3 by 2 and i can't multiply that
by 6 by 100 so somehow we need to concatenate these inputs here together so that we can do
something along these lines which currently does not work so how do we transform this 32 by 3 by 2
into a 32 by 6 so that we can actually perform this multiplication over here i'd like to
show you that there are usually many ways of implementing what you'd like to do in
torch and some of them will be faster better shorter etc and that's because torch is a very large
library and it's got lots and lots of functions so if you just go to the documentation and click on torch you'll
see that my slider here is very tiny and that's because there are so many functions that you can call on these
tensors to transform them create them multiply them add them perform all kinds of
different operations on them and so this is kind of like
the space of possibility if you will now one of the things that you can do is if we can control here ctrl f for
concatenate and we see that there's a function torque.cat short for concatenate
and this concatenates the given sequence of tensors in a given dimension and these sensors must have the same
shape etc so we can use the concatenate operation to in a naive way concatenate
these three embeddings for each input so in this case we have m of
amp of the shape and really what we want to do is we want to retrieve these three parts and concatenate them
so we want to grab all the examples we want to grab
first the zeroth index and then all of
this so this plucks out the 32 by 2 embeddings of just the first
word here and so basically we want this guy we want the first dimension and we want
the second dimension and these are the three pieces individually
and then we want to treat this as a sequence and we want to torch that cat on that sequence so this is the list
tor.cat takes a sequence of tensors and then we have to tell it along which dimension to concatenate
so in this case all these are 32 by 2 and we want to concatenate not across dimension 0 by the cross dimension one
so passing in one gives us a result the shape of this is 32 by 6 exactly as
we'd like so that basically took 32 and squashed these by concatenating them into 32 by
6. now this is kind of ugly because this code would not generalize if we want to
later change the block size right now we have three inputs three words but what if we had five
then here we would have to change the code because i'm indexing directly well torch comes to rescue again because that
turns out to be a function called unbind and it removes a tensor dimension
so it removes the tensor dimension returns a tuple of all slices along a given dimension
without it so this is exactly what we need and basically when we call torch dot
unbind torch dot unbind of m
and pass in dimension 1 index 1 this gives us a list of
a list of tensors exactly equivalent to this so running this gives us a line
3 and it's exactly this list and so we can call torch.cat on it
and along the first dimension and this works and this shape is the same
but now this is uh it doesn't matter if we have block size 3 or 5 or 10 this will just work
so this is one way to do it but it turns out that in this case there's actually a significantly better and more efficient
way and this gives me an opportunity to hint at some of the internals of torch.tensor
so let's create an array here of elements from 0 to 17
and the shape of this is just 18. it's a single picture of 18 numbers
it turns out that we can very quickly re-represent this as different sized and
dimensional tensors we do this by calling a view and we can say that actually this is not
a single vector of 18 this is a two by nine tensor or alternatively this is a
nine by two tensor or this is actually a three by three by two tensor
as long as the total number of elements here multiply to be the same this will just work and
in pytorch this operation calling that view is extremely efficient
and the reason for that is that in each tensor there's something called the underlying storage
and the storage is just the numbers always as a one-dimensional vector and this is how this tensor is represented
in the computer memory it's always a one-dimensional vector but when we call that view we are
manipulating some of attributes of that tensor that dictate how this one-dimensional sequence is interpreted
to be an n-dimensional tensor and so what's happening here is that no memory is being changed copied moved or
created when we call that view the storage is identical but when you call that view
some of the internal attributes of the view of the sensor are being manipulated and changed in
particular that's something there's something called a storage offset strides and shapes and those are manipulated so that this one-dimensional
sequence of bytes is seen as different and dimensional arrays there's a blog post here from eric
called pi torch internals where he goes into some of this with respect to tensor and how the view of the tensor is
represented and this is really just like a logical construct of representing the physical
memory and so this is a pretty good um blog post that you can go into i might also
create an entire video on the internals of torch tensor and how this works for here we just note that this is an
extremely efficient operation and if i delete this and come back to our end
we see that the shape of our end is 32 by three by two but we can simply ask for pytorch to view this instead as
a 32 by six and the way this gets flattened into a 32 by six array
just happens that these two get stacked up in a single row and so that's basically
the concatenation operation that we're after and you can verify that this actually gives the exact same result as what we
had before so this is an element y equals and you can see that all the elements of these two tensors are the same
and so we get the exact same result so long story short we can actually just
come here and if we just view this as a 32x6
instead then this multiplication will work and give us the hidden states that we're after
so if this is h then h shape is now the 100 dimensional activations for
every one of our 32 examples and this gives the desired result let me do two things here number one let's not
use 32 we can for example do something like
m.shape at 0 so that we don't hard code these numbers and this would work for any size of this
amp or alternatively we can also do negative one when we do negative one pi torch will infer what this should be
because the number of elements must be the same and we're saying that this is 6 by church will derive that this must be
32 or whatever else it is if m is of different size the other thing is here um
one more thing i'd like to point out is here when we do the concatenation
this actually is much less efficient because um this concatenation would create a whole new tensor with a whole
new storage so new memory is being created because there's no way to concatenate tensors just by manipulating
the view attributes so this is inefficient and creates all kinds of new memory
uh so let me delete this now we don't need this and here to calculate h we want to also
dot 10h of this to get our oops to get our h
so these are now numbers between negative one and one because of the 10h and we have that the shape is 32 by 100
and that is basically this hidden layer of activations here for every one of our 32 examples
now there's one more thing i've lost over that we have to be very careful with and that this and that's this plus here
in particular we want to make sure that the broadcasting will do what we like the shape of this is 32 by 100 and the
ones shape is 100. so we see that the addition here will broadcast these two and in particular we
have 32 by 100 broadcasting to 100. so broadcasting will align on the right
create a fake dimension here so this will become a 1 by 100 row vector and then it will copy vertically
for every one of these rows of 32 and do an element wise addition so in this case the correct thing will
be happening because the same bias vector will be added to all the rows
of this matrix so that is correct that's what we'd like and it's always good
practice you just make sure so that you don't shoot yourself in the foot and finally let's create the final layer here
implementing the output layer
so let's create w2 and v2
the input now is 100 and the output number of neurons will be for us 27 because we have 27 possible
characters that come next so the biases will be 27 as well
so therefore the logits which are the outputs of this neural net are going to be um
h multiplied by w2 plus b2
logistic shape is 32 by 27 and the logits look
good now exactly as we saw in the previous video we want to take these logits and we want to first exponentiate
implementing the negative log likelihood loss
them to get our fake counts and then we want to normalize them into a probability so prob is counts divide
and now counts dot sum along the first dimension and keep them as true exactly as in the
previous video and so prob that shape now is 32 by 27
and you'll see that every row of prob sums to one so it's normalized
so that gives us the probabilities now of course we have the actual letter that comes next and that comes from this
array y which we which we created during the dataset creation so why is this last
piece here which is the identity of the next character in the sequence that we'd like to now predict
so what we'd like to do now is just as in the previous video we'd like to index into the rows of prob and in each row
we'd like to pluck out the probability assigned to the correct character as given here
so first we have torch.range of 32 which is kind of like a iterator over
numbers from 0 to 31 and then we can index into prob in the following way
prop in torch.range of 32 which iterates the roads and in each row we'd like to grab
this column as given by y so this gives the current probabilities
as assigned by this neural network with this setting of its weights to the correct character in the sequence
and you can see here that this looks okay for some of these characters like this is basically 0.2
but it doesn't look very good at all for many other characters like this is 0.0701 probability and so the network
thinks that some of these are extremely unlikely but of course we haven't trained the neural network yet so
this will improve and ideally all of these numbers here of course are one because then we are correctly predicting
the next character now just as in the previous video we want to take these probabilities we want
to look at the lock probability and then we want to look at the average probability and the negative of it to create the
negative log likelihood loss so the loss here is 17
and this is the loss that we'd like to minimize to get the network to predict the correct character in the sequence
okay so i rewrote everything here and made it a bit more respectable so here's our data set here's all the parameters
summary of the full network
that we defined i'm now using a generator to make it reproducible i clustered all the parameters into a
single list of parameters so that for example it's easy to count them and see that in total we currently have about
3400 parameters and this is the forward pass as we developed it and we arrive at a single number here
the loss that is currently expressing how well this neural network works with the
current setting of parameters now i would like to make it even more respectable so in particular see these
introducing F.cross_entropy and why
lines here where we take the logits and we calculate the loss
we're not actually reinventing the wheel here this is just um classification and many people use
classification and that's why there is a functional.cross entropy function in pytorch to calculate this much more
efficiently so we can just simply call f.cross entropy and we can pass in the logits and we can
pass in the array of targets y and this calculates the exact same loss
so in fact we can simply put this here and erase these three lines and we're
going to get the exact same result now there are actually many good reasons to prefer f.cross entropy over rolling your
own implementation like this i did this for educational reasons but you'd never use this in practice why is that
number one when you use f.cross entropy by torch will not actually create all these intermediate tensors because these
are all new tensors in memory and all this is fairly inefficient to run like this instead pytorch will cluster up all
these operations and very often create have fused kernels that very efficiently
evaluate these expressions that are sort of like clustered mathematical operations
number two the backward pass can be made much more efficient and not just because it's a fused kernel but also
analytically and mathematically it's much it's often a very much simpler backward pass to implement
we actually sell this with micrograd you see here when we implemented 10h the forward pass of this operation to
calculate the 10h was actually a fairly complicated mathematical expression but because it's a clustered
mathematical expression when we did the backward pass we didn't individually backward through the x and the two times
and the minus one in division etc we just said it's one minus t squared and that's a much simpler mathematical
expression and we were able to do this because we're able to reuse calculations and because we are able to mathematically
and analytically derive the derivative and often that expression simplifies mathematically and so there's much less
to implement so not only can can it be made more efficient because it runs in a fused kernel but also because the expressions
can take a much simpler form mathematically so that's number one number two
under the hood f that cross entropy can also be significantly more um
numerically well behaved let me show you an example of how this works
suppose we have a logits of negative 2 3 negative 3 0 and 5 and then we are taking the exponent of
it and normalizing it to sum to 1. so when logits take on this values everything is well and good and we get a
nice probability distribution now consider what happens when some of these logits take on more extreme values
and that can happen during optimization of the neural network suppose that some of these numbers grow
very negative like say negative 100 then actually everything will come out fine we still get the probabilities that
um you know are well behaved and they sum to one and everything is great
but because of the way the x works if you have very positive logits let's say positive 100 in here
you actually start to run into trouble and we get not a number here and the reason for that is that these
counts have an if here so if you pass in a very negative number
to x you just get a very negative sorry not negative but very small number very
very near zero and that's fine but if you pass in a very positive number suddenly we run out of range in
our floating point number that represents these counts so basically we're taking e and we're
raising it to the power of 100 and that gives us if because we run out of dynamic range on this floating point
number that is count and so we cannot pass very large logits
through this expression now let me reset these numbers to something reasonable
the way pi torch solved this is that you see how we have a well-behaved result here
it turns out that because of the normalization here you can actually offset logits by any arbitrary constant
value that you want so if i add 1 here you actually get the exact same result
or if i add 2 or if i subtract three any offset will produce the exact same
probabilities so because negative numbers are okay but positive numbers can actually overflow
this x what patrick does is it internally calculates the maximum value that occurs in the logits and it
subtracts it so in this case it would subtract five and so therefore the greatest number in
logits will become zero and all the other numbers will become some negative numbers and then the result of this is always
well behaved so even if we have 100 here previously not good but because pytorch will
subtract 100 this will work and so there's many good reasons to call
cross-entropy number one the forward pass can be much more efficient the backward pass can be much more efficient
and also things can be much more numerically well behaved okay so let's now set up the training of this neural
implementing the training loop, overfitting one batch
net we have the forward pass uh we don't need these
is that we have the losses equal to the f.cross entropy that's the forward pass
then we need the backward pass first we want to set the gradients to be zero so for p in parameters
we want to make sure that p dot grad is none which is the same as setting it to zero in pi torch and then lost that backward to populate
those gradients once we have the gradients we can do the parameter update so for p in parameters
we want to take all the data and we want to nudge it learning rate times p dot grad
and then we want to repeat this a few times
and let's print the loss here as well now this won't suffice and it will
create an error because we also have to go for pn parameters and we have to make sure that p dot
requires grad is set to true in pi torch and this should just work
okay so we started off with loss of 17 and we're decreasing it let's run longer
and you see how the loss decreases a lot here so
if we just run for a thousand times we get a very very low loss and that means that we're making very good
predictions now the reason that this is so straightforward right now is because we're only um
overfitting 32 examples so we only have 32 examples uh of the
first five words and therefore it's very easy to make this neural net fit only these two 32
examples because we have 3 400 parameters and only 32 examples so we're
doing what's called overfitting a single batch of the data and getting a very low loss and good
predictions um but that's just because we have so many parameters for so few examples so it's easy to
uh make this be very low now we're not able to achieve exactly zero and the reason for that is we can for
example look at logits which are being predicted and we can look at the max along the
first dimension and in pi torch max reports both the actual values that
take on the maximum number but also the indices of piece and you'll see that the indices are very
close to the labels but in some cases they differ
for example in this very first example the predicted index is 19 but the label is five
and we're not able to make loss be zero and fundamentally that's because here
the very first or the zeroth index is the example where dot dot dot is supposed to predict e but you see how
dot dot dot is also supposed to predict an o and dot dot is also supposed to predict an i and then s as well and so
basically e o a or s are all possible outcomes in a training set for the exact
same input so we're not able to completely over fit and um and make the loss be exactly zero so but
we're getting very close in the cases where there's a unique input for a unique
output in those cases we do what's called overfit and we basically get the exact same and the exact correct result
so now all we have to do is we just need to make sure that we read in the full data set and optimize
the neural net okay so let's swing back up where we created the dataset and we see that here we only use the
training on the full dataset, minibatches
first five words so let me now erase this and let me erase the print statements otherwise we'd be printing way too much
and so when we processed the full data set of all the words we now had 228 000 examples instead of just 32.
so let's now scroll back down to this is much larger reinitialize the weights the same number of parameters
they all require gradients and then let's push this print out lost.item to be here
and let's just see how the optimization goes if we run this
okay so we started with a fairly high loss and then as we're optimizing the loss is coming down
but you'll notice that it takes quite a bit of time for every single iteration so let's actually address that because
we're doing way too much work forwarding and backwarding 220 000 examples in practice what people usually do is
they perform forward and backward pass and update on many batches of the data
so what we will want to do is we want to randomly select some portion of the data set and that's a mini batch and then
only forward backward and update on that little mini batch and then we iterate on those many batches
so in pytorch we can for example use storage.randint we can generate numbers between 0 and 5
and make 32 of them i believe the size has to be a
tuple in my torch so we can have a tuple 32 of numbers
between zero and five but actually we want x dot shape of zero here and so this creates uh integers that
index into our data set and there's 32 of them so if our mini batch size is 32
then we can come here and we can first do a mini batch construct
so in the integers that we want to optimize in this single iteration
are in the ix and then we want to index into x
with ix to only grab those rows so we're only getting 32 rows of x
and therefore embeddings will again be 32 by three by two not two hundred thousand by three by two
and then this ix has to be used not just to index into x but also to index into y
and now this should be many batches and this should be much much faster so okay so it's instant almost
so this way we can run many many examples nearly instantly and decrease the loss
much much faster now because we're only dealing with mini batches the quality of our gradient is
lower so the direction is not as reliable it's not the actual gradient direction
but the gradient direction is good enough even when it's estimating on only 32 examples that it is useful and so
it's much better to have an approximate gradient and just make more steps than it is to evaluate the exact gradient and
take fewer steps so that's why in practice uh this works quite well
so let's now continue the optimization let me take out this lost item from here
and uh place it over here at the end okay so we're hovering around 2.5 or so
however this is only the loss for that mini batch so let's actually evaluate the loss here
for all of x and for all of y just so we have a full sense of exactly how all the model
is doing right now so right now we're at about 2.7 on the entire training set
so let's run the optimization for a while okay right 2.6
2.57 2.53
okay so one issue of course is we don't know if we're stepping too slow or too fast
so this point one i just guessed it so one question is how do you determine this learning rate
and how do we gain confidence that we're stepping in the right sort of speed so i'll show you one way
finding a good initial learning rate
to determine a reasonable learning rate it works as follows let's reset our parameters
to the initial settings and now let's
print in every step but let's only do 10 steps or so
or maybe maybe 100 steps we want to find like a very reasonable set
search range if you will so for example if this is like very low then
we see that the loss is barely decreasing so that's not that's like too low basically so let's
try this one okay so we're decreasing the loss but like not very quickly so that's a pretty
good low range now let's reset it again and now let's try to find the place at
which the loss kind of explodes uh so maybe at negative one
okay we see that we're minimizing the loss but you see how uh it's kind of unstable it goes up and down quite a bit
um so negative one is probably like a fast learning rate let's try negative 10.
okay so this isn't optimizing this is not working very well so negative 10 is way too big
negative one was already kind of big um so therefore
negative one was like somewhat reasonable if i reset so i'm thinking that the right learning
rate is somewhere between uh negative zero point zero zero one and um
negative one so the way we can do this here is we can use uh torch shot lens space
and we want to basically do something like this between zero and one but um
those number of steps is one more parameter that's required let's do a thousand steps this creates 1000
numbers between 0.01 and 1 but it doesn't really make sense to step
between these linearly so instead let me create learning rate exponent
and instead of 0.001 this will be a negative 3 and this will be a zero
and then the actual lrs that we want to search over are going to be 10 to the power of lre
so now what we're doing is we're stepping linearly between the exponents of these learning rates this is 0.001
and this is 1 because 10 to the power of 0 is 1. and therefore we are spaced
exponentially in this interval so these are the candidate learning rates that we want to sort of like search over
roughly so now what we're going to do is here we are going to run the
optimization for 1000 steps and instead of using a fixed number we are going to use learning rate
indexing into here lrs of i and make this i
so basically let me reset this to be again starting from random creating these learning rates between
negative zero points between 0.001 and um one but exponentially stopped
and here what we're doing is we're iterating a thousand times we're going to use the learning rate
um that's in the beginning very very low in the beginning is going to be 0.001 but by the end it's going to be
1. and then we're going to step with that learning rate and now what we want to do is we want to
keep track of the uh
learning rates that we used and we want to look at the losses that resulted
and so here let me track stats so lri.append lr
and um lost side that append loss that item
okay so again reset everything and then run
and so basically we started with a very low learning rate and we went all the way up to a learning rate of negative one
and now what we can do is we can plt that plot and we can plot the two so we can plot
the learning rates on the x-axis and the losses we saw on the y-axis and often you're going to find that your
plot looks something like this where in the beginning you had very low learning rates so
basically anything barely anything happened then we got to like a nice spot here
and then as we increase the learning rate enough we basically started to be kind of unstable here
so a good learning rate turns out to be somewhere around here um and because we have lri here
um we actually may want to um
do not lr not the learning rate but the exponent so that would be the lre at i is maybe
what we want to log so let me reset this and redo that calculation but now on the x axis we have the
[Music] exponent of the learning rate and so we can see the exponent of the learning
rate that is good to use it would be sort of like roughly in the valley here because here the learning rates are just
way too low and then here where we expect relatively good learning rates somewhere here and then here things are
starting to explode so somewhere around negative one x the exponent of the learning rate is a pretty good setting
and 10 to the negative one is 0.1 so 0.1 is actually 0.1 was actually a fairly
good learning rate around here and that's what we had in the initial setting
but that's roughly how you would determine it and so here now we can take out the tracking of these
and we can just simply set lr to be 10 to the negative one or
basically otherwise 0.1 as it was before and now we have some confidence that this is actually a fairly good learning
rate and so now we can do is we can crank up the iterations we can reset our optimization
and we can run for a pretty long time using this learning rate
oops and we don't want to print that's way too much printing so let me again reset
and run ten thousand stops
okay so we're 0.2 2.48 roughly let's run another 10 000 steps
2.46 and now let's do one learning rate decay what this means is we're going to take
our learning rate and we're going to 10x lower it and so we're at the late stages of training potentially and we may want
to go a bit slower let's do one more actually at 0.1 just to see if
we're making a dent here okay we're still making dent and by the way the bi-gram loss that we achieved last video
was 2.45 so we've already surpassed the bi-gram model and once i get a sense that this is
actually kind of starting to plateau off people like to do as i mentioned this learning rate decay so let's try to
decay the loss the learning rate i mean
and we achieve it about 2.3 now obviously this is janky and not exactly
how you would train it in production but this is roughly what you're going through you first find a decent learning
rate using the approach that i showed you then you start with that learning rate and you train for a while
and then at the end people like to do a learning rate decay where you decay the learning rate by say a factor of 10 and
you do a few more steps and then you get a trained network roughly speaking so we've achieved 2.3 and dramatically
improved on the bi-gram language model using this simple neural net as described here
using these 3 400 parameters now there's something we have to be careful with
splitting up the dataset into train/val/test splits and why
i said that we have a better model because we are achieving a lower loss 2.3 much lower than 2.45 with the
bi-gram model previously now that's not exactly true and the reason that's not true is that
this is actually fairly small model but these models can get larger and larger if you keep adding neurons and
parameters so you can imagine that we don't potentially have a thousand parameters we could have 10 000 or 100
000 or millions of parameters and as the capacity of the neural network grows
it becomes more and more capable of overfitting your training set what that means is that the loss on the
training set on the data that you're training on will become very very low as low as zero
but all that the model is doing is memorizing your training set verbatim so if you take that model and it looks like
it's working really well but you try to sample from it you will basically only get examples exactly as they are in the
training set you won't get any new data in addition to that if you try to evaluate the loss on some withheld names
or other words you will actually see that the loss on those can be very high and so basically
it's not a good model so the standard in the field is to split up your data set into three splits as we
call them we have the training split the dev split or the validation split and the test split
so training split test or um sorry dev or validation split
and test split and typically this would be say eighty percent of your data set this could be ten percent and this ten
percent roughly so you have these three splits of the data now these eighty percent of your
trainings of the data set the training set is used to optimize the parameters of the model just like we're doing here
using gradient descent these 10 percent of the examples the dev or validation split
they're used for development over all the hyper parameters of your model so hyper parameters are for example the
size of this hidden layer the size of the embedding so this is a hundred or a two for us but we could try
different things the strength of the regularization which we aren't using yet so far
so there's lots of different hybrid parameters and settings that go into defining your neural net and you can try many different variations of them and
see whichever one works best on your validation split so this is used to train the parameters
this is used to train the hyperprimers and test split is used to evaluate
basically the performance of the model at the end so we're only evaluating the loss on the test plate very very sparingly and very
few times because every single time you evaluate your test loss and you learn something from it
you are basically starting to also train on the test split so you are only allowed to test the loss
on a test set very very few times otherwise you risk
overfitting to it as well as you experiment on your model so let's also split up our training data
into train dev and test and then we are going to train on train and only evaluate on tests very very
sparingly okay so here we go here is where we took all the words and
put them into x and y tensors so instead let me create a new cell here and let me just copy paste some code
here because i don't think it's that complex but
we're going to try to save a little bit of time i'm converting this to be a function now and this function takes some list of
words and builds the arrays x and y for those words only
and then here i am shuffling up all the words so these are the input words that we get
we are randomly shuffling them all up and then um we're going to
set n1 to be the number of examples that there's 80 of the words and n2 to be
90 of the way of the words so basically if len of words is 32 000 n1 is
well sorry i should probably run this n1 is 25 000 and n2 is 28 000.
and so here we see that i'm calling build data set to build the training set x and y
by indexing into up to and one so we're going to have only 25 000 training words
and then we're going to have roughly n2 minus n1
3 3 000 validation examples or dev examples and we're going to have
when of words basically minus and two or 3 204 examples
here for the test set so now we have x's and y's
for all those three splits
oh yeah i'm printing their size here inside the function as well
but here we don't have words but these are already the individual examples made from those words
so let's now scroll down here and the data set now for training is
more like this and then when we reset the network
when we're training we're only going to be training using x train
x train and y train so that's the only thing we're training
on
let's see where we are on the single batch let's now train maybe a few more steps
training neural networks can take a while usually you don't do it inline you launch a bunch of jobs and you wait for them to finish um can take in multiple
days and so on luckily this is a very small network
okay so the loss is pretty good oh we accidentally used a learning rate that is way too low
so let me actually come back we use the decay learning rate of 0.01
so this will train much faster and then here when we evaluate let's use the dep set here
xdev and ydev to evaluate the loss okay
and let's now decay the learning rate and only do say 10 000 examples
and let's evaluate the dev loss ones here okay so we're getting about 2.3 on dev
and so the neural network when it was training did not see these dev examples it hasn't optimized on them and yet
when we evaluate the loss on these dev we actually get a pretty decent loss and so we can also look at what the
loss is on all of training set oops and so we see that the training and the
dev loss are about equal so we're not over fitting um this model is not powerful enough to
just be purely memorizing the data and so far we are what's called underfitting
because the training loss and the dev or test losses are roughly equal so what that typically means is that our network
is very tiny very small and we expect to make performance improvements by scaling
up the size of this neural net so let's do that now so let's come over here and let's increase the size of the
experiment: larger hidden layer
neural net the easiest way to do this is we can come here to the hidden layer which currently has 100 neurons and
let's just bump this up so let's do 300 neurons and then this is also 300 biases and
here we have 300 inputs into the final layer so
let's initialize our neural net we now have ten thousand ex ten thousand parameters instead of three thousand parameters
and then we're not using this and then here what i'd like to do is i'd like to actually uh keep track of uh
tap um okay let's just do this let's keep stats
again and here when we're keeping track of the
loss let's just also keep track of the steps and let's just have i here
and let's train on thirty thousand or rather say
okay let's try thirty thousand and we are at point one
and we should be able to run this and optimize the neural net
and then here basically i want to plt.plot the steps
against the loss
so these are the x's and y's and this is the loss function and how it's being
optimized now you see that there's quite a bit of thickness to this and that's because we are optimizing over these mini batches
and the mini batches create a little bit of noise in this uh where are we in the def set we are at
2.5 so we still haven't optimized this neural net very well and that's probably because we made it
bigger it might take longer for this neural net to converge um and so let's continue training
um yeah let's just continue training
one possibility is that the batch size is so low that uh we just have way too much noise
in the training and we may want to increase the batch size so that we have a bit more um correct gradient and we're
not thrashing too much and we can actually like optimize more properly
okay this will now become meaningless because we've reinitialized these so
yeah this looks not pleasing right now but there probably is like a tiny improvement but it's so hard
to tell let's go again 2.52
let's try to decrease the learning rate by factor two
okay we're at 2.32 let's continue training
we basically expect to see a lower loss than what we had before because now we have a much much bigger model and we
were under fitting so we'd expect that increasing the size of the model should help the neural net
2.32 okay so that's not happening too well now one other concern is that even
though we've made the 10h layer here or the hidden layer much much bigger it could be that the bottleneck of the
network right now are these embeddings that are two dimensional it can be that we're just cramming way too many
characters into just two dimensions and the neural net is not able to really use that space effectively and that that is
sort of like the bottleneck to our network's performance okay 2.23 so just by decreasing the
learning rate i was able to make quite a bit of progress let's run this one more time
and then evaluate the training and the dev loss
now one more thing after training that i'd like to do is i'd like to visualize the um
embedding vectors for these characters before we scale up the
embedding size from two because we'd like to make uh this bottleneck potentially go away
but once i make this greater than two we won't be able to visualize them so here okay we're at 2.23 and 2.24
so um we're not improving much more and maybe the bottleneck now is the character embedding size which is two
visualizing the character embeddings
so here i have a bunch of code that will create a figure and then we're going to visualize
the embeddings that were trained by the neural net on these characters because right now the embedding has just two so we can
visualize all the characters with the x and the y coordinates as the two embedding locations for each of these
characters and so here are the x coordinates and the y coordinates which are the columns
of c and then for each one i also include the text of the little character
so here what we see is actually kind of interesting the network has basically learned to
separate out the characters and cluster them a little bit uh so for example you see how the vowels
a e i o u are clustered up here so that's telling us that is that the neural net treats these is very similar
right because when they feed into the neural net the embedding uh for all these characters is very similar and so the
neural net thinks that they're very similar and kind of like interchangeable if that makes sense um
then the the points that are like really far away are for example q q is kind of treated as an exception and q has a very
special embedding vector so to speak similarly dot which is a special character is all the way out here
and a lot of the other letters are sort of like clustered up here and so it's kind of interesting that there's a
little bit of structure here after the training and it's not definitely not random and
these embeddings make sense so we're now going to scale up the embedding size and won't be able to
visualize it directly but we expect that because we're under fitting and we made this layer much bigger and
did not sufficiently improve the loss we're thinking that the um constraint to better performance right
now could be these embedding pictures so let's make them bigger okay so let's scroll up here
experiment: larger embedding size
and now we don't have two dimensional embeddings we are going to have say 10 dimensional embeddings for each
word then this layer will receive 3 times 10 so 30
inputs will go into the hidden layer
let's also make the hidden layer a bit smaller so instead of 300 let's just do 200 neurons in that hidden layer
so now the total number of elements will be slightly bigger at 11 000
and then here we have to be a bit careful because um okay the learning rate we set to 0.1
here we are hardcoded in six and obviously if you're working in production you don't wanna be hard-coding magic numbers but instead of
six this should now be thirty um and let's run for fifty thousand
iterations and let me split out the initialization here outside
so that when we run this cell multiple times it's not going to wipe out our loss
in addition to that here let's instead of logging lost.item let's
actually log the let's do log 10
i believe that's a function of the loss and i'll show you why in a second let's
optimize this basically i'd like to plot the log loss instead of the loss because when you
plot the loss many times it can have this hockey stick appearance and log squashes it in
uh so it just kind of like looks nicer so the x-axis is step i and the y-axis will be the loss i
and then here this is 30. ideally we wouldn't be hard-coding these
okay so let's look at the loss okay it's again very thick because the mini batch size is very small but the
total loss over the training set is 2.3 and the the tests or the def set is 2.38
as well so so far so good uh let's try to now decrease the learning rate by a factor of 10
and train for another 50 000 iterations
we'd hope that we would be able to beat uh 2.32
but again we're just kind of like doing this very haphazardly so i don't actually have confidence that our
learning rate is set very well that our learning rate decay which we just do at random is set very well
and um so the optimization here is kind of suspect to be honest and this is not how you would do it typically in
production in production you would create parameters or hyper parameters out of all these settings and then you
would run lots of experiments and see whichever ones are working well for you
okay so we have 2.17 now and 2.2 okay so you
see how the training and the validation performance are starting to slightly slowly depart
so maybe we're getting the sense that the neural net is getting good enough or
that number of parameters is large enough that we are slowly starting to overfit
let's maybe run one more iteration of this and see where we get
but yeah basically you would be running lots of experiments and then you are slowly scrutinizing whichever ones give you the best depth performance and then
once you find all the hyper parameters that make your dev performance good you take that model and
you evaluate the test set performance a single time and that's the number that you report in your paper or wherever
else you want to talk about and brag about your model
so let's then rerun the plot and rerun the train and death
and because we're getting lower loss now it is the case that the embedding size of these was holding us back very likely
okay so 2.162.19 is what we're roughly getting so there's many ways to go from many
ways to go from here we can continue tuning the optimization we can continue for example playing with
the sizes of the neural net or we can increase the number of uh words or characters in our case that we
are taking as an input so instead of just three characters we could be taking more characters as an input and that
could further improve the loss okay so i changed the code slightly so we have here 200 000 steps of the
summary of our final code, conclusion
optimization and in the first 100 000 we're using a learning rate of 0.1 and then in the next 100 000 we're using a
learning rate of 0.01 this is the loss that i achieve and these are the performance on the
training and validation loss and in particular the best validation loss i've been able to obtain in the
last 30 minutes or so is 2.17 so now i invite you to beat this number
and you have quite a few knobs available to you to i think surpass this number so number one you can of course change
the number of neurons in the hidden layer of this model you can change the dimensionality of the embedding
lookup table you can change the number of characters that are feeding in as an input
as the context into this model and then of course you can change the details of the optimization how long are
we running what is the learning rate how does it change over time how does it decay
you can change the batch size and you may be able to actually achieve a much better convergence speed
in terms of how many seconds or minutes it takes to train the model and get
your result in terms of really good loss and then of course i actually invite you
to read this paper it is 19 pages but at this point you should actually be able to read a good chunk of this paper and
understand pretty good chunks of it and this paper also has quite a few ideas for improvements that you can play
with so all of those are not available to you and you should be able to beat this number i'm leaving that as an exercise
to the reader and that's it for now and i'll see you next time
sampling from the model
before we wrap up i also wanted to show how you would sample from the model so we're going to generate 20 samples
at first we begin with all dots so that's the context and then until we generate
the zeroth character again we're going to embed the current context
using the embedding table c now usually uh here the first dimension was the size
of the training set but here we're only working with a single example that we're generating so this is just the mission
one just for simplicity and so this embedding then gets
projected into the end state you get the logits now we calculate the probabilities for
that you can use f.softmax of logits and that just basically
exponentiates the logits and makes them sum to one and similar to cross entropy it is careful that there's no overflows
once we have the probabilities we sample from them using torture multinomial to get our next index and then we shift the
context window to append the index and record it and then we can just
decode all the integers to strings and print them out and so these are some example samples
and you can see that the model now works much better so the words here are much more word like or name like so we have
things like ham joes
you know it's starting to sound a little bit more name-like so we're definitely making progress but we can still improve
on this model quite a lot okay sorry there's some bonus content i wanted to mention that i want to make
google collab (new!!) notebook advertisement
these notebooks more accessible and so i don't want you to have to like install jupyter notebooks and torch and
everything else so i will be sharing a link to a google colab and google collab will look like a
notebook in your browser and you can just go to the url and you'll be able to execute all of the code that you saw in
the google collab and so this is me executing the code in this lecture and i shortened it a little bit but basically
you're able to train the exact same network and then plot and sample from the model and everything is ready for
you to like tinker with the numbers right there in your browser no installation necessary
so i just wanted to point that out and the link to this will be in the video description

intro
hi everyone today we are continuing our implementation of make more now in the last lecture we implemented the multier
perceptron along the lines of benj 2003 for character level language modeling so we followed this paper took in a few
characters in the past and used an MLP to predict the next character in a sequence so what we'd like to do now is
we'd like to move on to more complex and larger neural networks like recurrent neural networks and their variations
like the grw lstm and so on now before we do that though we have to stick around the level of malalia perception
on for a bit longer and I'd like to do this because I would like us to have a very good intuitive understanding of the
activations in the neural net during training and especially the gradients that are flowing backwards and how they
behave and what they look like and this is going to be very important to understand the history of the development of these architectures
because we'll see that recurr neural networks while they are very expressive in that they are a universal
approximator and can in principle Implement uh all the algorithms uh we'll see that they are not very easily
optimizable with the first order gradient based techniques that we have available to us and that we use all the time and the key to understanding why
they are not optimizable easily is to understand the the activations and the gradients and how they behave during
training and we'll see that a lot of the variants since recur neural networks have tried to improve that situation and
so that's the path that we have to take and uh let's get started so the starting code for this lecture is largely the
starter code
code from before but I've cleaned it up a little bit so you'll see that we are importing all the torch and math plb utilities
we're reading in the words just like before these are eight example words there's a total of 32,000 of them here's
a vocabulary of all the lowercase letters and the special dot token here
we are reading the data set and processing it and um creating three
splits the train Dev and the test split now in MLP this is the identical same
MLP except you see that I removed a bunch of magic numbers that we had here and instead we have the dimensionality
of the embedding space of the characters and the number of hidden units in the hidden layer and so I've pulled them
outside here uh so that we don't have to go and change all these magic numbers all the time we have the same neural net
with 11,000 parameters that we optimize now over 200,000 steps with a batch size of 32 and you'll see that I refactor I
refactored the code here a little bit but there are no functional changes I just created a few extra variables a few
more comments and I removed all the magic numbers and otherwise is the exact same thing then when we optimize we saw
that our loss looked something like this we saw that the train and Val loss were about
2.16 and so on here I refactored the uh code a little bit for the evaluation of
arbitary splits so you pass in a string of which split you'd like to evaluate and then here depending on train Val or
test I index in and I get the correct split and then this is the forward pass of the network and evaluation of the
loss and printing it so just making that nicer uh one thing that you'll notice
here is I'm using a decorator torch. nograd which you can also um look up and
read the documentation of basically what this decorator does on top of a function is that whatever happens in this
function is assumed by uh torch to never require any gradients so it will not do
any of the bookkeeping that it does to keep track of all the gradients in anticipation of an eventual backward
pass it's it's almost as if all the tensors that get created here have a required grad of false and so it just
makes everything much more efficient because you're telling torch that I will not call that backward on any of this computation and you don't need to
maintain the graph under the hood so that's what this does and you can also
use a context manager uh with torch du nograd and you can look those
up then here we have the sampling from a model um just as before just a for
Passive neural nut getting the distribution sent from it adjusting the context window and repeating until we
get the special end token and we see that we are starting to get much nicer looking words simple from the model it's
still not amazing and they're still not fully name like uh but it's much better than what we had with the BAM
model so that's our starting point now the first thing I would like to scrutinize is the initialization I can tell that our
fixing the initial loss
network is very improperly configured at initialization and there's multiple things wrong with it but let's just
start with the first one look here on the zeroth iteration the very first iteration we are recording a loss of 27
and this rapidly comes down to roughly one or two or so so I can tell that the initialization is all messed up because
this is way too high in training of neural Nets it is almost always the case that you will have a rough idea for what
loss to expect at initialization and that just depends on the loss function and the problem setup in this case I do
not expect 27 I expect a much lower number and we can calculate it together basically at initialization what we like
is that um there's 27 characters that could come next for any one training example at initialization we have no
reason to believe any characters to be much more likely than others and so we'd expect that the propy distribution that
comes out initially is a uniform distribution assigning about equal probability to all the 27
characters so basically what we' like is the probability for any character would
be roughly 1 over 20 7 that is the probability we should
record and then the loss is the negative log probability so let's wrap this in a
tensor and then then we can take the log of it and then the negative log probability is the loss we would expect
which is 3.29 much much lower than 27 and so what's happening right now is
that at initialization the neural nut is creating probity distributions that are all messed up some characters are very
confident and some characters are very not confident confident and then basically what's happening is that the network is very confidently wrong and uh
that that's what makes it um record very high loss so here's a smaller four-dimensional example of the issue
let's say we only have four characters and then we have logits that come out of the neural net and they are very very
close to zero then when we take the softmax of all zeros we get probabilities there are a diffused
distribution so sums to one and is exactly uniform and then in this case if the
label is say two it doesn't actually matter if this if the label is two or three or one or zero because it's a
uniform distribution we're recording the exact same loss in this case 1.38 so this is the loss we would expect for a
four-dimensional example and now you can see of course that as we start to manipulate these logits uh we're going
to be changing the law here so it could be that we lock out and by chance uh this could be a very high number like
you know five or something like that then case we'll record a very low loss because we're assigning the correct probability at initialization by chance
to the correct label much more likely it is that some other dimension will have a
high uh logit and then what will happen is we start to record much higher loss
and what can come what can happen is basically the logits come out like something like this you know and they
take on Extreme values and we record really high loss
um for example if we have to 4. random of four so these are uniform um sorry
these are normally distributed um numbers uh four of
them then here we can also print the logits probabilities that come out of it
and the loss and so because these logits are near zero for the most part the loss
that comes out is is okay uh but suppose this is like times 10
now you see how because these are more extreme values it's very unlikely that you're going to be guessing the correct
bucket and then you're confidently wrong and recording very high loss if your loes are coming out even more
extreme you might get extremely insane losses like infinity even at
initialization um so basically this is not good and we want the loges to be roughly zero um
when the network is initialized in fact the lits can don't have to be just zero they just have to be equal so for
example if all the logits are one then because of the normalization inside the softmax this will actually come out okay
but by symmetry we don't want it to be any arbitrary positive or negative number we just want it to be all zeros
and record the loss that we expect at initialization so let's now concretely see where things go wrong in our example
here we have the initialization let me reinitialize the neuronet and here let me break after the very first iteration
so we only see the initial loss which is 27 so that's way too high and intuitively
now we can expect the variables involved and we see that the logits here if we just print some of
these if we just print the first row we see that the Lo just take on quite extreme values and that's what's
creating the fake confidence in incorrect answers and makes the loss um
get very very high so these loes should be much much closer to zero so now let's
think through how we can achieve logits coming out of this neur not to be more closer to zero you see here that loes
are calculated as the hidden states multip by W2 plus B2 so first of all
currently we're initializing B2 as random values uh of the right size but
because we want roughly zero we don't actually want to be adding a bias of random numbers so in fact I'm going to add a times zero here to make sure that
B2 is just um basically zero at initialization and second this is H
multip by W2 so if we want logits to be very very small then we would be multiplying W2 and making that
smaller so for example if we scale down W2 by 0.1 all the elements then if I do
again just a very first iteration you see that we are getting much closer to what we expect so rough roughly what we
want is about 3.29 this is 4.2 I can make this maybe even
smaller 3.32 okay so we're getting closer and closer now you're probably
wondering can we just set this to zero then we get of course exactly what we're looking for um at
initialization and the reason I don't usually do this is because I'm I'm very nervous and I'll show you in a second
why you don't want to be setting W's or weights of a neural nut exactly to zero um you you usually want it to be small
numbers instead of exactly zero um for this output layer in this specific case
I think it would be fine but I'll show you in a second where things go wrong very quick quickly if you do that so
let's just go with 0.01 in that case our loss is close enough but has some entropy it's not
exactly zero it's got some little entropy and that's used for symmetry breaking as we'll see in a second the
logits are now coming out much closer to zero and everything is well and good so if I just erase these and I now take
away the break statement we can run the optimization with this new initialization and let's
just see what losses we record okay so I let it run and you see that we started off good
and then we came down a bit the plot of the loss uh now doesn't have this hockey shape appearance um
because basically what's happening in the hockey stick the very first few iterations of the loss what's happening
during the optimization is the optimization is just squashing down the logits and then it's rearranging the
logits so basically we took away this easy part of the loss function where just the the weights were just being
shrunk down and so therefore we're we don't we don't get these easy gains in the beginning and we're just getting
some of the hard gains of training the actual neural nut and so there's no hockey stick appearance so good things
are happening in that both number one losset initialization is what we expect and the the loss doesn't look like a
hockey stick and this is true for any neuron that you might train um and something to look out for and second the
loss that came out is actually quite a bit improved unfortunately I erased what we had here before I believe this was 2.
um2 and this was this was 2.16 so we get a slightly improved result and the
reason for that is uh because we're spending more Cycles more time optimizing the neuronet actually instead
of just uh spending the first several thousand iterations probably just squashing down the
weights because they are so way too high in the beginning in the initialization so something to look out for and uh
that's number one now let's look at the second problem let me reinitialize our neural net and let me reintroduce The
fixing the saturated tanh
Brak statement so we have a reasonable initial loss so even though everything is looking good on the level of the loss
and we get something that we expect there's still a deeper problem looking inside this neural net and its
initialization so the logits are now okay the problem now is with the values
of H the activations of the Hidden States now if we just visualize this
Vector sorry this tensor h it's kind of hard to see but the problem here roughly speaking is you see how many of the
elements are one or negative 1 now recall that torch. 10 the 10 function is
a squashing function it takes arbitrary numbers and it squashes them into a range of negative 1 and one and it does
so smoothly so let's look at the histogram of H to get a better idea of the distribution of the values inside
this tensor we can do this first well we can see that H is 32
examples and 200 activations in each example we can view it as1 to stretch it
out into one large vector and we can then call two list to
convert this into one large python list of floats and then we can pass this into
PLT doist for histogram and we say we want 50 bins and a semicolon to suppress
a bunch of output we don't want so we see this histogram and we see that most the values by far take on
value of netive one and one so this 10 H is very very active and we can also look
at basically why that is we can look at the pre activations that feed into the
10 and we can see that the distribution of the pre activations are is very very
broad these take numbers between -5 and 15 and that's why in a torure 10
everything is being squashed and capped to be in the range of negative 1 and one and lots of numbers here take on very
extreme values now if you are new to neural networks you might not actually see this as an issue but if you're well
vered in the dark arts of back propagation and then having an intuitive sense of how these gradients flow through a neural net you are looking at
your distribution of 10h activations here and you are sweating so let me show you why we have to keep in mind that
during back propagation just like we saw in microad we are doing backward passs starting at the loss and flowing through
the network backwards in particular we're going to back propagate through this torch. 10h and this layer here is made up of
200 neurons for each one of these examples and uh it implements an elementwise 10 so let's look at what
happens in 10h in the backward pass we can actually go back to our previous uh microgr code in the very first lecture
and see how we implemented 10 AG we saw that the input here was X and then we
calculate T which is the 10 age of X so that's T and T is between 1 and 1 it's
the output of the 10 H and then in the backward pass how do we back propagate through a 10 H we take out that grad um
and then we multiply it this is the chain rule with the local gradient which took the form of 1 - t ^2 so what
happens if the outputs of your t h are very close to1 or 1 if you plug in t one
here you're going to get a zero multiplying out. grad no matter what out. grad is we are killing the gradient
and we're stopping effectively the back propagation through this 10 unit similarly when t is1 this will again
become zero and out that grad just stops and intuitively this makes sense because
this is a 10h neuron and what's happening is if its output is very close to one then we are
in the tail of this 10 and so changing basically the
input is not going to impact the output of the 10 too much because it's it's so
it's in a flat region of the 10 H and so therefore there's no impact on the loss
and so so indeed the the weights and the biases along with the 10h neuron do not
impact the loss because the output of the 10 unit is in the flat region of the 10 and there's no influence we can we
can be changing them whatever we want however we want and the loss is not impacted that's so that's another way to
justify that indeed the gradient would be basically zero it vanishes indeed uh when T equals zero we
get one times out that grad so when the 10 h takes on exactly value of zero then
out grad is just passed through so basically what this is doing right is if
T is equal to zero then this the 10 unit is uh sort of inactive and uh gradient
just passes through but the more you are in the flat tails the more the gradient is squashed so in fact you'll see that
the the gradient flowing through 10 can only ever decrease and the amount that it decreases is um proportional through
a square here um depending on how far you are in the flat tail so this 10 H
and so that's kind of what's Happening Here and through this the concern here is that if all of these um outputs H are
in the flat regions of negative 1 and one then the gradients that are flowing through the network will just get
destroyed at this layer now there is some redeeming quality here and that we can actually
get a sense of the problem here as follows I wrote some code here and basically what we want to do here is we
want to take a look at H take the the absolute value and see how often it is
in the in a flat uh region so say greater than 099 and what you get is the following
and this is a Boolean tensor so uh in the Boolean tensor you get a white if
this is true and a black if this is false and so basically what we have here is the 32 examples and 200 hidden
neurons and we see that a lot of this is white and what that's telling us is that
all these 10h neurons were very very active and uh they're in a flat tail and
so in all these cases uh the back the backward gradient would get uh
destroyed now we would be in a lot of trouble if for for any one of these 200
neurons if it was the case that the entire column is white because in that
case we have what's called a dead neuron and this is could be a 10 neuron where the initialization of the weights and the biases could be such that no single
example ever activates uh this 10h in the um sort of active part of the 10age
if all the examples land in the tail then this neuron will never learn it is
a dead neuron and so just scrutinizing this and looking for Columns of
completely white uh we see that this is not the case so uh I don't see a single
neuron that is all of uh you know white and so therefore it is the case that for every one of these 10h neurons uh we do
have some examples that activate them in the uh active part of the 10 and so some gradients will flow through and this
neuron will learn and the neuron will change and it will move and it will do something but you can sometimes get get
yourself in cases where you have dead neurons and the way this manifests is that um for 10h neuron this would be
when no matter what inputs you plug in from your data set this 10h neuron always fir completely one or completely negative
one and then it will just not learn because all the gradients will be just zeroed out uh this is true not just for
10 but for a lot of other nonlinearities that people use in neural networks so we certainly used 10 a lot but sigmoid will
have the exact same issue because it is a squashing neuron and so the same will be true for sigmoid uh but um but um you
know um basically the same will actually apply to sigmoid the same will also apply to reu
so reu has a completely flat region here below zero so if you have a reu neuron
then it is a pass through um if it is positive and if it's if the preactivation is negative it will just
shut it off since the region here is completely flat then during back propagation uh this would be exactly
zeroing out the gradient um like all of the gradient would be set exactly to zero instead of just like a very very
small number depending on how positive or negative T is and so you can get for
example a dead reu neuron and a dead reu neuron would basically look like
basically what it is is if a neuron with a reu nonlinearity never activates so
for any examples that you plug in in the data set it never turns on it's always in this flat region then this re neuron
is a dead neuron its weights and bias will never learn they will never get a gradient because the neuron never
activated and this can sometimes happen at initialization uh because the way and a biases just make it so that by chance
some neurons are just forever dead but it can also happen during optimization if you have like a too high of learning
rate for example sometimes you have these neurons that get too much of a gradient and they get knocked out off
the data manifold and what happens is that from then on no example ever activates this
neuron so this neuron remains dead forever so it's kind of like a permanent brain damage in a in a mind of a network
and so sometimes what can happen is if your learning rate is very high for example and you have a neural net with neurons you train the neuron net and you
get some last loss but then actually what you do is you go through the entire training set and you forward um your
examples and you can find neurons that never activate they are dead neurons in your network and so those neurons will
will never turn on and usually what happens is that during training these Rel neurons are changing moving Etc and
then because of a high gradient somewhere by chance they get knocked off and then nothing ever activates them and
from then on they are just dead uh so that's kind of like a permanent brain damage that can happen to some of these
neurons these other nonlinearities like leyu will not suffer from this issue as much because you can see that it doesn't
have flat Tails you'll almost always get gradients and uh elu is also fairly uh
frequently used um it also might suffer from this issue because it has flat parts so that's just something to be
aware of and something to be concerned about and in this case we have way too many um activations AG that take on
Extreme values and because there's no column of white I think we will be okay
and indeed the network optimizes and gives us a pretty decent loss but it's just not optimal and this is not
something you want especially during initialization and so basically what's happening is that uh this H
preactivation that's floating to 10 H it's it's too extreme it's too large
it's creating very um it's creating a distribution that is too saturated in both sides of the 10 H and it's not
something you want because it means that there's less training uh for these neurons because they update um less
frequently so how do we fix this well H preactivation is MCAT which comes from C
so these are uniform gsan but then it's multiply by W1 plus B1 and H preact is
too far off from zero and that's causing the issue so we want this reactivation to be closer to zero very similar to
what we had with logits so here we want actually something very very
similar now it's okay to set the biases to very small number we can either multiply by 0 01 to get like a little
bit of entropy um I sometimes like to do that um just so that there's like a
little bit of variation and diversity in the original initialization of these 10 H neurons and I find in practice that
that can help optimization a little bit and then the weights we can also just like squash so let's multiply everything
by 0.1 let's rerun the first batch and now let's look at this and well first let's
look here you see now because we multiply dou by 0.1 we have a much better histogram
and that's because the pre activations are now between 1.5 and 1.5 and this we expect much much less white okay there's
no white so basically that's because there are no neurons that saturated
above 99 in either direction so this actually a pretty decent place to be um
maybe we can go up a little bit sorry am I am I changing W1 here so
maybe we can go to 0 2 okay so maybe something like this is
is a nice distribution so maybe this is what our initialization should be so let me now
erase these and let me starting with
initialization let me run the full optimization without the break and uh let's see what
we get okay so the optimization finished and I re the loss and this is the result that we get and then just as a reminder
I put down all the losses that we saw previously in this lecture so we see that we actually do get an improvement
here and just as a reminder we started off with a validation loss of 2.17 when we started by fixing the softmax being
confidently wrong we came down to 2.13 and by fixing the 10h layer being way too saturated we came down to 2.10
and the reason this is happening of course is because our initialization is better and so we're spending more time doing productive training instead of um
not very productive training because our gradients are set to zero and uh we have to learn very simple things like uh the
overconfidence of the softmax in the beginning and we're spending Cycles just like squashing down the weight Matrix so
this is illustrating um basically initialization and its impacts on performance uh just by being aware of
the internals of these neural net and their activations their gradients now we're working with a very small Network
this is just one layer multi-layer perception so because the network is so shallow the optimization problem is
actually quite easy and very forgiving so even though our initialization was terrible the network still learned
eventually it just got a bit worse result this is not the case in general though once we actually start um working
with much deeper networks that have say 50 layers uh things can get uh much more complicated and uh these problems stack
up and so you can actually get into a place where the network is basically not training at all if your initialization
is bad enough and the deeper your network is and the more complex it is the less forgiving it is to some of
these errors and so um something to definitely be aware of and uh something
to scrutinize something to plot and something to be careful with and um yeah
calculating the init scale: “Kaiming init”
okay so that's great that that worked for us but what we have here now is all these magic numbers like0 2 like where
do I come up with this and how am I supposed to set these if I have a large neural net with lots and lots of layers
and so obviously no one does this by hand there's actually some relatively principled ways of setting these scales
um that I would like to introduce to you now so let me paste some code here that I prepared just to motivate the
discussion of this so what I'm doing here is we have some random input here x that is drawn
from a gan and there's 1,000 examples that are 10 dimensional and then we have a waiting layer here
that is also initialized using caution just like we did here and we these
neurons in the hidden layer look at 10 inputs and there are 200 neurons in this hidden layer and then we have here just
like here um in this case the multiplication X multip by W to get the pre activations of these
neurons and basically the analysis here looks at okay suppose these are uniform gion and these weights are uniform gion
if I do X W and we forget for now the bias and the
nonlinearity then what is the mean and the standard deviation of these gions so in the beginning here the input is uh
just a normal Gan distribution mean zero and the standard deviation is one and the standard deviation again is just the
measure of a spread of the gion but then once we multiply here and we look at the um histogram of Y we see
that the mean of course stays the same it's about zero because this is a symmetric operation but we see here that
the standard deviation has expanded to three so the input standard deviation was one but now we've grown to three and
so what you're seeing in the histogram is that this Gan is expanding and so um we're expanding this
Gan um from the input and we don't want that we want most of the neural net to have relatively similar activations uh
so unit gion roughly throughout the neural net and so the question is how do we scale these W's to preserve the uh um
to preserve this distribution to uh remain aan and so intuitively if I multiply
here uh these elements of w by a larger number let's say by
five then this gsan gross and gross in standard deviation so now we're at 15 so
basically these numbers here in the output y take on more and more extreme values but if we scale it down like .2
then conversely this Gan is getting smaller and smaller and it's shrinking
and you can see that the standard deviation is 6 and so the question is what do I multiply by here to exactly
preserve the standard deviation to be one and it turns out that the correct answer mathematically when you work out
through the variance of uh this multiplication here is that you are supposed to divide by the square root of
the fan in the fan in is the basically the uh number of input elements here 10
so we are supposed to divide by 10 square root and this is one way to do the square root you raise it to a power
of 0. five that's the same as doing a square root so when you divide by the um
square root of 10 then we see that the output caution it has exactly standard
deviation of one now unsurprisingly a number of papers have looked into how
but to best initialized neural networks and in the case of multilayer perceptrons we can have fairly deep
networks that have these nonlinearity in between and we want to make sure that the activations are well behaved and
they don't expand to infinity or Shrink all the way to zero and the question is how do we initialize the weights so that
these activations take on reasonable values throughout the network now one paper that has studied this in quite a
bit of detail that is often referenced is this paper by King hatal called delving deep into rectifiers now in this
case they actually study convolution neur neurals and they study especially the reu nonlinearity and the p
nonlinearity instead of a 10h nonlinearity but the analysis is very similar and um basically what happens
here is for them the the relu nonlinearity that they care about quite a bit here is a squashing function where
all the negative numbers are simply clamped to zero so the positive numbers are pass through but everything negative
is just set to zero and because uh you are basically throwing away half of the distribution they find in their analysis
of the forward activations in the neural that you have to compensate for that with a
gain and so here they find that basically when they initialize their
weights they have to do it with a zero mean Gan whose standard deviation is square < TK of 2 over the Fanon what we
have here is we are initializing gashin with the square root of Fanon this NL
here is the Fanon so what we have is sare root of one over the Fanon because
we have the division here now they have to add this factor of two because of the reu which basically
discards half of the distribution and clamps it at zero and so that's where you get an additional Factor now in
addition to that this paper also studies not just the uh sort of behavior of the activations in the forward pass of the
neural net but it also studies the back propagation and we have to make sure that the gradients also are well behaved
and so um because ultimately they end up updating our parameters and what they
find here through a lot of analysis that I invite you to read through but it's not exactly approachable what they find
is basically if you properly initialize the forward pass the backward pass is also approximately initialized up to a
constant factor that has to do with the size of the number of um hidden neurons
in an early and a late layer and uh but basically they find
empirically that this is not a choice that matters too much now this timing initialization is also implemented in
pytorch so if you go to torch. and then. init documentation you'll find climing normal and in my opinion this is
probably the most common way of initializing neural networks now and it takes a few keyword arguments here so
number one it wants to know the mode would you like to normalize the activations or would you like to
normalize the gradients to to be always uh gsh in with zero mean and a unit or
one standard deviation and because they find in the paper that this doesn't matter too much most of the people just
leave it as the default which is Fan in and then second passing the nonlinearity that you are using because depending on
the nonlinearity we need to calculate a slightly different gain and so if your nonlinearity is just um linear so
there's no nonlinearity then the gain here will be one and we have the exact same uh kind of formula that we've come
up here but if the nonlinearity is something else we're going to get a slightly different gain and so if we
come up here to the top we see that for example in the case of reu this gain is a square root of two and the reason it's
a square root because in this
paper you see how the two is inside of the square root so the gain is a square
root of two in the case of linear or identity we just get a gain of one in a
case of 10 H which is what we're using here the advised gain is a 5 over3 and intuitively why do we need a gain on top
of the initialization is because 10 just like reu is a contractive uh
transformation so that means is you're taking the output distribution from this matrix multiplication and then you are
squashing it in some way now reu squashes it by taking everything below zero and clamping it to zero 10 also
squashes it because it's a contractive operation it will take the Tails and it will squeeze them in and so in order to
fight the squeezing in we need to boost the weights a little bit so that we renormalize everything back to standard
unit standard deviation so that's why there's a little bit of a gain that comes out now I'm skipping through this
section A little bit quickly and I'm doing that actually intentionally and the reason for that is because about 7
years ago when this paper was written you had to actually be extremely careful with the activations and ingredients and
their ranges and their histograms and you had to be very careful with the precise setting of gains and the scrutinizing of the nonlinearities used
and so on and everything was very finicky and very fragile and to be very properly arranged for the neural nut to
train especially if your neural nut was very deep but there are a number of modern innovations that have made everything significantly more stable and
more well behaved and it's become less important to initialize these networks exactly right and some of those modern
Innovations for example are residual connections which we will cover in the future the use of a number of uh
normalization uh layers like for example batch normalization layer normalization group normalization we're going to go
into a lot of these as well and number three much better optimizers not just stochastic gradient descent the simple
Optimizer we're basically using here but a slightly more complex optimizers like ARS prop and especially Adam and so all
of these modern Innovations make it less important for you to precisely calibrate the neutralization of the neural net all
that being said in practice uh what should we do in practice when I initialize these neurals I basically
just uh normalize my weights by the square root of the Fanon uh so basically
uh roughly what we did here is what I do now if we want to be exactly accurate here we and go by um in it of uh timing
normal this is how it would implemented we want to set the standard deviation to be gain over the square root of fan in
right so to set the standard deviation of our weights we will proceed as
follows basically when we have a torch. Ranon and let's say I just create a th numbers we can look at the standard
deviation of this and of course that's one that's the amount of spread let's make this a bit bigger so it's closer to one so that's the spread of the Gan of
zero mean and unit standard deviation now basically when you take these and you multiply by
say2 that basically scales down the Gan and that makes it standard deviation 02
so basically the number that you multiply by here ends up being the standard deviation of this caution so
here this is a um standard deviation point2 caution here when we sample our
W1 but we want to set the standard deviation to gain over square root of
fan mode which is Fanon so in other words we want to mul mly by uh gain
which for 10 H is 5 over3 5 over3 is the gain and then
times
um or I guess sorry divide uh square root of the fan in and
in this example here the fan in was 10 and I just noticed that actually here the fan in for W1 is is actually an
embed times block size which as you all recall is actually 30 and that's because each character is 10 dimensional but
then we have three of them and we can catenate them so actually the fan in here was 30 and I should have used 30 here probably but basically we want 30
uh square root so this is the number this is what our standard deviation we want to be and this number turns out to
be3 whereas here just by fiddling with it and looking at the distribution and making sure it looks okay uh we came up
with 02 and so instead what we want to do here is we want to make the standard deviation b
um 5 over3 which is our gain divide this
amount times2 square root and these brackets here are not that uh necessary
but I'll just put them here for clarity this is basically what we want this is the timing in it in our case for a 10h
nonlinearity and this is how we would initialize the neural net and so we're multiplying by .3 instead of multiplying
by .2 and so we can we can initialize this
way and then we can train the neural net and see what we get okay so I trained the neural net and we end up in roughly
the same spot so looking at the validation loss we now get 2.10 and previously we also had 2.10 there's a
little bit of a difference but that's just the randomness of the process I suspect but the big deal of course is we get to the same spot but we did not have
to introduce any um magic numbers that we got from just looking at histograms
and guessing checking we have something that is semi- principled and will scale us to uh much bigger networks and uh
something that we can sort of use as a guide so I mentioned that the precise setting of these initializations is not
batch normalization
as important today due to some Modern Innovations and I think now is a pretty good time to introduce one of those modern Innovations and that is batch
normalization so bat normalization came out in uh 2015 from a team at Google and
it was an extremely impact paper because it made it possible to train very deep neuron Nets quite reliably and uh it
basically just worked so here's what bash rization does and let's implement it
um basically we have these uh hidden States H preact right and we were
talking about how we don't want these uh these um preactivation states to be way
too small because the then the 10 H is not um doing anything but we don't want them to be too large because then the 10
H is saturated in fact we want them to be roughly roughly Gan so zero mean and
a unit or one standard deviation at least at initialization so the Insight from the
bachor liation paper is okay you have these hidden States and you'd like them to be roughly Gan then why not take the
hidden States and uh just normalize them to be Gan and it sounds kind of crazy but you
can just do that because uh standardizing hidden States so that
their unit caution is a perfect ly differentiable operation as we'll soon see and so that was kind of like the big Insight in this paper and when I first
read it my mind was blown because you can just normalize these hidden States and if you'd like unit Gan States in
your network uh at least initialization you can just normalize them to be unit gion so uh let's see how that works so
we're going to scroll to our preactivation here just before they enter into the 10h now the idea again is
remember we're trying to make these roughly Gan and that's because if these are way too small numbers then the 10 H
here is kind of inactive but if these are very large numbers then the 10 H is
way too saturated and gr is no flow so we'd like this to be roughly goshan so
the Insight in Bat normalization again is that we can just standardize these activations so they are exactly Gan so
here H preact has a shapee of 32 by 200 32
examples by 200 neurons in the hidden layer so basically what we can do is we can take H pract and we can just
calculate the mean um and the mean we want to calculate across the zero
Dimension and we want to also keep them as true so that we can easily broadcast
this so the shape of this is 1 by 200 in other words we are doing the mean over
all the uh elements in the batch and similarly we can calculate the
standard deviation of these activations and that will also be 1 by
200 now in this paper they have the uh sort of prescription here and see
here we are calculating the mean which is just taking uh the average
value of any neurons activation and then the standard deviation is basically kind
of like um this the measure of the spread that we've been using which is
the distance of every one of these values away from the mean and that
squared and averaged that's the that's the variance
and then if you want to take the standard deviation you would square root the variance to get the standard
deviation so these are the two that we're calculating and now we're going to normalize or standardize these X's by
subtracting the mean and um dividing by the standard deviation so basically
we're taking in pract and we subtract the mean
and then we divide by the standard deviation this is exactly what these two
STD and mean are calculating oops sorry this is the mean and this is
the variance you see how the sigma is a standard deviation usually so this is Sigma Square which the variance is the
square of the standard deviation so this is how you standardize these values and what this will do is
that every single neuron now and its firing rate will be exactly unit Gan on these 32 examples at least of this batch
that's why it's called batch normalization we are normalizing these batches and then we could in principle
train this notice that calculating the mean and your standard deviation these are just mathematical formulas they're
perfectly differentiable all of this is perfectly differentiable and we can just train this the problem is you actually
won't achieve a very good result with this and the reason for that
is we want these to be roughly Gan but only at initialization uh but we don't
want these be to be forced to be Garian always we we'd like to allow the neuron
net to move this around to potentially make it more diffuse to make it more sharp to make some 10 neurons maybe be
more trigger more trigger happy or less trigger happy so we'd like this distribution to move around and we'd
like the back propagation to tell us how the distribution should move around and so in addition to this idea of
standardizing the activations that any point in the network uh we have to also
introduce this additional component in the paper here described as scale and shift and so basically what we're doing
is we're taking these normalized inputs and we are additionally scaling them by some gain and offsetting them by some
bias to get our final output from this layer and so what that amounts to is the
following we are going to allow a batch normalization gain to be initialized at
just uh once and the ones will be in the shape of 1 by n
hidden and then we also will have a BN bias which will be torch. zeros and it
will also be of the shape n by 1 by n hidden and then here the BN gain will
multiply this and the BN bias will offset it here so because this is initialized to
one and this to zero at initialization each neurons firing values in this batch will be
exactly unit gion and will have nice numbers no matter what the distribution of the H pract is coming in coming out
it will be un Gan for each neuron and that's roughly what we want at least at initialization um and then during
optimization we'll be able to back propagate into BN gain and BM bias and change them so the network is given the
full ability to do with this whatever it wants uh internally here we just have to make
sure sure that we um include these in the parameters of the neural nut because
they will be trained with back propagation so let's initialize this and
then we should be able to
train and then we're going to also copy this line which is the batch
normalization layer here on a single line of code and we're going to swing down here and we're also going to do the
exact same thing at test time here so similar to train time we're
going to normalize uh and then scale and that's going to give us our train and validation
loss and we'll see in a second that we're actually going to change this a little bit but for now I'm going to keep it this way so I'm just going to wait
for this to converge okay so I allowed the neural nut to converge here and when we scroll down we see that our
validation loss here is 2.10 roughly which I wrote down here and we see that
this is actually kind of comparable to some of the results that we've achieved uh previously now I'm not actually
expecting an improvement in this case and that's because we are dealing with a very simple neural nut that has just a
single hidden layer so in fact in this very simple case of just one hidden layer we were able to actually calculate
what the scale of w should be to make these pre activations already have a roughly Gan shape so the bat
normalization is not doing much here but you might imagine that once you have a much deeper neural nut that has lots of
different types of operations and there's also for example residual connections which we'll cover and so on
it will become basically very very difficult to tune the scales of your weight matrices such that all the
activations throughout the neural nut are roughly gsen and so that's going to become very quickly intractable but
compared to that it's going to be much much easier to sprinkle batch normalization layers throughout the neural net so in particular it's common
to to look at every single linear layer like this one one this is a linear layer multiplying by a weight Matrix and adding a bias or for example
convolutions which we'll cover later and also perform basically a multiplication with a weight Matrix but in a more
spatially structured format it's custom it's customary to take these linear layer or convolutional layer and append
a b rization layer right after it to control the scale of these activations at every point in the neural nut so we'd
be adding these bom layers throughout the neural nut and then this controls the scale of these AC ations throughout
the neural net it doesn't require us to do uh perfect mathematics and care about the activation distributions uh for all
these different types of neural network uh Lego building blocks that you might want to introduce into your neural net
and it significantly stabilizes uh the training and that's why these uh layers are quite popular now the stability
offered by bash normalization actually comes at a terrible cost and that cost is that if you think about what's
Happening Here something something terribly strange and unnatural is happening it used to be that we have a
single example feeding into a neural nut and then uh we calculate its activations and its loits and this is a
deterministic sort of process so you arrive at some logits for this example and then because of efficiency of
training we suddenly started to use batches of examples but those batches of examples were processed independently
and it was just an efficiency thing but now suddenly in batch normalization because of the normalization through the
batch we are coupling these examples mathematically and in the forward pass and the backward pass of a neural l so
now the hidden State activations H pract in your log jits for any one input
example are not just a function of that example and its input but they're also a function of all the other examples that
happen to come for a ride in that batch and these examples are sampled randomly
and so what's happening is for example when you look at H pract that's going to feed into H the hidden State activations
for for example for for any one of these input examples is going to actually change slightly depending on what other
examples there are in a batch and and depending on what other examples happen to come for a ride H is going to change
subtly and it's going to like Jitter if you imagine sampling different examples because the the statistics of the mean
and the standard deviation are going to be impacted and so you'll get a Jitter for H and you'll get a Jitter for
loits and you think that this would be a bug uh or something undesirable but in a
very strange way this actually turns out to be good in your Network training and
as a side effect and the reason for that is that you can think of this as kind of like a regularizer because what's
happening is you have your input and you get your age and then depending on the other examples this is jittering a bit
and so what that does is that it's effectively padding out any one of these input examples and it's introducing a
little bit of entropy and um because of the padding out it's actually kind of like a form of a data augmentation which
we'll cover in the future and it's kind of like augmenting the input a little bit and jittering it and that makes it
harder for the neural nut to overfit to these concrete specific examples so by introducing all this noise it actually
like Pats out the examples and it regularizes the neural nut and that's one of the reasons why uh deceivingly as
a second order effect uh this is actually a regularizer and that has made it harder uh for us to remove the use of
batch normalization uh because basically no one likes this property that the the examples in the batch are coupled
mathematically and in the forward pass and at least all kinds of like strange uh results uh we'll go into some of that
in a second as well um and it leads to a lot of bugs and um and so on and so no
one likes this property uh and so people have tried to um deprecate the use of
bat normalization and move to other normalization techniques that do not couple the examples of a batch examples
are ler normalization instance normalization group normalization and so on and we'll come we'll come some these
uh later um but basically long story short bat normalization was the first
kind of normalization layer to be introduced it worked extremely well it happened to have this regularizing
effect it stabilized training and people have been trying to remove it and move
to some of the other normalization techniques uh but it's been hard because it it just works quite well and some of
the reason that it works quite well is again because of this regular rizing effect and because of the because it is
quite effective at um controlling the activations and their distributions uh so that's kind of like
the brief story of Bash normalization and I'd like to show you one of the other weird sort of outcomes of this
coupling so here's one of the strange outcomes that I only glossed over previously when I was evaluating the
loss on the validation set basically once we've trained a neural net we'd like to deploy it in some kind of a
setting and we'd like to be able to feed in a single individual example and get a prediction out from our neural net but
how do we do that when our neural net now in a forward pass estimates the statistics of the mean understand
deviation of a batch the neur lot expects batches as an input now so how do we feed in a single example and get
sensible results out and so the proposal in the batch normalization paper is the
following what we would like to do here is we would like to basically have a step after training that uh calculates
and sets the bach uh mean and standard deviation a single time over the training set and so I wrote this code
here in interest of time and we're going to call what's called calibrate the bachor statistics and basically what we
do is torch torch. nograd telling pytorch that none of this we will call
Dot backward on and it's going to be a bit more efficient we're going to take the training set get the pre activations
for every single training example and then one single time estimate the mean and standard deviation over the entire
training set and then we're going to get B and mean and B and standard deviation and now these are fixed numbers
estimating over the entire training set and here instead of estimating it
dynamically we are going to instead here use B and mean and here we're just going to use B
and standard deviation and so at test time we are going to fix these clamp them and use
them during inference and now you see that we get basically
identical result uh but the benefit that we've gained is that we can now also forward a single example because the
mean and standard deviation are now fixed uh sort of tensor that said nobody actually wants to
estimate this mean and standard deviation as a second stage after neural network training because everyone is
lazy and so this batch normalization paper actually introduced one more idea which is that we are can we can estimate
the mean and standard deviation in a running man running manner during training of the neuron nut and then we
can uh simply just have a single stage of training and on the side of that training we are estimating the running
mean and standard deviation so let's see what that would look like let me basically take the mean here that we are
estimating on the batch and let me call this B and mean on the I iteration um and then here this is BN
sdd um bnsd at I
okay uh and the mean comes here and the STD comes here so so far I've done
nothing I've just uh moved around and I created these EXT extra variables for the mean and standard deviation and I've
put them here so so far nothing has changed but what we're going to do now is we're going to keep running mean of
both of these values during training so let me swing up here and let me create a BN meanor running and I'm going to
initialize it at uh zeros and then BN STD running which I'll
initialize at once because um in the beginning because
of the way we ized W1 uh and B1 H pract will be roughly unit gion so the mean
will be roughly zero and a standard deviation roughly one so I'm going to initialize these that way but then here
I'm going to update these and in pytorch um uh these uh mean and standard
deviation that are running uh they're not actually part of the gradient based optimization we're never going to derive gradients with respect to them they're
they're updated on the side of training and so what we're going to do here is we're going to say with torch. nograd
telling pytorch that the update here is not supposed to be building out a graph because there will be no dot
backward but this running is basically going to be 0.99 uh9 times the current
Value Plus 0.001 times the um this value
this new mean and in the same way bnsd running will be mostly what it used to be
but it will receive a small update in the direction of what the current standard deviation
is and as you're seeing here this update is outside and on the side of the
gradient based optimization and it's simply being updated not using gradient scent it's just being updated using U
janky like Smooth um sort of uh running
mean Manner and so while the network is training and these pre activations are
sort of changing and shifting around during during back propagation we are keeping track of the typical mean and
standard deviation and we're estimating them once and when I run
this now I'm keeping track of this in the running Manner and what we're hoping for of course is that the me BN meore
running and BN meore STD are going to be very similar to the ones that we
calculated here before and that way we don't need a second stage because we've sort of combined the two stages and
we've put them on the side of each other if you want to look at it that way and this is how this is also implemented in
The Bash normalization uh layer impi torch so during training um the exact
same thing will happen and then later when you're using inference it will use the estimated running mean of both the
mean and standard deviation of those hidden States so let's wait for the optimization to converge and hopefully
the running mean and standard deviation are roughly equal to these two and then we can simply use it here and we don't
need this stage of explicit calibration at the end okay so the optimization finished I'll rerun the explicit
estimation and then the B and mean from the explicit estimation is here and B
and mean from the running estimation during the during the optimization you
can see is very very similar it's not identical but it's pretty close and the same way BN STD is this
and BN STD running is this and so you can see that once again they are fairly
similar values not identical but pretty close and so then here instead of being
mean we can use the BN mean running instead of bnsd we can use bnsd
running and uh hopefully the validation loss will not be impacted too much okay so it's basically identical
and this way we've eliminated the need for this explicit stage of calibration because we are doing it in line over
here okay so we're almost done with batch normalization there are only two more notes that I'd like to make number
one I've skipped a discussion over what is this plus Epsilon doing here this Epsilon is usually like some small fixed
number for example one5 by default and what it's doing is that it's basically preventing a division by zero in the
case that the variance over your batch is exactly zero in that case uh here we
normally have a division by zero but because of the plus Epsilon uh this is going to become a small number in the denominator instead and things will be
more well behaved so feel free to also add a plus Epsilon here of a very small number it doesn't actually substantially
change the result I'm going to skip it in our case just because uh this is unlikely to happen in our very simple example here and the second thing I want
you to notice is that we're being wasteful here and it's very subtle but right here where we are adding the bias
into H preact these biases now are actually useless because we're adding
them to the H preact but then we are calculating the mean for every one of these neurons and subtracting it so
whatever bias you add here is going to get subtracted right here and so these
biases are not doing anything in fact they're being subtracted out and they don't impact the rest of the calculation
so if you look at b1. grad it's actually going to be zero because it's being subtracted out and doesn't actually have
any effect and so whenever you're using bash normalization layers then if you have any weight layers before like a
linear or a c or something like that you're better off coming here and just like not using bias so you don't want to
use bias and then here you don't want to add it because it's that spirous instead
we have this B normalization bias here and that b rization bias is now in charge of the biasing of this
distribution instead of this B1 that we had here originally and so uh basically
bash normalization layer has its own bias and there's no need to have a bias in the layer before it because that bias
is going to be subtracted out anyway so that's the other small detail to be careful with sometimes it's not going to
do anything catastrophic this B1 will just be useless it will never get any gradient uh it will not learn it will
stay constant and it's just wasteful but it doesn't actually really uh impact anything otherwise okay so I rearranged
batch normalization: summary
the code a little bit with comments and I just wanted to give a very quick summary of The Bash normalization layer
we are using bash normalization to control the statistics of activations in the neural net it is common to sprinkle
bash normalization layer across the neural net and usually we will place it after layer that have multiplications
like for example a linear layer or convolutional layer which we may cover in the future now the bat normalization
internally has parameters for the gain and the bias and these are trained using
back propagation it also has two buffers the buffers are the mean and the
standard deviation the running mean and the running mean of the standard deviation and these are not trained
using back propagation these are trained using this uh janky update of kind of like a running mean
update so um these are sort of the parameters and the buffers of Bator layer and then
really what it's doing is it's calculating the mean and a standard deviation of the activations uh that are
feeding into the Bator layer over that batch then it's centering that batch to
be unit gion and then it's offsetting and scaling it by the Learned bias and
gain and then on top of that it's keeping track of the mean and standard deviation of the inputs and it's
maintaining this running mean and standard deviation and this will later be used at inference so that we don't
have to reestimate the mean stand deviation all the time and in addition that allows us to basically forward
individual examples at test time so that's the bash normalization layer it's a fairly complicated layer um but this
is what it's doing internally now I wanted to show you a little bit of a real example so you can search resnet
real example: resnet50 walkthrough
which is a residual neural network and these are common types of neural networks used for image
classification and of course we haven't come resnets in detail so I'm not going to explain all the pieces of it but for
now just note that the image feeds into a reset on the top here and there's many many layers with repeating structure all
the way to predictions of what's inside that image this repeating structure is made up of these blocks and these blocks
are just sequentially stacked up in this deep neural network now the code for
this uh the block basically that's used and repeated sequentially in series is
called this bottleneck block bottleneck block and there's a lot here this is all
pych and of course we haven't covered all of it but I want to point out some small pieces of it here in the init is
where we initialize the neuronet so this code of block here is basically the kind of stuff we're doing here we're initializing all the layers and in the
forward we are specifying how the neuron lot acts once you actually have the input so this code here is along the
lines of what we're doing here and now these blocks are replicated
and stacked up serially and that's what a residual Network would be and so
notice What's Happening Here com one um these are convolution layers and these
convolution layers basically they're the same thing as a linear layer except convolutional layers don't apply um
convolutional layers are used for images and so they have SP structure and basically this linear multiplication and
bias offset are done on patches instead of math instead of the full input so
because these images have structure spatial structure convolutions just basically do WX plus b but they do it on
overlapping patches of the input but otherwise it's WX plus P then we have the norm layer which by
default here is initialized to be a bash Norm in 2D so two- dimensional bash normalization layer and then we have a
nonlinearity like reu so instead of uh here they use reu we are using 10 in
this case but both both are just nonlinearities and you can just use them relatively interchangeably for very deep
networks re typically empirically work a bit better so see the motif that's being
repeated here we have convolution bat normalization reu convolution bat normalization re Etc and then here this
is residual connection that we haven't covered yet but basically that's the exact same pattern we have here with we
have a weight layer like a convolution or like a linear layer bash
normalization and then 10h which is nonlinearity but basically a weight
layer a normalization layer and nonlinearity and that's the motif that you would be stacking up when you create
these deep neural networks exactly as it's done here and one more thing I'd like you to notice is that here when
they are initializing the com layers like com 1 by one the depth for that is
right here and so it's initializing an nn. Tod which is a convolution layer in pytorch and there's a bunch of keyword
arguments here that I'm not going to explain yet but you see how there's bias equals false the bias equals false is
exactly for the same reason as bias is not used in our case you see how I eras
the use of bias and the use of bias is spous because after this weight layer there's a bash normalization and The
Bash normalization subtracts that bias and then has its own bias so there's no need to introduce these spous parameters
it wouldn't hurt performance it's just useless and so because they have this motif of C Bast umbrell they don't need
a bias here because there's a bias inside here so by the way this example
here is very easy to find just do resonet pie torch and uh it's this example here so
this is kind of like the stock implementation of a residual neural network in pytorch and uh you can find
that here but of course I haven't covered many of these parts yet and I would also like to briefly descend into
the definitions of these pytorch layers and the the parameters that they take now instead of a convolutional layer
we're going to look at a linear layer uh because that's the one that we're using here this is a linear layer and I
haven't cover covered convolutions yet but as I mentioned convolutions are basically linear layers except on
patches so a linear layer performs a WX plus b except here they're calling the W
A transpose um so to calcul WX plus b very much like we did here to initialize this
layer you need to know the fan in the fan out and that's so that they can
initialize this W this is the fan in and the fan out so they know how how big the
weight Matrix should be you need to also pass in whether you whether or not you want a bias and if you set it to false
then no bias will be uh inside this layer um and you may want to do that
exactly like in our case if your layer is followed by a normalization layer such as batch
Norm so this allows you to basically disable a bias now in terms of the initial ation if we swing down here this
is reporting the variables used inside this linear layer and our linear layer
here has two parameters the weight and the bias in the same way they have a weight and a bias and they're talking
about how they initialize it by default so by default P will initialize your weights by taking the
Fanon and then um doing one over fanin square root and then instead of a normal
distribution they are using a uniform distribution so it's very much the same thing but
they are using a one instead of 5 over three so there's no gain being calculated here the gain is just one but
otherwise is exactly one over the square root of fan in exactly as we have
here so one over the square root of K is the is the scale of the weights but when
they are drawing the numbers they're not using a gussion by default they're using a uniform distribution by default and so
they draw uniformly from negative of K to squ of K but it's the exact same thing and the
same motivation from for with respect to what we've seen in this lecture and the
reason they're doing this is if you have a roughly gsan input this will ensure that out of this layer you will have a
roughly Gan output and you you basically achieve that by scaling the weights by
one over the square root of fan in so that's what this is doing and then the second thing is the
bash normalization layer so let's look at what that looks like in pytorch so here we have a onedimensional b
normalization layer exactly as we are using here and there are a number of keyword arguments going into it as well
so we need to know the number of features uh for us that is 200 and that is needed so that we can initialize
these parameters here the gain the bias and the buffers for the running uh mean
and standard deviation then they need to know the value of Epsilon here and by default
this is one5 you don't typically change this too much then they need to know the momentum
and the momentum here as they explain is basically used for these uh running mean and running standard deviation so by
default the momentum here is 0.1 the momentum we are using here in this example is
0.001 and basically rough you may want to change this sometimes and roughly
speaking if you have a very large batch size then typically what you'll see is that when you estimate the mean and the
standard deviation for every single batch size if it's large enough you're going to get roughly the same result
and so therefore you can use slightly higher momentum like 0.1 but for a batch size as small as 32
the mean and standard deviation here might take on slightly different numbers because there's only 32 examples we are using to estimate the mean and standard
deviation so the value is changing around a lot and if your momentum is 0.1
that that might not be good enough for this value to settle and um converge to
the actual mean and standard deviation over the entire training set and so basically if your batch size is very
small uh momentum of 0.1 is potentially dangerous and it might make it so that the running uh mean and stand deviation
are is thrashing too much during training and it's not actually converging
properly uh aine equals true determines whether this batch normalization layer has these learnable Aline parameters the
uh the gain and the bias and this is almost always kept to true I'm not actually sure why you would want to
change this to false um then track running stats is determining
whether or not B rization layer of pytorch will be doing this and um one reason you may you may
want to skip the running stats is because you may want to for example estimate them at the end as a stage two
like this and in that case you don't want the bat normalization layer to be doing all this extra compute that you're not going to
use and uh finally we need to know which device we're going to run this bash normalization on a CPU or a GPU and what
the data type should be uh half Precision single Precision double precision and so on so that's the bat normalization layer
otherwise they link to the paper is the same formula we've implemented and everything is the same exactly as we've
done here okay so that's everything that I wanted to cover for this lecture really
summary of the lecture
what I wanted to talk about is the importance of understanding the activations and the gradients and their statistics in neural networks and this
becomes increasingly important especially as you make your neural networks bigger larger and deeper we looked at the distributions basically
at the output layer and we saw that if you have two confident mispredictions because the activations are too messed
up at the last layer you can end up with these hockey stick losses and if you fix this you get a better loss at the end of
training because your training is not doing wasteful work then we also saw that we need to control the activations
we don't want them to uh you know squash to zero or explode to infinity and
because that you can run into a lot of trouble with all of these uh nonlinearities and these neural Nets and basically you want everything to be
fairly homogeneous throughout the neural net you want roughly goshan activations throughout the neural net let me talked
about okay if we want roughly Gan activations how do we scale these weight matrices and biases during
initialization of the neural nut so that we don't get um you know so everything is as controlled as
possible um so that give us a large boost in Improvement and then I talked
about how that strategy is not actually uh Poss for much much deeper neural nuts
because um when you have much deeper neural nuts with lots of different types of layers it becomes really really hard
to precisely set the weights and the biases in such a way that the activations are roughly uniform
throughout the neural nut so then I introduced the notion of a normalization layer now there are many normalization
layers that that people use in practice bat normalization layer normalization instance normalization group
normalization we haven't covered most of them but I've introduced the first one and also the one that I believe came out
first and that's called Bat normalization and we saw how bat normalization Works uh this is a layer
that you can sprinkle throughout your deep neural net and the basic idea is if you want roughly gsh in activations well
then take your activations and um take the mean and the standard deviation and Center your data and you can do that
because the centering operation is differentiable but and on top of that we
actually had to add a lot of bells and whistles and that gave you a sense of the complexities of the batch normalization layer because now we're
centering the data that's great but suddenly we need the gain and the bias and now those are
trainable and then because we are coupling all of the training examples now suddenly the question is how do you
do the inference where to do to do the inference we need to now estimate these
um mean and standard deviation once uh or the entire training set and then use
those at inference but then no one likes to do stage two so instead we fold everything everything into the bat
normalization later during training and try to estimate these in the running manner so that everything is a bit
simpler and that gives us the bat normalization layer um and as I
mentioned no one likes this layer it causes a huge amount of bugs um and
intuitively it's because it is coupling examples um in the for pass of a neural nut and uh I've shot myself in the foot
with this layer over and over again in my life and I don't want you to suffer the same uh so basically try to avoid it
as much as possible uh some of the other alternatives to these layers are for example group normalization or layer
normalization and those have become more common uh in more recent deep learning
uh but we haven't covered those yet uh but definitely bash normalization was very influential at the time when it
came out in roughly 2015 because it was kind of the first time that you could train reliably uh much deeper neural
nuts and fundamentally the reason for that is because this layer was very effective at controlling the statistics
of the activations in the neural nut so that's the story so far and um that's
all I wanted to cover and in the future lectures hopefully we can start going into recurrent R Nets and um recurring
neural Nets as we'll see are just very very deep networks because you uh you unroll the loop and uh when you actually
optimize these neurals and that's where a lot of this um analysis around the activation
statistics and all these normalization layers will become very very important for uh good performance so we'll see
that next time bye okay so I lied I would like us to do one more summary here as a bonus and I think it's useful
just kidding: part2: PyTorch-ifying the code
as to have one more summary of everything I've presented in this lecture but also I would like us to start by torify our code a little bit so
it looks much more like what you would encounter in PCH so you'll see that I will structure our code into these
modules like a link uh module and a borm module and I'm
putting the code inside these modules so that we can construct neural networks very much like we would construct them in pytorch and I will go through this in
detail so we'll create our neural net then we will do the optimization loop as
we did before and then the one more thing that I want to do here is I want to look at the activation statistics
both in the forward pass and in the backward pass and then here we have the evaluation and sampling just like before
so let me rewind all the way up here and and go a little bit slower so here I creating a linear layer you'll notice
that torch.nn has lots of different types of layers and one of those layers is the linear layer torch. n. linear
takes a number of input features output features whether or not we should have a bias and then the device that we want to
place this layer on and the data type so I will emit these two but otherwise we
have the exact same thing we have the fan in which is the number of inputs fan out the number of outputs and whether or
not we want to use a bias and internally inside this layer there's a weight and a bias if you'd like it it
is typical to initialize the weight using um say random numbers drawn from
aashan and then here's the coming initialization um that we discussed already in this lecture and that's a
good default and also the default that I believe pytor chooses and by default the bias is usually initialized to zeros now
when you call this module uh this will basically calculate W * X plus b if you
have a b and then when you also call that parameters on this module it will return uh the tensors that are the
parameters of this layer now next we have the bash normalization layer so I've written that here and this is very
similar to pytorch nn. bashor 1D layer as shown here so I'm kind of um taking these
three parameters here the dimensionality the Epsilon that we will use in the division and the momentum that we will
use in keeping track of these running stats the running mean and the running variance um now py actually takes quite
a few more things but I'm assuming some of their settings so for us Aline will be true that means that we will be using
a gamma and beta after the normalization the track running stats will be true so we will be keeping track of the running
mean and the running variance in the in the bat Norm our device by default is the CPU and the data type by default is
uh float float 32 so those are the defaults otherwise
uh we are taking all the same parameters in this bachom layer so first I'm just saving them now here's something new
there's a doc training which by default is true and pytorch andn modules also have this attribute. training and that's
because many modules in borm is included in that have a different Behavior
whether you are training your interet and or whether you are running it in an evaluation mode and calculating your
evaluation loss or using it for inference on some test examples and bashor is an example of this because
when we are training we are going to be using the mean and the variance estimated from the current batch but
during inference we are using the running mean and running variance and so also if we are training we are updating
mean and variance but if we are testing then these are not being updated they're kept fixed and so this flag is necessary
and by default true just like in pytorch now the parameters of B 1D are the gamma and the beta
here and then the running mean and running variance are called buffers in pyto
nomenclature and these buffers are trained using exponential moving average
here explicitly and they are not part of the back propagation and stochastic radient descent so they are not sort of
like parameters of this layer and that's why when we C when we have a parameters here we only return gamma and beta we do
not return the mean and the variance this is trained sort of like internally here um every forward pass using
exponential moving average so that's the initialization now in a forward pass if
we are training then we use the mean and the variance estimated by the batch let me pull up the paper
here we calculate the mean and the variance now up above I was estimating
the standard deviation and keeping track of the standard deviation here in the running standard deviation instead of
running variance but let's follow the paper exactly here they calculate the variance which is the standard deviation
squared and that's what's get track of in a running variance instead of a running standard
deviation uh but those two would be very very similar I believe um if we are not training then
we use running mean and variance we normalize and then here I am calculating
the output of this layer and I'm also assigning it to an attribute called out now out is something that I'm using in
our modules here uh this is not what you would find in pytorch we are slightly deviating from it I'm creating a DOT out
because I would like to very easily um maintain all those variables so that we can create statistics of them and plot
them but pytorch and modules will not have a do out attribute and finally here
we are updating the buffers using again as I mentioned exponential moving average uh provide given the provided
momentum and importantly you'll notice that I'm using the torch. nogra context manager and I doing this because if we
don't use this then pytorch will start building out an entire computational graph out of these tensors because it is
expecting that we will eventually call Dot backward but we are never going to be calling dot backward on anything that
includes running mean and running variance so that's why we need to use this context manager so that we are not
um sort of maintaining them using all this additional memory um so this will make it more efficient and it's just
telling pyour that there will no backward we just have a bunch of tensors we want to update them that's it and
then we return okay now scrolling down we have the 10h layer this is very very similar
to uh torch. 10h and it doesn't do too much it just calculates 10 as you might
expect so uh that's torch. 10h and uh there's no parameters in this layer but
because these are layers um it now becomes very easy to sort of like stack them up into uh basically just a list um
and uh we can do all the initializations that we're used to so we have the initial sort of embedding Matrix we have
our layers and we can call them sequentially and then again with Tor no grb but there's some initializations
here so we want to make the output softmax a bit less confident like we saw and in addition to that because we are
using a six layer multi-layer percep on here so you see how I'm stacking linear 10age linear Tage Etc uh I'm going to be
using the gain here and I'm going to play with this in a second so you'll see how uh when we change this what happens
to the statistics finally the parameters are basically the embedding Matrix and all
the parameters in all the layers and notice here I'm using a double list apprehension if you want to call it that
but for every layer in layers and for every parameter in each of those layers we are just stacking up all those piece
uh all those parameters now in total we have 46,000 um
parameters and I'm telling P that all of them require
gradient then here uh we have everything here we are actually mostly used to uh
we are sampling a batch we are doing a forward pass the forward pass now is just the linear application of all the
layers in order followed by the cross entropy and then in the backward pass you'll notice that for every single
layer I now iterate over all the outputs and I'm telling pytorch to retain the gradient of them and then here we are
already used to uh all the all the gradient set To None do the backward to fill in the gradients uh do an update
using stochastic gradient sent and then uh track some statistics and then I am going to break after a single iteration
viz #1: forward pass activations statistics
now here in this cell in this diagram I I'm visualizing the histogram the histograms of the for pass activations
and I'm specifically doing it at the 10 each layers so iterating over all the layers except for the very last one
which is basically just the U soft Max layer um if it is a 10h layer and I'm
using a 10h layer just because they have a finite output netive 1 to 1 and so it's very easy to visualize here so you
see 1 to one and it's a finite range and easy to work with I take the out tensor
from that layer into T and then I'm calculating the mean the standard deviation and the percent saturation of
T and the way I Define the percent saturation is that t. absolute value is greater than 97 so that means we are
here at the tals of the 10 H and remember that when we are in the tales of the 10 H that will actually stop
gradients so we don't want this to be too high now here I'm calling torch.
histogram and then I am plotting this histogram so basically what this is doing is that every different type of
layer and they have a different color we are looking at how many um values in
these tensors take on any of the values Below on this axis here so the first
layer is fairly saturated uh here at 20% so you can see that it's got Tails here
but then everything sort of stabilizes and if we had more layers here it would actually just stabilize at around the standard deviation of about 65 and the
saturation would be roughly 5% and the reason that the stabilizes and gives us a nice distribution here is because gain
is set to 5 over3 now here this gain you see that by
default we initialize with 1 /un of fan in but then here during initialization I
come in and I erator all the layers and if it's a linear layer I boost that by the gain now we saw that one so
basically if we just do not use a gain then what happens if I redraw this you
will see that the standard deviation is shrinking and the saturation is coming
to zero and basically what's happening is the first layer is you know pretty decent but then further layers are just
kind of like shrinking down to zero and it's happening slowly but it's shrinking to zero and the reason for that is when
you just have a sandwich of linear layers alone then a then initializing
our weights in this manner we saw previously would have conserved the standard deviation of one but because we
have this interspersed 10 in layers in there these 10h layers are squashing
functions and so they take your distribution and they slightly squash it and so some gain is necessary to keep
expanding it to fight the squashing so it just turns out that 5
over3 is a good value so if we have something too small like one we saw that things will come toward zero but if it's
something too high let's do two then here we see that um
well let me do something a bit more extreme because so it's a bit more visible let's try three okay so we see here that the
saturations are going to be way too large okay so three would create way too
saturated activations so 5 over3 is a good setting for a sandwich of linear
layers with 10h activations and it roughly stabilizes the standard deviation at a reasonable point now
honestly I have no idea where 5 over3 came from in pytorch um when we were looking at the coming initialization um
I see empirically that it stabilizes this sandwich of linear an 10age and that the saturation is in a good range
um but I don't actually know if this came out of some math formula I tried searching briefly for where this comes
from uh but I wasn't able to find anything uh but certainly we see that empirically these are very nice ranges
our saturation is roughly 5% which is a pretty good number and uh this is a good
setting of The gain in this context similarly we can do the exact same thing with the gradients so here is a very
viz #2: backward pass gradient statistics
same Loop if it's a 10h but instead of taking a layer do out I'm taking the grad and then I'm also showing the mean
and the standard deviation and I'm plotting the histogram of these values and so you'll see that the gradient
distribution is uh fairly reasonable and in particular what we're looking for is that all the different layers in this
sandwich has roughly the same gradient things are not shrinking or exploding so
uh we can for example come here and we can take a look at what happens if this gain was way too small so this was
0.5 then you see the first of all the activations are shrinking to zero but also the gradients are doing something
weird the gradients started out here and then now they're like expanding out and similarly if we for example have
a too high of a gain so like three then we see that also the gradients have there's some asymmetry
going on where as you go into deeper and deeper layers the activation CS are changing and so that's not what we want
and in this case we saw that without the use of batro as we are going through right now we had to very carefully set
those gains to get nice activations in both the forward pass and the backward pass now before we move on to bat
the fully linear case of no non-linearities
normalization I would also like to take a look at what happens when we have no 10h units here so erasing all the 10
nonlinearities but keeping the gain at 5 over3 we now have just a giant linear
sandwich so let's see what happens to the activations as we saw before the correct gain here
is one that is the standard deviation preserving gain so 1.66 7 is too high
and so what's going to happen now is the following uh I have to change this to be linear so we are because there's no more
10h layers and let me change this to linear as well so what we're seeing is um the
activations started out on the blue and have by layer four become very diffuse
so what's happening to the activations is this and with the gradients on the top layer the activation the gradient
statistics are the purple and then they diminish as you go down deeper in the layers and so basically you have an
asymmetry like in the neuron net and you might imagine that if you have very deep neural networks say like 50 layers or
something like that this just uh this is not a good place to be uh so that's why
before bash normalization this was incredibly tricky to to set in particular if this is too large of a
gain this happens and if it's too little of a gain then this happens so the opposite
of that basically happens here we have a um shrinking and a uh diffusion
depending on which direction you look at it from and so certainly this is not what you want and in this case the
correct setting of The gain is exactly one just like we're doing at initialization and then we see that the
uh statistics for the forward and a backward pass are well behaved and so the reason I want to show you this is
that basically like getting neural nness to train before these normalization layers and before the use of advanced
optimizers like adom which we still have to cover and residual connections and so on uh training neurs basically looked
like this it's like a total Balancing Act you have to make sure that everything is precisely orchestrated and
you have to care about the activations and the gradients and their statistics and then maybe you can train something uh but it was it was basically
impossible to train very deep networks and this is fundamentally the the reason for that you'd have to be very very
careful with your initialization um the other point here is you might be asking yourself by the
way I'm not sure if I covered this why do we need these 10h layers at all uh
why do we include them and then have to worry about the gain and uh the reason for that of course is that if you just have a stack of linear layers then
certainly we're getting very easily nice activations and so on uh but this is just massive linear sandwich and it
turns out that it collapses to a single linear layer in terms of its uh representation power so if you were to
plot the output as a function of the input you're just getting a linear function no matter how many linear layers you stack up you still just end
up with a linear transformation all the WX plus BS just collapse into a large WX
plus b with slightly different W's and slightly different B um but interestingly even though the forward
pass collapses to just a linear layer because of back propagation and uh the dynamics of the backward pass the
optimization natur is not identical you actually end up with uh all kinds of interesting um Dynamics in the backward
pass uh because of the uh the way the chain Ru is calculating it and so optimizing a linear layer by itself and
optimizing a sandwich of 10 linear layers in both cases those are just a linear transformation in the forward
pass but the training Dynamics would be different and there's entire papers that analyze in fact like infinitely layered
uh linear layers and and so on and so there's a lot of things to that you can play with there uh but basically the tal
linearities allow us to um turn this sandwich from just a
linear uh function into uh a neural network that can in principle um
approximate any arbitrary function okay so now I've reset the code to use the linear tanh sandwich like before and I
viz #3: parameter activation and gradient statistics
reset everything so the gain is 5 over three uh we can run a single step of optimization and we can look at the
activation statistics of the forward pass and the backward pass but I've added one more plot here that I think is
really important to look at when you're training your neural nuts and to consider and ultimately what we're doing
is we're updating the parameters of the neural nut so we care about the parameters and their values and their
gradients so here what I'm doing is I'm actually iterating over all the parameters available and then I'm only
um restricting it to the two-dimensional parameters which are basically the weights of the linear layers and I'm
skipping the biases and I'm skipping the um gamas and the betas in the bom just
for Simplicity but you can also take a look at those as well but what's happening with the weights is um
instructive by itself so here we have all the different weights their shapes uh so this is the
embedding layer the first linear layer all the way to the very last linear layer and then we have the mean the
standard deviation of all these parameters the histogram and you can see that actually doesn't look that amazing
so there's some trouble in Paradise even though these gradients looked okay there's something weird going on here
I'll get to that in a second and the last thing here is the gradient to data ratio so sometimes I like to visualize
this as well because what this gives you a sense of is what is the scale of the gradient compared to the scale of the
actual values and this is important because we're going to end up taking a step update um that is the learning rate
times the gradient onto the data and so if the gradient has too large of magnitude if the numbers in there are
too large compared to the numbers in data then you'd be in trouble but in this case the gradient to data is our
low numbers so the values inside grad are 1,000 times smaller than the values
inside data in these weights most of them now notably that is not true about
the last layer and so the last layer actually here the output layer is a bit of a troublemaker in the way that this
is currently arranged because you can see that the um last layer here in pink
takes on values that are much larger than some of the values inside um inside
the neural nut so the standard deviations are roughly 1 and3 throughout except for the last last uh layer which
actually has roughly one -2 standard deviation of gradients and so the gradients on the last layer are
currently about 100 times greater sorry 10 times greater than all the other
weights inside the neural net and so that's problematic because in the simple stochastic rting theend setup you would
be training this last layer about 10 times faster than you would be training the other layers at
initialization now this actually like kind of fixes itself a little bit if you train for a bit longer so for example if
I greater than 1,000 only then do a break let me reinitialize and then let
me do it 1,000 steps and after 1,000 steps we can look at the forward pass
okay so you see how the neurons are a bit are saturating a bit and we can also look at the backward pass but otherwise
they look good they're about equal and there's no shrinking to zero or exploding to Infinities and you can see
that here in the weights uh things are also stabilizing a little bit so the Tails of the last pink layer are
actually coming coming in during the optimization but certainly this is like a little bit of troubling especially if
you are using a very simple update rule like stochastic gradient descent instead of a modern Optimizer like Adam now I'd
viz #4: update:data ratio over time
like to show you one more plot that I usually look at when I train neural networks and basically the gradient to
data ratio is not actually that informative because what matters at the end is not the gradient to data ratio
but the update to the data ratio because that is the amount by which we will actually change the data in these
tensors so coming up here what I'd like to do is I'd like to introduce a new update to data uh ratio it's going to be
list and we're going to build it out every single iteration and here I'd like to keep track of basically the
ratio every single iteration so without any gradients I'm
comparing the update which is learning rate times the times the gradient that is the update that we're
going to apply to every parameter uh so see I'm iterating over all the parameters and then I'm taking
the basically standard deviation of the update we're going to apply and divided by the um actual content the data of of
that parameter and its standard deviation so this is the ratio of basically how great are the updates to
the values in these tensors then we're going to take a log of it and actually I'd like to take a log
10 um just so it's a nicer visualization um so we're going to be
basically looking at the exponents of uh the of this division here and then that
item to pop out the float and we're going to be keeping track of this for all the parameters and adding it to
these UD answer so now let me reinitialize and run a th iterations we
can look at the activations the gradients and the parameter gradients as
we did before but now I have one more plot here to introduce and what's Happening Here is
we're are interval parameters and I'm constraining it again like I did here to just the
weights so the number of dimensions in these sensors is two and then I'm basically plotting all of these um
update ratios over time so when I plot this I plot those ratios
and you can see that they evolve over time during initialization they take on certain values and then these updates s
of like start stabilizing usually during training then the other thing that I'm plotting here is I'm plotting here like
an approximate value that is a Rough Guide for what it roughly should be and it should be like roughly
one3 and so that means that basically there's some values in the tensor um and
they take on certain values and the updates to them at every iteration are no more than roughly 1,000th of the
actual like magnitude in those tensors uh if this was much larger like for example if this was um if the log of
this was like say negative 1 this is actually updating those values quite a lot they're undergoing a lot of change
but the reason that the final rate the final uh layer here is an outlier is because this layer was artificially
shrunk down to keep the soft Max um incom unconfident
so here you see how we multiplied The Weight by 0.1 uh in the initialization to make the
last layer prediction less confident that made that artificially made the
values inside that tensor way too low and that's why we're getting temporarily a very high ratio but you see that that
stabilizes over time once uh that weight starts to learn starts to learn but
basically I like to look at the evolution of this update ratio for all my parameters usually and I like to make
sure that it's not too much above onean neg3 roughly uh so around3 on this log
plot if it's below -3 usually that means that the parameters are not trained fast enough so if our learning rate was very
low let's do that experiment uh let's initialize and then let's actually do a learning rate of say
one3 here so 0.001 if your learning rate is way too
low this plot will typically reveal it so
you see how all of these updates are way too small so the size of the update is
uh basically uh 10,000 times um in magnitude to the size of the numbers in
that tensor in the first place so this is a symptom of training way too slow so this is another way to sometimes
set the learning rate and to get a sense of what that learning rate should be and ultimately this is something that you would uh keep track of
if anything the learning rate here is a little bit on the higher side uh because
you see that um we're above the black line of3 we're somewhere around -2.5
it's like okay and uh but everything is like somewhat stabilizing and so this looks like a pretty decent setting of of
um learning rates and so on but this is something to look at and when things are miscalibrated you will you will see very
quickly so for example everything looks pretty well behaved right but just as a comparison
when things are not properly calibrated what does that look like let me come up here and let's say that for example uh
what do we do let's say that we forgot to apply this a fan in normalization so
the weights inside the linear layers are just sampled from aaan and all the stages what happens to our how do we
notice that something's off well the activation plot will tell you whoa your neurons are way too saturated uh the
gradients are going to be all messed up uh the histogram for these weights are going to be all messed up as well and
there's a lot of asymmetry and then if we look here I suspect it's all going to be also pretty messed up so uh you see
there's a lot of uh discrepancy in how fast these layers are learning and some of them are learning way too fast so uh1
1.5 those are very large numbers in terms of this ratio again you should be somewhere around3 and not much more
about that um so this is how miscalibrations of your neuron nuts are going to manifest and these kinds of
plots here are a good way of um sort of bringing um those miscalibrations sort
of uh to your attention and so you can address them okay so so far we've seen
bringing back batchnorm, looking at the visualizations
that when we have this linear tanh sandwich we can actually precisely calibrate the gains and make the
activations the gradients and the parameters and the updates all look pretty decent but it definitely feels a
little bit like balancing of a pencil on your finger and that's because this gain
has to be very precisely calibrated so now let's introduce bat normalization layers into the fix into the mix and
let's let's see how that helps fix the problem so here I'm going to take the bachom 1D
class and I'm going to start placing it inside and as I mentioned before the
standard typical place you would place it is between the linear layer so right after it but before the nonlinearity but
people have definitely played with that and uh in fact you can get very similar results even if you place it after the
nonlinearity um and the other thing that I wanted to mention is it's totally fine to also place it at the end uh after the
last linear layer and before the L function so this is potentially fine as well um and in this case this would be
output would be WAP size um now because the last layer is
Bash we would not be changing the weight to make the softmax less confident we'd be changing the gamma because gamma
remember in the bathroom is the variable that multiplicatively interacts with the output of that
normalization so we can initialize this sandwich now we can train and we can see
that the activations uh are going to of course look uh very good and they are going to necessarily look good because
now before every single 10h layer there is a normalization in the bashor so this
is unsurprisingly all uh looks pretty good it's going to be standard deviation of roughly 65 2% and roughly equal
standard deviation throughout the entire layers so everything looks very homogeneous the gradients look good the
weights look good and their distributions and then the
updates also look um pretty reasonable uh we are going above3 a little bit but
not by too much so all the parameters are training at roughly the same rate um
here but now what we've gained is um we are going to be slightly less
um brittle with respect to the gain of these so for example I can make the gain
be say2 here um which is much much much slower than what we had with the tan
H but as we'll see the activations will actually be exactly unaffected uh and that's because of again this explicit
normalization the gradients are going to look okay the weight gradients are going to look okay okay but actually the
updates will change and so even though the forward and backward pass to a very large extent
look okay because of the backward pass of the Bator and how the scale of the incoming activations interacts in the
Bator and its uh backward pass this is actually changing the um the scale of
the updates on these parameters so the grades on gradients of these weights are affected so we still don't get it
completely free pass to pass in arbitral um weights here but it everything else
is significantly more robust in terms of the forward backward and the weight
gradients it's just that you may have to retune your learning rate if you are changing sufficiently the the scale of
the activations that are coming into the batch Norms so here for example this um we changed the gains of these linear
layers to be greater and we're seeing that the updates are coming out lower as a
result and then finally we can also so if we are using borms we don't actually need to necessarily let me reset this to
one so there's no gain we don't necessarily even have to um normalize by fan in sometimes so if I take out the
fan in so these are just now uh random gsh in we'll see that because of borm
this will actually be relatively well behaved so the statistic look of course in the
forward pass look good the gradients look good the uh backward uh the weight
updates look okay A little bit of fat tails on some of the layers and uh this looks okay as well
but as you as you can see uh we're significantly below ne3 so we'd have to bump up the learning rate of this bachor
uh so that we are training more properly and in particular looking at this roughly looks like we have to 10x the
learning rate to get to about one3 so we' come here and we would
change this to be update of 1.0 and if I reinitialize
then we'll see that everything still of course looks good and now we are roughly here and we expect this to be an okay
training run so long story short we are significantly more robust to the gain of these linear layers whether or not we
have to apply the fan in and then we can change the gain uh but we actually do
have to worry a little bit about the update um scales and making sure that uh the learning rate is properly calibrated
here but this the activations of the forward backward pass and the updates are are looking significantly more well
behaved except for the global scale that is potentially being adjusted here okay
summary of the lecture for real this time
so now let me summarize there are three things I was hoping to achieve with this section number one I wanted to introduce
you to bat normalization which is one of the first modern innovations that we're looking into that helped stabilize very
deep neural networks and their training and I hope you understand how the B normalization works and um how it would
be used in a neural network number two I was hoping to py torify some of our code
and wrap it up into these uh modules so like linear bash 1D 10h Etc these are
layers or modules and they can be stacked up into neural nuts like Lego building blocks and these layers
actually exist in pytorch and if you import torch NN then you can actually
the way I've constructed it you can simply just use pytorch by prepending n and Dot to all these different
layers and actually everything will just work because the API that I've developed
here is identical to the API that pytorch uses and the implementation also is basically as far as I'm Weare
identical to the one in pytorch and number three I tried to introduce you to the diagnostic tools that you would use
to understand whether your neural network is in a good State dynamically so we are looking at the statistics and
histograms and activation of the forward pass activ activations the backward pass gradients and then also we're looking at
the weights that are going to be updated as part of stochastic gradi in ascent and we're looking at their means standard deviations and also the ratio
of gradients to data or even better the updates to data and we saw that
typically we don't actually look at it as a single snapshot Frozen in time at some particular iteration typically
people look at this as a over time just like I've done here and they look at these update to data ratios and they
make sure everything looks okay and in particular I said said that um W3 or basically ne3 on the lock scale is
a good uh rough euristic for what you want this ratio to be and if it's way too high then probably the learning rate
or the updates are a little too too big and if it's way too small that the learning rate is probably too small so
that's just some of the things that you may want to play with when you try to get your neural network to uh work with
very well now there's a number of things I did not try to achieve I did not try to
beat our previous performance as an example by introducing using the bash layer actually I did try um and I found
the new I used the learning rate finding mechanism that I've described before I tried to train a borm layer a borm
neural nut and uh I actually ended up with results that are very very similar to what we've obtained before and that's
because our performance now is not bottlenecked by the optimization which is what borm is helping with the
performance at this stage is bottleneck by what I suspect is the context length of our context so currently we are
taking three characters to predict the fourth one and I think we need to go beyond that and we need to look at more powerful architectures like recurrent
neural networks and Transformers in order to further push um the lock probabilities that we're achieving on
this data set and I also did not try to have a full explanation of all of these
activations the gradients and the backward pass and the statistics of all these gradients and so you may have found some of the parts here un
intuitive and maybe you're slightly confused about okay if I change the uh gain here how come that we need a
different learning rate and I didn't go into the full detail because you'd have to actually look at the backward pass of all these different layers and get an
intuitive understanding of how that works and I did not go into that in this lecture the purpose really was just to
introduce you to the diagnostic tools and what they look like but there's still a lot of work remaining on the intuitive level to understand the
initialization the backward pass and how all of that interacts uh but you shouldn't feel too bad because honestly
we are getting to The Cutting Edge of where the field is we certainly haven't I would say soled
initialization and we haven't soled back propagation and these are still very much an active area of research people
are still trying to figure out what is the best way to initialize these networks what is the best update rule to use um and so on so none of this is
really solved and we don't really have all the answers to all the to you know all these cases but at least uh you know
we're making progress and at least we have some tools to tell us uh whether or not things are on the right track for
now so I think we've made positive progress in this lecture and I hope you enjoyed that
and I will see you next time





intro: why you should care & fun history
hi everyone so today we are once again continuing our implementation of make more now so far we've come up to here
montalia perceptrons and our neural net looked like this and we were implementing this over the last few
lectures now I'm sure everyone is very excited to go into recurring neural networks and all of their variants and how they work
and the diagrams look cool and it's very exciting and interesting and we're going to get a better result but unfortunately
I think we have to remain here for one more lecture and the reason for that is
we've already trained this multilio perceptron right and we are getting pretty good loss and I think we have a pretty decent understanding of the
architecture and how it works but the line of code here that I take an issue with is here lost up backward that is we
are taking a pytorch auto grad and using it to calculate all of our gradients along the way and I would like to remove
the use of lost at backward and I would like us to write our backward pass manually on the level of tensors and I
think that this is a very useful exercise for the following reasons I actually have an entire blog post on
this topic but I'd like to call back propagation a leaky abstraction
and what I mean by that is back propagation does doesn't just make your neural networks just work magically it's
not the case they can just Stack Up arbitrary Lego blocks of differentiable functions and just cross your fingers and back propagate and everything is
great things don't just work automatically it is a leaky abstraction in the sense that you can shoot yourself
in the foot if you do not understanding its internals it will magically not work or not work optimally and you will need
to understand how it works under the hood if you're hoping to debug it and if you are hoping to address it in your neural nut
um so this blog post here from a while ago goes into some of those examples so for example we've already covered them
some of them already for example the flat tails of these functions and how
you do not want to saturate them too much because your gradients will die the case of dead neurons which I've already
covered as well the case of exploding or Vanishing gradients in the case of repair neural
networks which we are about to cover and then also you will often come across
some examples in the wild this is a snippet that I found uh in a random code base on the internet where
they actually have like a very subtle but pretty major bug in their implementation and the bug points at the
fact that the author of this code does not actually understand by propagation so they're trying to do here is they're
trying to clip the loss at a certain maximum value but actually what they're trying to do is they're trying to
collect the gradients to have a maximum value instead of trying to clip the loss at a maximum value and
um indirectly they're basically causing some of the outliers to be actually ignored because when you clip a loss of
an outlier you are setting its gradient to zero and so have a look through this
and read through it but there's basically a bunch of subtle issues that you're going to avoid if you actually know what you're doing and that's why I
don't think it's the case that because pytorch or other Frameworks offer autograd it is okay for us to ignore how
it works now we've actually already covered covered autograd and we wrote micrograd
but micrograd was an autograd engine only on the level of individual scalars so the atoms were single individual
numbers and uh you know I don't think it's enough and I'd like us to basically think about back propagation on level of
tensors as well and so in a summary I think it's a good exercise I think it is
very very valuable you're going to become better at debugging neural networks and making sure that you
understand what you're doing it is going to make everything fully explicit so you're not going to be nervous about what is hidden away from you and
basically in general we're going to emerge stronger and so let's get into it a bit of a fun historical note here is
that today writing your backward pass by hand and manually is not recommended and no one does it except for the purposes
of exercise but about 10 years ago in deep learning this was fairly standard and in fact pervasive so at the time
everyone used to write their own backward pass by hand manually including myself and it's just what you would do
so we used to ride backward pass by hand and now everyone just calls lost that backward uh we've lost something I want
to give you a few examples of this so here's a 2006 paper from Jeff Hinton and
Russell selectinov in science that was influential at the time and this was training some architectures called
restricted bolstery machines and basically it's an auto encoder trained here and this is from roughly 2010 I had
a library for training researchable machines and this was at the time written in Matlab so python was not used
for deep learning pervasively it was all Matlab and Matlab was this a scientific Computing package that everyone would
use so we would write Matlab which is barely a programming language as well but I've had a very convenient tensor
class and was this a Computing environment and you would run here it would all run on a CPU of course but you
would have very nice plots to go with it and a built-in debugger and it was pretty nice now the code in this package
in 2010 that I wrote for fitting research multiple machines to a large
extent is recognizable but I wanted to show you how you would well I'm creating the data in the XY batches I'm
initializing the neural nut so it's got weights and biases just like we're used to and then this is the training Loop
where we actually do the forward pass and then here at this time they didn't even necessarily use back propagation to
train neural networks so this in particular implements contrastive Divergence which estimates a gradient
and then here we take that gradient and use it for a parameter update along the lines that we're used to
um yeah here but you can see that basically people are meddling with these gradients uh
directly and inline and themselves uh it wasn't that common to use an auto grad engine here's one more example from a
paper of mine from 2014 um called the fragmented embeddings and here what I was doing is I was
aligning images and text um and so it's kind of like a clip if you're familiar with it but instead of
working on the level of entire images and entire sentences it was working on the level of individual objects and little pieces of sentences and I was
embedding them and then calculating very much like a clip-like loss and I dig up the code from 2014 of how I implemented
this and it was already in numpy and python and here I'm planting the cost function
and it was standard to implement not just the cost but also the backward pass manually so here I'm calculating the
image embeddings sentence embeddings the loss function I calculate this course this is the loss function and then once
I have the loss function I do the backward pass right here so I backward through the loss function and through
the neural nut and I append regularization so everything was done by hand manually and you were just right
out the backward pass and then you would use a gradient Checker to make sure that your numerical estimate of the gradient
agrees with the one you calculated during back propagation so this was very standard for a long time but today of
course it is standard to use an auto grad engine um but it was definitely useful and I
think people sort of understood how these neural networks work on a very intuitive level and so I think it's a good exercise again and this is where we
want to be okay so just as a reminder from our previous lecture this is The jupyter Notebook that we implemented at
the time and we're going to keep everything the same so we're still going to have a two layer
multiplayer perceptron with a batch normalization layer so the forward pass will be basically identical to this
lecture but here we're going to get rid of lost and backward and instead we're going to write the backward pass manually
starter code
now here's the starter code for this lecture we are becoming a back prop ninja in this notebook
and the first few cells here are identical to what we are used to so we are doing some imports loading the data
set and processing the data set none of this changed now here I'm introducing a utility
function that we're going to use later to compare the gradients so in particular we are going to have the gradients that we estimate manually
ourselves and we're going to have gradients that Pi torch calculates and we're going to be checking for
correctness assuming of course that pytorch is correct um then here we have the initialization
that we are quite used to so we have our embedding table for the characters the first layer second layer and the batch
normalization in between and here's where we create all the parameters now you will note that I
changed the initialization a little bit uh to be small numbers so normally you would set the biases to be all zero here
I am setting them to be small random numbers and I'm doing this because if your variables are initialized to
exactly zero sometimes what can happen is that can mask an incorrect implementation of a gradient
um because uh when everything is zero it sort of like simplifies and gives you a much simpler expression of the gradient than you would otherwise get and so by
making it small numbers I'm trying to unmask those potential errors in these calculations
you also notice that I'm using uh B1 in the first layer I'm using a bias despite
batch normalization right afterwards um so this would typically not be what you do because we talked about the fact
that you don't need the bias but I'm doing this here just for fun um because we're going to have a gradient with respect to it and we can
check that we are still calculating it correctly even though this bias is asparious so here I'm calculating a single batch
and then here I'm doing a forward pass now you'll notice that the forward pass is significantly expanded from what we
are used to here the forward pass was just um here now the reason that the forward pass is
longer is for two reasons number one here we just had an F dot cross entropy but here I am bringing back a explicit
implementation of the loss function and number two I've broken up the implementation into
manageable chunks so we have a lot a lot more intermediate tensors along the way
in the forward pass and that's because we are about to go backwards and calculate the gradients in this back
propagation from the bottom to the top so we're going to go upwards and just
like we have for example the lock props tensor in a forward pass in the backward pass we're going to have a d-lock probes
which is going to store the derivative of the loss with respect to the lock props tensor and so we're going to be prepending D to every one of these
tensors and calculating it along the way of this back propagation so as an example we have a b and raw
here we're going to be calculating a DB in raw so here I'm telling pytorch that
we want to retain the grad of all these intermediate values because here in exercise one we're going to calculate
the backward pass so we're going to calculate all these D values D variables and use the CNP function I've introduced
above to check our correctness with respect to what pi torch is telling us this is going to be exercise one uh
where we sort of back propagate through this entire graph now just to give you a very quick preview of what's going to happen in
exercise two and below here we have fully broken up the loss and back
propagated through it manually in all the little Atomic pieces that make it up but here we're going to collapse the
laws into a single cross-entropy call and instead we're going to analytically derive using math and paper and pencil
the gradient of the loss with respect to the logits and instead of back propagating through all of its little
chunks one at a time we're just going to analytically derive what that gradient is and we're going to implement that
which is much more efficient as we'll see in the in a bit then we're going to do the exact same
thing for patch normalization so instead of breaking up bass drum into all the old tiny components we're going to use
uh pen and paper and Mathematics and calculus to derive the gradient through the bachelor Bachelor layer so we're
going to calculate the backward passthrough bathroom layer in a much more efficient expression instead of backward propagating through all of its
little pieces independently so there's going to be exercise three and then in exercise four we're going to
put it all together and this is the full code of training this two layer MLP and we're going to basically insert our
manual back prop and we're going to take out lost it backward and you will basically see that you can get all the
same results using fully your own code and the only thing we're using from
pytorch is the torch.tensor to make the calculations efficient but otherwise you
will understand fully what it means to forward and backward and neural net and train it and I think that'll be awesome so let's get to it
okay so I read all the cells of this notebook all the way up to here and I'm
going to erase this and I'm going to start implementing backward pass starting with d lock problems so we want
to understand what should go here to calculate the gradient of the loss with respect to all the elements of the log
props tensor now I'm going to give away the answer here but I wanted to put a quick note here that I think would be most
pedagogically useful for you is to actually go into the description of this video and find the link to this Jupyter
notebook you can find it both on GitHub but you can also find Google collab with it so you don't have to install anything you'll just go to a website on Google
collab and you can try to implement these derivatives or gradients yourself and then if you are not able to come to
my video and see me do it and so work in Tandem and try it first yourself and then see me give away the answer and I
think that'll be most valuable to you and that's how I recommend you go through this lecture so we are starting here with d-log props
exercise 1: backproping the atomic compute graph
now d-lock props will hold the derivative of the loss with respect to
all the elements of log props what is inside log blobs the shape of
this is 32 by 27. so it's not going to surprise you that D log props should
also be an array of size 32 by 27 because we want the derivative loss with respect to all of its elements so the
sizes of those are always going to be equal now how how does log props influence the
loss okay loss is negative block probes indexed with range of N and YB and then
the mean of that now just as a reminder YB is just a basically an array of all
the correct indices um so what we're doing here is we're
taking the lock props array of size 32 by 27. right
and then we are going in every single row and in each row we are plugging plucking out the index eight and then 14
and 15 and so on so we're going down the rows that's the iterator range of N and then we are always plucking out the
index of the column specified by this tensor YB so in the zeroth row we are taking the eighth column in the first
row we're taking the 14th column Etc and so log props at this plugs out
all those log probabilities of the correct next character in a sequence
so that's what that does and the shape of this or the size of it is of course 32 because our batch size is 32.
so these elements get plugged out and then their mean and the negative of that
becomes loss so I always like to work with simpler examples to understand the numerical
form of derivative what's going on here is once we've plucked out these examples
um we're taking the mean and then the negative so the loss basically I can write it this way is the negative
of say a plus b plus c and the mean of those three numbers would be say negative would divide three
that would be how we achieve the mean of three numbers ABC although we actually have 32 numbers here
and so what is basically the loss by say like d a right
well if we simplify this expression mathematically this is negative one over three of A and negative plus negative
one over three of B plus negative 1 over 3 of c and so what
is D loss by D A it's just negative one over three and so you can see that if we don't just have a b and c but we have 32 numbers
then D loss by D um you know every one of those numbers is going to be one over N More generally
because n is the um the size of the batch 32 in this case
so D loss by um D Lock probs is negative 1 over n
in all these places now what about the other elements inside lock problems because lock props is
large array you see that lock problems at shape is 32 by 27. but only 32 of
them participate in the loss calculation so what's the derivative of all the other most of the elements that do not
get plucked out here while their loss intuitively is zero sorry they're gradient intuitively is
zero and that's because they did not participate in the loss so most of these numbers inside this
tensor does not feed into the loss and so if we were to change these numbers then the loss doesn't change which is
the equivalent of way of saying that the derivative of the loss with respect to them is zero they don't impact it
so here's a way to implement this derivative then we start out with torch.zeros of shape 32 by 27 or let's
just say instead of doing this because we don't want to hard code numbers let's do torch.zeros like
block probs so basically this is going to create an array of zeros exactly in the shape of log probs
and then we need to set the derivative of negative 1 over n inside exactly these locations so here's what we can do
the lock props indexed in The Identical way will be just set to negative one over
zero divide n right just like we derived here
so now let me erase all this reasoning and then this is the candidate derivative for D log props let's
uncomment the first line and check that this is correct okay so CMP ran and let's go back to CMP
and you see that what it's doing is it's calculating if the calculated value by us which is DT
is exactly equal to T dot grad as calculated by pi torch and then this is making sure that all the elements are
exactly equal and then converting this to a single Boolean value because we don't want the Boolean tensor we just
want to Boolean value and then here we are making sure that okay if they're not exactly equal maybe
they are approximately equal because of some floating Point issues but they're very very close so here we are using torch.allclose
which has a little bit of a wiggle available because sometimes you can get very very close but if you use a
slightly different calculation because a floating Point arithmetic you can get a slightly different result so this is
checking if you get an approximately close result and then here we are checking the maximum uh basically the value that has
the highest difference and what is the difference in the absolute value difference between those two and so we
are printing whether we have an exact equality an approximate equality and what is the largest difference
and so here we see that we actually have exact equality and so therefore of course we
also have an approximate equality and the maximum difference is exactly zero so basically our d-log props is exactly
equal to what pytors calculated to be lockprops.grad in its back propagation
so so far we're working pretty well okay so let's now continue our back propagation we have that lock props depends on
probes through a log so all the elements of probes are being element wise applied log to
now if we want deep props then then remember your micrograph training we have like a log node it takes in
probs and creates log probs and the props will be the local derivative of
that individual Operation Log times the derivative loss with respect to its output which in this case is D log props
so what is the local derivative of this operation well we are taking log element wise and we can come here and we can see
well from alpha is your friend that d by DX of log of x is just simply one of our X
so therefore in this case X is problems so we have d by DX is one over X which
is one of our probes and then this is the local derivative and then times we want to chain it
so this is chain rule times do log props let me uncomment this and let me run the
cell in place and we see that the derivative of props as we calculated here is exactly correct
and so notice here how this works probes that are props is going to be inverted
and then element was multiplied here so if your probes is very very close to one that means you are your network is
currently predicting the character correctly then this will become one over one and D log probes just gets passed
through but if your probabilities are incorrectly assigned so if the correct character here is getting a very low
probability then 1.0 dividing by it will boost this
and then multiply by the log props so basically what this line is doing intuitively is it's taking the examples
that have a very low probability currently assigned and it's boosting their gradient uh you can you can look
at it that way next up is Count some imp so we want the river of this now let me
just pause here and kind of introduce What's Happening Here in general because I know it's a little bit confusing we
have the locusts that come out of the neural nut here what I'm doing is I'm finding the maximum in each row and I'm
subtracting it for the purposes of numerical stability and we talked about how if you do not do this you run
numerical issues if some of the logits take on two large values because we end up exponentiating them
so this is done just for safety numerically then here's the exponentiation of all the sort of like
logits to create our accounts and then we want to take the some of these counts
and normalize so that all of the probes sum to one now here instead of using one over count
sum I use uh raised to the power of negative one mathematically they are identical I just found that there's
something wrong with the pytorch implementation of the backward pass of division um and it gives like a real result but
that doesn't happen for star star native one that's why I'm using this formula instead but basically all that's
happening here is we got the logits we're going to exponentiate all of them and want to normalize the counts to
create our probabilities it's just that it's happening across multiple lines so now
here we want to First Take the derivative we
want to back propagate into account sumiv and then into counts as well so what should be the count sum M now we
actually have to be careful here because we have to scrutinize and be careful with the shapes so counts that shape and
then count some inverse shape are different so in particular counts as 32 by 27 but
this count sum m is 32 by 1. and so in this multiplication here we also have an
implicit broadcasting that pytorch will do because it needs to take this column tensor of 32 numbers and replicate it
horizontally 27 times to align these two tensors so it can do an element twice multiply
so really what this looks like is the following using a toy example again what we really have here is just props
is counts times conservative so it's a C equals a times B but a is 3 by 3 and b is just three by
one a column tensor and so pytorch internally replicated this elements of B and it did that across all the columns
so for example B1 which is the first element of B would be replicated here across all the columns in this
multiplication and now we're trying to back propagate through this operation to count some m
so when we're calculating this derivative it's important to realize that these two
this looks like a single operation but actually is two operations applied sequentially the first operation that
pytorch did is it took this column tensor and replicated it across all the
um across all the columns basically 27 times so that's the first operation it's a replication and then the second
operation is the multiplication so let's first background through the multiplication
if these two arrays are of the same size and we just have a and b of both of them
three by three then how do we mult how do we back propagate through a multiplication so if we just have
scalars and not tensors then if you have C equals a times B then what is uh the
order of the of C with respect to B well it's just a and so that's the local derivative
so here in our case undoing the multiplication and back propagating through just the multiplication itself
which is element wise is going to be the local derivative which in this case is
simply counts because counts is the a so this is the local derivative and then
times because the chain rule D props so this here is the derivative or the
gradient but with respect to replicated B but we don't have a replicated B we just
have a single B column so how do we now back propagate through the replication
and intuitively this B1 is the same variable and it's just reused multiple times
and so you can look at it as being equivalent to a case we've encountered in micrograd
and so here I'm just pulling out a random graph we used in micrograd we had an example where a single node
has its output feeding into two branches of basically the graph until the last function and we're talking about how the
correct thing to do in the backward pass is we need to sum all the gradients that arrive at any one node so across these
different branches the gradients would sum so if a node is used multiple times the
gradients for all of its uses sum during back propagation so here B1 is used multiple times in all
these columns and therefore the right thing to do here is to sum horizontally across all the rows so I'm
going to sum in Dimension one but we want to retain this Dimension so that the uh so that counts
some end and its gradient are going to be exactly the same shape so we want to make sure that we keep them as true so
we don't lose this dimension and this will make the count sum M be exactly shape 32 by 1.
so revealing this comparison as well and running this we see that we get an exact
match so this derivative is exactly correct and let me erase
this now let's also back propagate into counts which is the other variable here
to create probes so from props to count some INF we just did that let's go into counts as well
so decounts will be the chances are a so DC by d a is just B
so therefore it's count summative um and then times chain rule the props
now councilman is three two by One D probs is 32 by 27.
so um those will broadcast fine and will give us decounts there's no additional
summation required here um there will be a broadcasting that happens in this multiply here because
count some M needs to be replicated again to correctly multiply D props but
that's going to give the correct result so as far as the single operation is concerned so we back probably go from
props to counts but we can't actually check the derivative counts uh I have it
much later on and the reason for that is because count sum in depends on counts and so there's a second Branch here that
we have to finish because can't summon back propagates into account sum and count sum will buy properly into counts
and so counts is a node that is being used twice it's used right here in two props and it goes through this other
Branch through count summative so even though we've calculated the first contribution of it we still have
to calculate the second contribution of it later okay so we're continuing with this Branch we have the derivative for count
sum if now we want the derivative of count sum so D count sum equals what is
the local derivative of this operation so this is basically an element wise one over counts sum
so count sum raised to the power of negative one is the same as one over count sum if we go to all from alpha we
see that x to the negative one D by D by D by DX of it is basically Negative X to
the negative 2. right one negative one over squared is the same as Negative X to the negative two
so D count sum here will be local derivative is going to be negative
um counts sum to the negative two that's the local derivative times chain rule
which is D count sum in so that's D count sum
let's uncomment this and check that I am correct okay so we have perfect equality
and there's no sketchiness going on here with any shapes because these are of the same shape okay next up we want to back
propagate through this line we have that count sum it's count.sum along the rows so I wrote out
um some help here we have to keep in mind that counts of course is 32 by 27 and count sum is 32 by 1. so in this
back propagation we need to take this column of derivatives and transform it
into a array of derivatives two-dimensional array so what is this operation doing we're
taking in some kind of an input like say a three by three Matrix a and we are summing up the rows into a column tells
her B1 b2b3 that is basically this so now we have the derivatives of the
loss with respect to B all the elements of B and now we want to derivative loss with
respect to all these little A's so how do the B's depend on the ace is
basically what we're after what is the local derivative of this operation well we can see here that B1 only
depends on these elements here the derivative of B1 with respect to all of
these elements down here is zero but for these elements here like a11 a12 Etc the
local derivative is one right so DB 1 by D A 1 1 for example is one so it's one
one and one so when we have the derivative of loss with respect to B1
did a local derivative of B1 with respect to these inputs is zeros here but it's one on these guys
so in the chain rule we have the local derivative uh times sort of the derivative of B1 and so
because the local derivative is one on these three elements the look of them are multiplying the derivative of B1
will just be the derivative of B1 and so you can look at it as a router basically
an addition is a router of gradient whatever gradient comes from above it just gets routed equally to all the
elements that participate in that addition so in this case the derivative of B1 will just flow equally to the derivative
of a11 a12 and a13 . so if we have a derivative of all the elements of B and in this column tensor
which is D counts sum that we've calculated just now we basically see that what that amounts
to is all of these are now flowing to all these elements of a and they're doing that horizontally
so basically what we want is we want to take the decount sum of size 30 by 1 and
we just want to replicate it 27 times horizontally to create 32 by 27 array
so there's many ways to implement this operation you could of course just replicate the tensor but I think maybe
one clean one is that the counts is simply torch dot once like
so just an two-dimensional arrays of ones in the shape of counts so 32 by 27
times D counts sum so this way we're letting the broadcasting here basically
implement the replication you can look at it that way but then we have to also be careful
because decounts was already calculated we calculated earlier here and that was
just the first branch and we're now finishing the second Branch so we need to make sure that these gradients add so
plus equals and then here um let's comment out the comparison and
let's make sure crossing fingers that we have the correct result so pytorch agrees with us on this gradient as well
okay hopefully we're getting a hang of this now counts as an element-wise X of Norm legits so now we want D Norm logits
and because it's an element price operation everything is very simple what is the local derivative of e to the X it's famously just e to the x so this is
the local derivative that is the local derivative now we
already calculated it and it's inside counts so we may as well potentially just reuse counts that is the local
derivative times uh D counts
funny as that looks constant decount is derivative on the normal objects and now
let's erase this and let's verify and it looks good
so that's uh normal agents okay so we are here on this line now the
normal objects we have that and we're trying to calculate the logits and deloget Maxes
so back propagating through this line now we have to be careful here because the shapes again are not the same and so
there's an implicit broadcasting Happening Here so normal jits has this shape 32 by 27
logist does as well but logit Maxis is only 32 by one so there's a broadcasting
here in the minus now here I try to sort of write out a
two example again we basically have that this is our C equals a minus B and we see that because of the shape
these are three by three but this one is just a column and so for example every element of C we
have to look at how it uh came to be and every element of C is just the corresponding element of a minus uh
basically that associated b so it's very clear now that the
derivatives of every one of these c's with respect to their inputs are one for
the corresponding a and it's a negative one for the corresponding B
and so therefore um the derivatives on the C will flow
equally to the corresponding Ace and then also to the corresponding base but
then in addition to that the B's are broadcast so we'll have to do the additional sum just like we did before
and of course the derivatives for B's will undergo a minus because the local derivative here is uh negative one
so DC three two by D B3 is negative one so let's just Implement that basically
delugits will be uh exactly copying the derivative on normal objects
so delugits equals the norm logits and I'll do a DOT clone for safety so we're just
making a copy and then we have that the loaded Maxis will be the negative of the non-legits
because of the negative sign and then we have to be careful because logic Maxis is a column
and so just like we saw before because we keep replicating the same elements
across all the columns then in the backward pass because we keep reusing this these are all just
like separate branches of use of that one variable and so therefore we have to do a Sum along one would keep them
equals true so that we don't destroy this dimension and then the logic Maxes will be the
same shape now we have to be careful because this deloaches is not the final deloaches and that's because not only do
we get gradient signal into logits through here but the logic Maxes as a function of logits and that's a second
Branch into logits so this is not yet our final derivative for logits we will come back later for the second branch
for now the logic Maxis is the final derivative so let me uncomment this CMP here and let's just run this
and logit Maxes hit by torch agrees with us so that was the derivative into through
this line now before we move on I want to pause here briefly and I want to look at these
logic Maxes and especially their gradients we've talked previously in the previous lecture that the only reason we're doing
this is for the numerical stability of the softmax that we are implementing here and we talked about how if you take
these logents for any one of these examples so one row of this logit's tensor if you add or subtract any value
equally to all the elements then the value of the probes will be unchanged
you're not changing soft Max the only thing that this is doing is it's making sure that X doesn't overflow and the
reason we're using a Max is because then we are guaranteed that each row of logits the highest number is zero and so
this will be safe and so um basically what that has repercussions
if it is the case that changing logit Maxis does not change the props and therefore there's not change the loss
then the gradient on logic masses should be zero right because saying those two things is the same
so indeed we hope that this is very very small numbers so indeed we hope this is zero now because of floating Point uh
sort of wonkiness um this doesn't come out exactly zero only in some of the rows it does but we
get extremely small values like one e negative nine or ten and so this is telling us that the values of loaded
Maxes are not impacting the loss as they shouldn't it feels kind of weird to back propagate
through this branch honestly because if you have any implementation of like f
dot cross entropy and pytorch and you you block together all these elements and you're not doing the back propagation piece by piece then you
would probably assume that the derivative through here is exactly zero uh so you would be sort of
um skipping this branch because it's only done for numerical stability but
it's interesting to see that even if you break up everything into the full atoms and you still do the computation as
you'd like with respect to numerical stability uh the correct thing happens and you still get a very very small
gradients here um basically reflecting the fact that the values of these do not matter with
respect to the final loss okay so let's now continue back propagation through this line here we've
just calculated the logit Maxis and now we want to back prop into logits through this second branch
now here of course we took legits and we took the max along all the rows and then we looked at its values here now the way
this works is that in pytorch this thing here
the max returns both the values and it Returns the indices at which those values to count the maximum value
now in the forward pass we only used values because that's all we needed but in the backward pass it's extremely
useful to know about where those maximum values occurred and we have the indices
at which they occurred and this will of course helps us to help us do the back propagation because what should the
backward pass be here in this case we have the largest tensor which is 32 by 27 and in each row we find the maximum
value and then that value gets plucked out into loaded Maxis and so intuitively
um basically the derivative flowing through here then should be one
times the look of derivatives is 1 for the appropriate entry that was plucked out
and then times the global derivative of the logic axis so really what we're doing here if you
think through it is we need to take the deloachet Maxis and we need to scatter it to the correct positions in these
logits from where the maximum values came and so um
I came up with one line of code sort of that does that let me just erase a bunch of stuff here so the line of uh you
could do it kind of very similar to what we've done here where we create a zeros and then we populate uh the correct
elements uh so we use the indices here and we would set them to be one but you
can also use one hot so F dot one hot and then I'm taking the
lowest of Max over the First Dimension dot indices and I'm telling uh pytorch
that the dimension of every one of these tensors should be um
27 and so what this is going to do is okay I apologize this is crazy filthy
that I am sure of this it's really just a an array of where the Maxes came from in each row and that
element is one and the all the other elements are zero so it's a one-half Vector in each row and these indices are
now populating a single one in the proper place and then what I'm doing here is I'm
multiplying by the logit Maxis and keep in mind that this is a column
of 32 by 1. and so when I'm doing this times the logic Maxis the logic Maxes
will broadcast and that column will you know get replicated and in an element wise multiply will ensure that each of
these just gets routed to whichever one of these bits is turned on and so that's another way to implement
uh this kind of a this kind of a operation and both of these can be used
I just thought I would show an equivalent way to do it and I'm using plus equals because we already calculated the logits here and this is
not the second branch so let's look at logits and make sure that this
is correct and we see that we have exactly the correct answer
next up we want to continue with logits here that is an outcome of a matrix multiplication and a bias offset in this
linear layer so I've printed out the shapes of all these intermediate tensors we see that
logits is of course 32 by 27 as we've just seen then the H here is 32 by 64. so these
are 64 dimensional hidden States and then this W Matrix projects those 64 dimensional vectors into 27 dimensions
and then there's a 27 dimensional offset which is a one-dimensional vector
now we should note that this plus here actually broadcasts because H multiplied by by W2 will give us a 32 by 27. and so
then this plus B2 is a 27 dimensional lecture here now in the rules of broadcasting what's
going to happen with this bias Vector is that this one-dimensional Vector of 27 will get aligned with a padded dimension
of one on the left and it will basically become a row vector and then it will get replicated vertically 32 times to make
it 32 by 27 and then there's an element-wise multiply now
the question is how do we back propagate from logits to the hidden States the weight Matrix W2 and the bias B2
and you might think that we need to go to some Matrix calculus and then we have
to look up the derivative for a matrix multiplication but actually you don't have to do any of that and you can go
back to First principles and derive this yourself on a piece of paper and specifically what I like to do and I
what I find works well for me is you find a specific small example that you then fully write out and then in the
process of analyzing how that individual small example works you will understand the broader pattern and you'll be able
to generalize and write out the full general formula for what how these derivatives flow in an expression like
this so let's try that out so pardon the low budget production here but what I've done here is I'm writing
it out on a piece of paper really what we are interested in is we have a multiply B plus C and that creates a d
and we have the derivative of the loss with respect to D and we'd like to know what the derivative of the losses with
respect to a b and c now these here are little two-dimensional examples of a matrix
multiplication Two by Two Times a two by two plus a 2 a vector of just two elements
C1 and C2 gives me a two by two now notice here that I have a bias
Vector here called C and the bisex vector is C1 and C2 but as I described
over here that bias Vector will become a row Vector in the broadcasting and will replicate vertically so that's what's
happening here as well C1 C2 is replicated vertically and we see how we have two rows of C1 C2 as a result
so now when I say write it out I just mean like this basically break up this matrix multiplication into the actual
thing that that's going on under the hood so as a result of matrix multiplication and how it works d11 is
the result of a DOT product between the first row of a and the First Column of B so a11 b11 plus a12 B21 plus C1
and so on so forth for all the other elements of D and once you actually write it out it becomes obvious this is
just a bunch of multipliers and um adds and we know from micrograd how
to differentiate multiplies and adds and so this is not scary anymore it's not just matrix multiplication it's just uh
tedious unfortunately but this is completely tractable we have DL by D for all of these and we want DL by uh all
these little other variables so how do we achieve that and how do we actually get the gradients okay so the low budget
production continues here so let's for example derive the derivative of the loss with respect to
a11 we see here that a11 occurs twice in our simple expression right here right here
and influences d11 and D12 . so this is so what is DL by d a one
one well it's DL by d11 times the local
derivative of d11 which in this case is just b11 because that's what's multiplying a11 here
so uh and likewise here the local derivative of D12 with respect to a11 is just B12 and so B12 well in the chain
rule therefore multiply the L by d 1 2. and then because a11 is used both to
produce d11 and D12 we need to add up the contributions of both of those sort
of chains that are running in parallel and that's why we get a plus just adding up those two
um those two contributions and that gives us DL by d a one one we can do the exact same analysis for the other one
for all the other elements of a and when you simply write it out it's just super simple
um taking of gradients on you know expressions like this you find that
this Matrix DL by D A that we're after right if we just arrange all the all of
them in the same shape as a takes so a is just too much Matrix so d l by D A
here will be also just the same shape tester with the derivatives now so deal
by D a11 Etc and we see that actually we can express what we've written out here as a matrix
multiplied and so it just so happens that D all by that all of these formulas that we've
derived here by taking gradients can actually be expressed as a matrix multiplication and in particular we see
that it is the matrix multiplication of these two array matrices so it is the um DL by D and then Matrix
multiplying B but B transpose actually so you see that B21 and b12 have changed
place whereas before we had of course b11 B12 B2 on B22 so you see that this other
Matrix B is transposed and so basically what we have long story short just by doing very simple
reasoning here by breaking up the expression in the case of a very simple example is that DL by d a is which is
this is simply equal to DL by DD Matrix multiplied with B transpose
so that is what we have so far now we also want the derivative with respect to um B and C now
for B I'm not actually doing the full derivation because honestly it's um it's not deep it's just uh annoying it's
exhausting you can actually do this analysis yourself you'll also find that if you take this these expressions and
you differentiate with respect to b instead of a you will find that DL by DB is also a matrix multiplication in this
case you have to take the Matrix a and transpose it and Matrix multiply that with bl by DD
and that's what gives you a deal by DB and then here for the offsets C1 and C2
if you again just differentiate with respect to C1 you will find an expression like this
and C2 an expression like this and basically you'll find the DL by DC
is simply because they're just offsetting these Expressions you just have to take the deal by DD Matrix
of the derivatives of D and you just have to sum across the columns and that
gives you the derivatives for C so long story short the backward Paths of a matrix multiply
is a matrix multiply and instead of just like we had D equals a times B plus C in the scalar case uh
we sort of like arrive at something very very similar but now uh with a matrix multiplication instead of a scalar
multiplication so the derivative of D with respect to a is
DL by DD Matrix multiplied B trespose and here it's a transpose multiply deal
by DD but in both cases it's a matrix multiplication with the derivative and
the other term in the multiplication and for C it is a sum
now I'll tell you a secret I can never remember the formulas that we just arrived for back proper gain information
multiplication and I can back propagate through these Expressions just fine and the reason this works is because the
dimensions have to work out uh so let me give you an example say I want to create DH
then what should the H be number one I have to know that the shape of DH must
be the same as the shape of H and the shape of H is 32 by 64. and then
the other piece of information I know is that DH must be some kind of matrix multiplication of the logits with W2
and delojits is 32 by 27 and W2 is a 64
by 27. there is only a single way to make the shape work out in this case and
it is indeed the correct result in particular here H needs to be 32 by 64. the only way to achieve that is to take
a deluges and Matrix multiply it with you see how
I have to take W2 but I have to transpose it to make the dimensions work out so w to transpose and it's the only way
to make these to Matrix multiply those two pieces to make the shapes work out and that turns out to be the correct
formula so if we come here we want DH which is d a and we see that d a is DL
by DD Matrix multiply B transpose so that's Delo just multiply and B is W2
so W2 transpose which is exactly what we have here so there's no need to remember these formulas similarly now if I want
dw2 well I know that it must be a matrix multiplication of D logits and H
and maybe there's a few transpose like there's one transpose in there as well and I don't know which way it is so I have to come to W2 and I see that its
shape is 64 by 27 and that has to come from some interest multiplication of these two
and so to get a 64 by 27 I need to take um
H I need to transpose it and then I need to Matrix multiply it um so that will become 64 by 32 and then
I need to make sure to multiply with the 32 by 27 and that's going to give me a 64 by 27. so I need to make sure it's
multiplied this with the logist that shape just like that that's the only way to make the dimensions work out and just
use matrix multiplication and if we come here we see that that's exactly what's here so a transpose a for us is H
multiplied with deloaches so that's W2 and then db2
is just the um vertical sum and actually in the same
way there's only one way to make the shapes work out I don't have to remember that it's a vertical Sum along the zero
axis because that's the only way that this makes sense because B2 shape is 27 so in order to get a um delugits
here is 30 by 27 so knowing that it's just sum over deloaches in some
Direction that direction must be zero because I
need to eliminate this Dimension so it's this so this is so let's kind of like the
hacky way let me copy paste and delete that and let me swing over here and this
is our backward pass for the linear layer uh hopefully so now let's uncomment
these three and we're checking that we got all the three derivatives correct and run
and we see that h wh and B2 are all exactly correct so we back propagated
through a linear layer now next up we have derivative for the h
already and we need to back propagate through 10h into h preact so we want to derive DH preact
and here we have to back propagate through a 10 H and we've already done this in micrograd and we remember that
10h has a very simple backward formula now unfortunately if I just put in D by DX of 10 h of X into both from alpha it
lets us down it tells us that it's a hyperbolic secant function squared of X it's not exactly helpful but luckily
Google image search does not let us down and it gives us the simpler formula and in particular if you have that a is
equal to 10 h of Z then d a by DZ by propagating through 10 H is just one
minus a square and take note that 1 minus a square a here is the output of
the 10h not the input to the 10h Z so the D A by DZ is here formulated in
terms of the output of that 10h and here also in Google image search we have the full derivation if you want to
actually take the actual definition of 10h and work through the math to figure out 1 minus standard square of Z
so 1 minus a square is the local derivative in our case that is 1 minus
uh the output of 10 H squared which here is H so it's h squared and that is the local
derivative and then times the chain rule DH so that is going to be our candidate
implementation so if we come here and then uncomment this let's hope for
the best and we have the right answer okay next up we have DH preact and we
want to back propagate into the gain the B and raw and the B and bias so here this is the bathroom parameters
being gained in bias inside the bash term that take the B and raw that is exact unit caution and then scale it and
shift it and these are the parameters of The Bachelor now here we have a
multiplication but it's worth noting that this multiply is very very different from this Matrix multiply here Matrix multiply are DOT products between
rows and Columns of these matrices involved this is an element twice multiply so things are quite a bit
simpler now we do have to be careful with some of the broadcasting happening in this line of code though so you see how BN
gain and B and bias are 1 by 64. but H preact and B and raw are 32 by 64.
so we have to be careful with that and make sure that all the shapes work out fine and that the broadcasting is correctly back propagated
so in particular let's start with the B and Gain so DB and gain should be
and here this is again elementorized multiply and whenever we have a times b equals c we saw that the local
derivative here is just if this is a the local derivative is just the B the other one so the local derivative is just B
and raw and then times chain rule so DH preact
so this is the candidate gradient now again we have to be careful because B
and Gain Is of size 1 by 64. but this here would be 32 by 64.
and so um the correct thing to do in this case of course is that b and gain here is a
rule Vector of 64 numbers it gets replicated vertically in this operation and so therefore the correct thing to do
is to sum because it's being replicated and therefore all the gradients in each
of the rows that are now flowing backwards need to sum up to that same tensor DB and Gain so we have to sum
across all the zero all the examples basically which is the direction in which this
gets replicated and now we have to be also careful because we um being gain is of shape 1 by 64. so in
fact I need to keep them as true otherwise I would just get 64.
now I don't actually really remember why the being gain and the BN bias I made them be 1 by 64.
um but the biases B1 and B2 I just made them be one-dimensional vectors they're
not two-dimensional tensors so I can't recall exactly why I left the gain and
the bias as two-dimensional but it doesn't really matter as long as you are consistent and you're keeping it the same
so in this case we want to keep the dimension so that the tensor shapes work next up we have B and raw so DB and raw
will be BN gain multiplying
dhreact that's our chain rule now what about the
um dimensions of this we have to be careful right so DH preact is 32 by 64. B and
gain is 1 by 64. so it will just get replicated and to create this
multiplication which is the correct thing because in a forward pass it also gets replicated in just the same way
so in fact we don't need the brackets here we're done and the shapes are already correct
and finally for the bias very similar this bias here is very very
similar to the bias we saw when you layer in the linear layer and we see that the gradients from each preact will
simply flow into the biases and add up because these are just these are just offsets
and so basically we want this to be DH preact but it needs to Sum along the right Dimension and in this case similar
to the gain we need to sum across the zeroth dimension the examples because of the way that the bias gets replicated
vertically and we also want to have keep them as true and so this will basically take this and
sum it up and give us a 1 by 64. so this is the candidate implementation
it makes all the shapes work let me bring it up down here and then
let me uncomment these three lines to check that we are getting the correct result for all the three tensors and
indeed we see that all of that got back propagated correctly so now we get to the batch Norm layer we see how here
being gay and being bias are the parameters so the back propagation ends but B and raw now is the output of the
standardization so here what I'm doing of course is I'm breaking up the batch form into manageable pieces so we can back
propagate through each line individually but basically what's happening is BN mean I is the sum
so this is the B and mean I I apologize for the variable naming B and diff is x
minus mu B and div 2 is x minus mu squared here inside the variance
B and VAR is the variance so uh Sigma Square this is B and bar and it's
basically the sum of squares so this is the x minus mu squared and
then the sum now you'll notice one departure here here it is normalized as 1 over m
uh which is number of examples here I'm normalizing as one over n minus 1 instead of N and this is deliberate and
I'll come back to that in a bit when we are at this line it is something called the bezels correction
but this is how I want it in our case bienvar inv then becomes basically
bienvar plus Epsilon Epsilon is one negative five and then it's one over
square root is the same as raising to the power of negative 0.5 right because 0.5 is square
root and then negative makes it one over square root so BM Bar M is a one over this uh
denominator here and then we can see that b and raw which is the X hat here is equal to the BN diff the numerator
multiplied by the um BN bar in
and this line here that creates pre-h pre-act was the last piece we've already back propagated through it
so now what we want to do is we are here and we have B and raw and we have to first back propagate into B and diff and
B and Bar M so now we're here and we have DB and raw and we need to back propagate through
this line now I've written out the shapes here and indeed bien VAR m is a shape 1 by 64. so
there is a broadcasting happening here that we have to be careful with but it is just an element-wise simple
multiplication by now we should be pretty comfortable with that to get DB and diff we know that this is just B and
varm multiplied with DP and raw
and conversely to get dbmring we need to take the end if
and multiply that by DB and raw so this is the candidate but of course
we need to make sure that broadcasting is obeyed so in particular B and VAR M
multiplying with DB and raw will be okay and give us 32 by 64 as we
expect but dbm VAR inv would be taking a 32 by
64. multiplying it by 32 by 64. so this is a 32 by 64. but of course DB this uh B and
VAR in is only 1 by 64. so the second line here needs a sum across the
examples and because there's this Dimension here we need to make sure that keep them is true
so this is the candidate let's erase this and let's swing down
here and implement it and then let's comment out dbm barif and DB and diff
now we'll actually notice that DB and diff by the way is going to be incorrect
so when I run this BMR m is correct B and diff is not
correct and this is actually expected because we're not done with b and diff
so in particular when we slide here we see here that b and raw as a function of B and diff but actually B and far of is
a function of B of R which is a function of B and df2 which is a function of B and diff so it comes here so bdn diff
um these variable names are crazy I'm sorry it branches out into two branches and we've only done one branch of it we
have to continue our back propagation and eventually come back to B and diff and then we'll be able to do a plus equals and get the actual card gradient
for now it is good to verify that CMP also works it doesn't just lie to us and tell us that everything is always
correct it can in fact detect when your gradient is not correct so it's that's
good to see as well okay so now we have the derivative here and we're trying to back propagate through this line
and because we're raising to a power of negative 0.5 I brought up the power rule and we see that basically we have that
the BM bar will now be we bring down the exponent so negative 0.5 times
uh X which is this and now raised to the power of negative
0.5 minus 1 which is negative 1.5 now we would have to also apply a small
chain rule here in our head because we need to take further the derivative of B
and VAR with respect to this expression here inside the bracket but because this is an elementalized operation and
everything is fairly simple that's just one and so there's nothing to do there so this is the local derivative and then
times the global derivative to create the chain rule this is just times the BM bar have
so this is our candidate let me bring this down and uncommon to the check
and we see that we have the correct result now before we propagate through the next line I want to briefly talk about the
brief digression: bessel’s correction in batchnorm
note here where I'm using the bezels correction dividing by n minus 1 instead of dividing by n when I normalize here
the sum of squares now you'll notice that this is departure from the paper which uses one over n
instead not one over n minus one their m is RN and
um so it turns out that there are two ways of estimating variance of an array one is the biased estimate which is one
over n and the other one is the unbiased estimate which is one over n minus one now confusingly in the paper this is uh
not very clearly described and also it's a detail that kind of matters I think um they are using the biased version
training time but later when they are talking about the inference they are mentioning that when they do the
inference they are using the unbiased estimate which is the n minus one version in
um basically for inference and to calibrate the running mean and
the running variance basically and so they they actually introduce a trained test mismatch where in training they use
the biased version and in the in test time they use the unbiased version I find this extremely confusing you can
read more about the bezels correction and why uh dividing by n minus one gives you a better estimate of the variance in
a case where you have population size or samples for the population that are very small and that is indeed
the case for us because we are dealing with many patches and these mini matches are a small sample of a larger
population which is the entire training set and so it just turns out that if you just estimate it using one over n that
actually almost always underestimates the variance and it is a biased estimator and it is advised that you use
the unbiased version and divide by n minus one and you can go through this article here that I liked that actually
describes the full reasoning and I'll link it in the video description now when you calculate the torture
variance you'll notice that they take the unbiased flag whether or not you want to divide by n or n minus one confusingly
they do not mention what the default is for unbiased but I believe unbiased by
default is true I'm not sure why the docs here don't cite that now in The Bachelor
1D the documentation again is kind of wrong and confusing it says that the standard deviation is calculated via the
biased estimator but this is actually not exactly right and people have pointed out that it is not right in a number of issues since
then because actually the rabbit hole is deeper and they follow the paper exactly
and they use the biased version for training but when they're estimating the running standard deviation we are using
the unbiased version so again there's the train test mismatch so long story short I'm not a fan of trained test
discrepancies I basically kind of consider the fact that we use the bias version
the training time and the unbiased test time I basically consider this to be a bug and I don't think that there's a
good reason for that it's not really they don't really go into the detail of the reasoning behind it in this paper so
that's why I basically prefer to use the bestless correction in my own work unfortunately Bastion does not take a
keyword argument that tells you whether or not you want to use the unbiased version of the bias version in both
train and test and so therefore anyone using batch normalization basically in my view has a bit of a bug in the code
um and this turns out to be much less of a problem if your batch mini batch sizes
are a bit larger but still I just might kind of uh unpardable so maybe someone can explain why this is okay but for now
I prefer to use the unbiased version consistently both during training and at this time and that's why I'm using one
over n minus one here okay so let's now actually back propagate through this line
so the first thing that I always like to do is I like to scrutinize the shapes first so in particular here looking at the
shapes of what's involved I see that b and VAR shape is 1 by 64. so it's a row
vector and BND if two dot shape is 32 by 64. so clearly here we're doing a sum over
the zeroth axis to squash the first dimension of of the shapes here using a
sum so that right away actually hints to me that there will be some kind of a replication or broadcasting in the
backward pass and maybe you're noticing the pattern here but basically anytime you have a sum in the forward pass that
turns into a replication or broadcasting in the backward pass along the same Dimension and conversely when we have a
replication or a broadcasting in the forward pass that indicates a variable reuse and so in the backward pass that
turns into a sum over the exact same dimension and so hopefully you're noticing that Duality that those two are kind of like
the opposite of each other in the forward and backward pass now once we understand the shapes the
next thing I like to do always is I like to look at a toy example in my head to sort of just like understand roughly how
uh the variable the variable dependencies go in the mathematical formula so here we have a two-dimensional array
of the end of two which we are scaling by a constant and then we are summing uh
vertically over the columns so if we have a two by two Matrix a and then we sum over the columns and scale we would
get a row Vector B1 B2 and B1 depends on a in this way whereas just sum they're
scaled of a and B2 in this way where it's the second column sump and scale
and so looking at this basically what we want to do now is we have the derivatives on B1 and B2 and we want to
back propagate them into Ace and so it's clear that just differentiating in your head the local derivative here is one
over n minus 1 times uh one uh for each one of these A's and um
basically the derivative of B1 has to flow through The Columns of a scaled by one over n minus one
and that's roughly What's Happening Here so intuitively the derivative flow tells us that DB and diff2
will be the local derivative of this operation and there are many ways to do this by the way but I like to do
something like this torch dot once like of bndf2 so I'll create a large array
two-dimensional of ones and then I will scale it so 1.0 divided by n minus 1.
so this is a array of um one over n minus one and that's sort of like the local derivative
and now for the chain rule I will simply just multiply it by dbm bar
and notice here what's going to happen this is 32 by 64 and this is just 1 by 64. so I'm letting the broadcasting do
the replication because internally in pytorch basically dbnbar which is 1 by
64 row vector well in this multiplication get um copied vertically until the two are
of the same shape and then there will be an element wise multiply and so that uh so that the broadcasting is basically
doing the replication and I will end up with the derivatives of DB and diff2 here
so this is the candidate solution let's bring it down here let's uncomment this line where we check
it and let's hope for the best and indeed we see that this is the correct formula next up let's
differentiate here and to be in this so here we have that b and diff is element y squared to create B and F2
so this is a relatively simple derivative because it's a simple element wise operation so it's kind of like the
scalar case and we have that DB and div should be if this is x squared then the
derivative of this is 2x right so it's simply 2 times B and if that's the local
derivative and then times chain Rule and the shape of these is the same they are of the
same shape so times this so that's the backward pass for this variable let me bring that down here
and now we have to be careful because we already calculated dbm depth right so this is just the end of the other uh you
know other Branch coming back to B and diff because B and diff was already back propagated to way over here
from being raw so we now completed the second branch and so that's why I have to do plus equals and if you recall we
had an incorrect derivative for being diff before and I'm hoping that once we append this last missing piece we have
the exact correctness so let's run ambient to be in div now actually shows
the exact correct derivative um so that's comforting okay so let's now back propagate through this line
here um the first thing we do of course is we check the shapes and I wrote them out
here and basically the shape of this is 32 by 64. hpbn is the same shape
but B and mean I is a row Vector 1 by 64. so this minus here will actually do
broadcasting and so we have to be careful with that and as a hint to us again because of The Duality a
broadcasting and the forward pass means a variable reuse and therefore there will be a sum in the backward pass
so let's write out the backward pass here now um back propagate into the hpbn
because this is these are the same shape then the local derivative for each one of the elements here is just one for the
corresponding element in here so basically what this means is that the gradient just simply copies it's just a
variable assignment it's quality so I'm just going to clone this tensor just for safety to create an exact copy of DB and
div and then here to back propagate into this one what I'm inclined to do here is
will basically be uh what is the local derivative well
it's negative torch.1's like of the shape of uh B and diff
right and then times
the um the derivative here dbf
and this here is the back propagation for the replicated B and mean I so I still have to back propagate
through the uh replication in the broadcasting and I do that by doing a sum so I'm going to take this whole
thing and I'm going to do a sum over the zeroth dimension which was the replication
so if you scrutinize this by the way you'll notice that this is the same shape as that and so what I'm doing uh
what I'm doing here doesn't actually make that much sense because it's just a array of ones multiplying DP and diff so
in fact I can just do this um and that is equivalent
so this is the candidate backward pass let me copy it here and then let me
comment out this one and this one enter
and it's wrong damn
actually sorry this is supposed to be wrong and it's supposed to be wrong because
we are back propagating from a b and diff into hpbn and but we're not done
because B and mean I depends on hpbn and there will be a second portion of that
derivative coming from this second Branch so we're not done yet and we expect it to be incorrect so there you
go uh so let's now back propagate from uh B and mean I into hpbn
um and so here again we have to be careful because there's a broadcasting along
um or there's a Sum along the zeroth dimension so this will turn into broadcasting in the backward pass now
and I'm going to go a little bit faster on this line because it is very similar to the line that we had before and
multiplies in the past in fact so the hpbn
will be the gradient will be scaled by 1 over n and then basically this gradient here on
dbn mean I is going to be scaled by 1 over n and then it's going to flow across all the
columns and deposit itself into the hpvn so what we want is this thing scaled by
1 over n only put the constant up front here
um so scale down the gradient and now we need to replicate it across all the um
across all the rows here so we I like to do that by torch.lunslike of basically
um hpbn and I will let the broadcasting do the work of replication
so
like that so this is uh the hppn and hopefully
we can plus equals that
so this here is broadcasting um and then this is the scaling so this
should be current okay so that completes the back propagation of the bathroom layer and we are now
here let's back propagate through the linear layer one here now because everything is getting a little
vertically crazy I copy pasted the line here and let's just back properly through this one line
so first of course we inspect the shapes and we see that this is 32 by 64. MCAT
is 32 by 30. W1 is 30 30 by 64 and B1 is just 64. so
as I mentioned back propagating through linear layers is fairly easy just by matching the shapes so let's do that we
have that dmcat should be um some matrix multiplication of dhbn
with uh W1 and one transpose thrown in there so to make uh MCAT be 32 by 30
I need to take dhpn 32 by 64 and multiply it by w1.
transpose to get the only one I need to end up
with 30 by 64. so to get that I need to take uh MCAT
transpose and multiply that by uh dhpion
and finally to get DB1 this is a addition and we saw that
basically I need to just sum the elements in dhpbn along some Dimension and to make the dimensions work out I
need to Sum along the zeroth axis here to eliminate this Dimension and we do
not keep dims uh so that we want to just get a single one-dimensional lecture of 64.
so these are the claimed derivatives let me put that here and let me
uncomment three lines and cross our fingers everything is great okay so we now
continue almost there we have the derivative of MCAT and we want to derivative we want to back propagate
into m so I again copied this line over here so this is the forward pass and then
this is the shapes so remember that the shape here was 32 by 30 and the original shape of M plus 32 by 3 by 10. so this
layer in the forward pass as you recall did the concatenation of these three 10-dimensional character vectors
and so now we just want to undo that so this is actually relatively straightforward operation because uh the
backward pass of the what is the view view is just a representation of the array it's just a logical form of how
you interpret the array so let's just reinterpret it to be what it was before so in other words the end is not uh 32
by 30. it is basically dmcat but if you view it as the original shape
so just m dot shape uh you can you can pass in tuples into
view and so this should just be okay
we just re-represent that view and then we uncomment this line here and
hopefully yeah so the derivative of M is correct
so in this case we just have to re-represent the shape of those derivatives into the original View so now we are at the final line and the
only thing that's left to back propagate through is this indexing operation here MSC at xB so as I did before I copy
pasted this line here and let's look at the shapes of everything that's involved and remind ourselves how this worked
so m.shape was 32 by 3 by 10. it says 32 examples and then we have
three characters each one of them has a 10 dimensional embedding and this was achieved by taking the
lookup table C which have 27 possible characters each of them 10 dimensional and we
looked up at the rows that were specified inside this tensor xB
so XB is 32 by 3 and it's basically giving us for each example the Identity or the index of which character is part
of that example and so here I'm showing the first five rows of three of this tensor xB
and so we can see that for example here it was the first example in this batch is that the first character and the
first character and the fourth character comes into the neural net and then we want to predict the next
character in a sequence after the character is one one four so basically What's Happening Here is
there are integers inside XB and each one of these integers is specifying which row of C we want to pluck out
right and then we arrange those rows that we've plucked out into 32 by 3 by
10 tensor and we just package them in we just package them into the sensor and now what's happening is that we have
D amp so for every one of these uh basically plucked out rows we have their gradients
now but they're arranged inside this 32 by 3 by 10 tensor so all we have to do now is
we just need to Route this gradient backwards through this assignment so we need to find which row of C that every
one of these um 10 dimensional embeddings come from and then we need to deposit them into DC
so we just need to undo the indexing and of course if any of these rows of C was
used multiple times which almost certainly is the case like the row one and one was used multiple times then we
have to remember that the gradients that arrive there have to add so for each occurrence we have to have
an addition so let's now write this out and I don't actually know if like a much better way
to do this than a for Loop unfortunately in Python um so maybe someone can come up with a vectorized efficient operation but for
now let's just use for loops so let me create a torch.zeros like C to initialize uh just uh 27 by 10
tensor of all zeros and then honestly 4K in range XB dot
shape at zero maybe someone has a better way to do this but for J and range
be that shape at one this is going to iterate over all the
um all the elements of XB all these integers and then let's get the index at this
position so the index is basically x b at KJ
so that an example of that like is 11 or 14 and so on and now in the forward pass we took
and we basically took um the row of C at index and we deposited
it into M at K of J that's what happened that's where they are packaged so now we need to go
backwards and we just need to route DM at the position KJ
we now have these derivatives for each position and it's 10 dimensional
and you just need to go into the correct row of C so DC rather at IX is this but plus
equals because there could be multiple occurrences uh like the same row could have been used many many times and so
all of those derivatives will just go backwards through the indexing and they
will add so this is my candidate solution
let's copy it here let's uncomment this and cross our
fingers hey so that's it we've back propagated through
this entire Beast so there we go totally makes sense
exercise 2: cross entropy loss backward pass
so now we come to exercise two it basically turns out that in this first exercise we were doing way too much work
uh we were back propagating way too much and it was all good practice and so on but it's not what you would do in
practice and the reason for that is for example here I separated out this loss calculation over multiple lines and I
broke it up all all to like its smallest atomic pieces and we back propagated through all of those individually
but it turns out that if you just look at the mathematical expression for the loss um then actually you can do the
differentiation on pen and paper and a lot of terms cancel and simplify and the mathematical expression you end up with
can be significantly shorter and easier to implement than back propagating through all the little pieces of
everything you've done so before we had this complicated forward paths going from logits to the
loss but in pytorch everything can just be glued together into a single call at that cross entropy you just pass in
logits and the labels and you get the exact same loss as I verify here so our previous loss and the fast loss coming
from the chunk of operations as a single mathematical expression is the same but
it's much much faster in a forward pass it's also much much faster in backward pass and the reason for that is if you
just look at the mathematical form of this and differentiate again you will end up with a very small and short expression so that's what we want to do
here we want to in a single operation or in a single go or like very quickly go
directly to delojits and we need to implement the logits as a function of logits and yb's
but it will be significantly shorter than whatever we did here where to get to deluggets we had to go all the way
here so all of this work can be skipped in a much much simpler mathematical
expression that you can Implement here so you can give it a shot yourself basically look at what exactly is the
mathematical expression of loss and differentiate with respect to the logits so let me show you a hint you can of
course try it fully yourself but if not I can give you some hint of how to get started mathematically
so basically What's Happening Here is we have logits then there's a softmax that takes the logits and gives you
probabilities then we are using the identity of the correct next character to pluck out a row of probabilities take
the negative log of it to get our negative block probability and then we average up all the log probabilities or
negative block probabilities to get our loss so basically what we have is for a single individual example rather we have
that loss is equal to negative log probability uh where P here is kind of
like thought of as a vector of all the probabilities so at the Y position where Y is the label
and we have that P here of course is the softmax so the ith component of P of
this probability Vector is just the softmax function so raising all the logits uh basically to the power of E
and normalizing so everything comes to 1. now if you write out P of Y here you can
just write out the soft Max and then basically what we're interested in is we're interested in the derivative of the loss with respect to the I logit
and so basically it's a d by DLI of this expression here where we have L indexed with the
specific label Y and on the bottom we have a sum over J of e to the L J and the negative block of all that so
potentially give it a shot pen and paper and see if you can actually derive the expression for the loss by DLI and then
we're going to implement it here okay so I'm going to give away the result here so this is some of the math I did to
derive the gradients analytically and so we see here that I'm just applying the
rules of calculus from your first or second year of bachelor's degree if you took it and we see that the expression
is actually simplify quite a bit you have to separate out the analysis in the case where the ith index that you're
interested in inside logits is either equal to the label or it's not equal to the label and then the expression
simplify and cancel in a slightly different way and what we end up with is something very very simple
and we either end up with basically pirai where p is again this Vector of
probabilities after a soft Max or P at I minus 1 where we just simply subtract a
one but in any case we just need to calculate the soft Max p e and then in the correct Dimension we need to
subtract one and that's the gradient the form that it takes analytically so let's implement this basically and we have to
keep in mind that this is only done for a single example but here we are working with batches of examples
so we have to be careful of that and then the loss for a batch is the average
loss over all the examples so in other words is the example for all the individual examples is the loss for each
individual example summed up and then divided by n and we have to back propagate through that as well and be
careful with it so deluggets is going to be of that soft Max
uh pytorch has a softmax function that you can call and we want to apply the softmax on the logits and we want to go
in the dimension that is one so basically we want to do the softmax along the rows of these logits
then at the correct positions we need to subtract a 1. so delugits at iterating
over all the rows and indexing into the columns provided by the correct labels inside YB
we need to subtract one and then finally it's the average loss
that is the loss and in the average there's a one over n of all the losses added up and so we need to also
propagate through that division so the gradient has to be scaled down by by n as well because of the mean
but this otherwise should be the result so now if we verify this we see that we don't get an exact match
but at the same time the maximum difference from logits from pytorch and
RD logits here is uh on the order of 5e negative 9. so it's a tiny tiny number
so because of floating point wantiness we don't get the exact bitwise result
but we basically get the correct answer approximately now I'd like to pause here briefly
before we move on to the next exercise because I'd like us to get an intuitive sense of what the logits is because it
has a beautiful and very simple explanation honestly um so here I'm taking the logits and I'm
visualizing it and we can see that we have a batch of 32 examples of 27 characters
and what is the logits intuitively right the logits is the probabilities that the
properties Matrix in the forward pass but then here these black squares are the positions of the correct indices
where we subtracted a one and so uh what is this doing right these
are the derivatives on the logits and so let's look at just the first row here
so that's what I'm doing here I'm clocking the probabilities of these logits and then I'm taking just the first row and this is the probability
row and then the logits of the first row and multiplying by n just for us so that
we don't have the scaling by n in here and everything is more interpretable we see that it's exactly equal to the
probability of course but then the position of the correct index has a minus equals one so minus one on that
position and so notice that um if you take Delo Jets at zero and you
sum it it actually sums to zero and so you should think of these uh gradients here
at each cell as like a force um we are going to be basically pulling
down on the probabilities of the incorrect characters and we're going to be pulling up on the probability at the
correct index and that's what's basically happening in each row and thus
the amount of push and pull is exactly equalized because the sum is zero so the
amount to which we pull down in the probabilities and the demand that we push up on the probability of the correct character is equal
so sort of the the repulsion and the attraction are equal and think of the neural app now as a like a massive uh
pulley system or something like that we're up here on top of the logits and we're pulling up we're pulling down the
properties of Incorrect and pulling up the property of the correct and in this complicated pulley system because everything is mathematically uh just
determined just think of it as sort of like this tension translating to this complicating pulling mechanism and then
eventually we get a tug on the weights and the biases and basically in each update we just kind of like tug in the
direction that we like for each of these elements and the parameters are slowly given in to the tug and that's what
training in neural net kind of like looks like on a high level and so I think the the forces of push
and pull in these gradients are actually uh very intuitive here we're pushing and pulling on the correct answer and the
incorrect answers and the amount of force that we're applying is actually proportional to uh the probabilities
that came out in the forward pass and so for example if our probabilities came out exactly correct so they would
have had zero everywhere except for one at the correct uh position then the the
logits would be all a row of zeros for that example there would be no push and pull so the amount to which your
prediction is incorrect is exactly the amount by which you're going to get a pull or a push in that dimension
so if you have for example a very confidently mispredicted element here then
um what's going to happen is that element is going to be pulled down very heavily and the correct answer is going
to be pulled up to the same amount and the other characters are not going to be influenced too much
so the amounts to which you mispredict is then proportional to the strength of the pole and that's happening
independently in all the dimensions of this of this tensor and it's sort of very intuitive and varies to think
through and that's basically the magic of the cross-entropy loss and what it's doing dynamically in the backward pass
of the neural net so now we get to exercise number three which is a very fun exercise
exercise 3: batch norm layer backward pass
um depending on your definition of fun and we are going to do for batch normalization exactly what we did for cross entropy loss in exercise number
two that is we are going to consider it as a glued single mathematical expression and back propagate through it
in a very efficient manner because we are going to derive a much simpler formula for the backward path of batch
normalization and we're going to do that using pen and paper so previously we've broken up
bastionalization into all of the little intermediate pieces and all the atomic operations inside it and then we back
propagate it through it one by one now we just have a single sort of
forward pass of a batch form and it's all glued together and we see that we get the exact same
result as before now for the backward pass we'd like to also Implement a single formula
basically for back propagating through this entire operation that is the bachelorization so in the forward pass previously we
took hpvn the hidden states of the pre-batch realization and created H
preact which is the hidden States just before the activation in the bachelorization paper each pbn is
X and each preact is y so in the backward pass what we'd like to do now is we have DH preact and we'd
like to produce d h previous and we'd like to do that in a very efficient manner so that's the name of
the game calculate the H previan given DH preact and for the purposes of this
exercise we're going to ignore gamma and beta and their derivatives because they take on a very simple form in a very
similar way to what we did up above so let's calculate this given that right
here so to help you a little bit like I did before I started off the implementation
here on pen and paper and I took two sheets of paper to derive the mathematical formulas for the backward
pass and basically to set up the problem uh just write out the MU Sigma Square
variance x i hat and Y I exactly as in the paper except for the bezel
correction and then in a backward pass we have the derivative of the loss with respect to
all the elements of Y and remember that Y is a vector there's there's multiple numbers here
so we have all the derivatives with respect to all the Y's and then there's a demo and a beta and
this is kind of like the compute graph the gamma and the beta there's the X hat and then the MU and the sigma squared
and the X so we have DL by DYI and we won't DL by d x i for all the I's in
these vectors so this is the compute graph and you have to be careful because I'm trying to
note here that these are vectors so there's many nodes here inside x x hat
and Y but mu and sigma sorry Sigma Square are just individual scalars
single numbers so you have to be careful with that you have to imagine there's multiple nodes here or you're going to
get your math wrong um so as an example I would suggest that you go in the following order one two
three four in terms of the back propagation so back propagating to X hat then into Sigma Square then into mu and
then into X um just like in a topological sort in micrograd we would go from right to left
you're doing the exact same thing except you're doing it with symbols and on a piece of paper
so for number one uh I'm not giving away too much if you want DL of d x i hat
then we just take DL by DYI and multiply it by gamma because of this expression
here where any individual Yi is just gamma times x i hat plus beta so it
doesn't help you too much there but this gives you basically the derivatives for all the X hats and so now try to go
through this computational graph and derive what is DL by D Sigma Square
and then what is DL by B mu and then one is D L by DX eventually so give it a go and I'm going
to be revealing the answer one piece at a time okay so to get DL by D Sigma Square we have to remember again like I
mentioned that there are many excess X hats here and remember that Sigma square is just a
single individual number here so when we look at the expression
for the L by D Sigma Square we have that we have to actually consider all the possible paths that um
we basically have that there's many X hats and they all feed off from they all depend on Sigma Square so Sigma square
has a large fan out there's lots of arrows coming out from Sigma square into all the X hats
and then there's a back propagating signal from each X hat into Sigma square and that's why we actually need to sum
over all those I's from I equal to 1 to m of the DL by d x i hat which is the
global gradient times the x i Hat by D Sigma Square which is the local gradient
of this operation here and then mathematically I'm just working it out here and I'm simplifying and you
get a certain expression for DL by D Sigma square and we're going to be using this expression when we back propagate
into mu and then eventually into X so now let's continue our back propagation into mu so what is D L by D mu now again
be careful that mu influences X hat and X hat is actually lots of values so for
example if our mini batch size is 32 as it is in our example that we were working on then this is 32 numbers and
32 arrows going back to mu and then mu going to Sigma square is just a single Arrow because Sigma square is a scalar
so in total there are 33 arrows emanating from you and then all of them
have gradients coming into mu and they all need to be summed up and so that's why when we look at the
expression for DL by D mu I am summing up over all the gradients of DL by d x i
hat times the x i Hat by being mu uh so that's the that's this arrow and
that's 32 arrows here and then plus the one Arrow from here which is the L by the sigma Square Times the sigma squared
by D mu so now we have to work out that expression and let me just reveal the rest of it
uh simplifying here is not complicated the first term and you just get an expression here
for the second term though there's something really interesting that happens when we look at the sigma squared by D
mu and we simplify at one point if we assume that in a
special case where mu is actually the average of X I's as it is in this case
then if we plug that in then actually the gradient vanishes and becomes exactly zero and that makes the entire
second term cancel and so these uh if you just have a mathematical expression like this and
you look at D Sigma Square by D mu you would get some mathematical formula for
how mu impacts Sigma Square but if it is the special case that Nu is actually equal to the average as it is
in the case of pastoralization that gradient will actually vanish and become zero so the whole term cancels and we
just get a fairly straightforward expression here for DL by D mu okay and now we get to the craziest part which is
uh deriving DL by dxi which is ultimately what we're after now let's count
first of all how many numbers are there inside X as I mentioned there are 32 numbers there are 32 Little X I's and
let's count the number of arrows emanating from each x i there's an arrow going to Mu an arrow
going to Sigma Square and then there's an arrow going to X hat but this Arrow here let's scrutinize
that a little bit each x i hat is just a function of x i and all the other scalars so x i hat
only depends on x i and none of the other X's and so therefore there are actually in
this single Arrow there are 32 arrows but those 32 arrows are going exactly parallel they don't interfere and
they're just going parallel between x and x hat you can look at it that way and so how many arrows are emanating
from each x i there are three arrows mu Sigma squared and the associated X hat
and so in back propagation we now need to apply the chain rule and we need to add up those three contributions
so here's what that looks like if I just write that out we have uh we're going through we're
chaining through mu Sigma square and through X hat and those three terms are just here
now we already have three of these we have d l by d x i hat
we have DL by D mu which we derived here and we have DL by D Sigma Square which we derived here but we need three other
terms here the this one this one and this one so I invite you to try to derive them it's
not that complicated you're just looking at these Expressions here and differentiating with respect to x i
so give it a shot but here's the result or at least what I got
um yeah I'm just I'm just differentiating with respect to x i for all these expressions and honestly I don't think
there's anything too tricky here it's basic calculus now it gets a little bit more tricky is
we are now going to plug everything together so all of these terms multiplied with all of these terms and
add it up according to this formula and that gets a little bit hairy so what ends up happening is
uh you get a large expression and the thing to be very careful with here of course
is we are working with a DL by dxi for specific I here but when we are plugging
in some of these terms like say um this term here deal by D signal squared
you see how the L by D Sigma squared I end up with an expression and I'm iterating over little I's here but I
can't use I as the variable when I plug in here because this is a different I from this eye
this I here is just a place or like a local variable for for a for Loop in here so here when I plug that in you
notice that I rename the I to a j because I need to make sure that this J is not that this J is not this I this J
is like like a little local iterator over 32 terms and so you have to be careful with that when you're plugging
in the expressions from here to here you may have to rename eyes into J's and you have to be very careful what is actually
an I with respect to the L by t x i so some of these are J's some of these
are I's and then we simplify this expression and I guess like the big thing to notice
here is a bunch of terms just kind of come out to the front and you can refactor them there's a sigma squared plus Epsilon raised to the power of
negative three over two uh this Sigma squared plus Epsilon can be actually separated out into three terms each of
them are Sigma squared plus Epsilon to the negative one over two so the three of them multiplied is equal to this and
then those three terms can go different places because of the multiplication so one of them actually comes out to the
front and will end up here outside one of them joins up with this term and one
of them joins up with this other term and then when you simplify the expression you'll notice that some of
these terms that are coming out are just the x i hats so you can simplify just by rewriting
that and what we end up with at the end is a fairly simple mathematical expression over here that I cannot simplify further
but basically you'll notice that it only uses the stuff we have and it derives the thing we need so we have the L by d
y for all the I's and those are used plenty of times here and also in
addition what we're using is these x i hats and XJ hats and they just come from the forward pass
and otherwise this is a simple expression and it gives us DL by d x i for all the I's and that's ultimately
what we're interested in so that's the end of Bachelor backward pass analytically let's now implement
this final result okay so I implemented the expression into a single line of code here and you
can see that the max diff is Tiny so this is the correct implementation of this formula now I'll just uh
basically tell you that getting this formula here from this mathematical expression was not trivial and there's a
lot going on packed into this one formula and this is a whole exercise by itself because you have to consider the
fact that this formula here is just for a single neuron and a batch of 32 examples but what I'm doing here is I'm
actually we actually have 64 neurons and so this expression has to in parallel evaluate the bathroom backward pass for
all of those 64 neurons in parallel independently so this has to happen basically in every single
um column of the inputs here and in addition to that you see how
there are a bunch of sums here and we need to make sure that when I do those sums that they broadcast correctly onto
everything else that's here and so getting this expression is just like highly non-trivial and I invite you
to basically look through it and step through it and it's a whole exercise to make sure that this this checks out but
once all the shapes are green and once you convince yourself that it's correct you can also verify that Patrick's gets
the exact same answer as well and so that gives you a lot of peace of mind that this mathematical formula is
correctly implemented here and broadcasted correctly and replicated in parallel for all of the 64 neurons
inside this bastrum layer okay and finally exercise number four asks you to
exercise 4: putting it all together
put it all together and uh here we have a redefinition of the entire problem so you see that we reinitialize the neural
nut from scratch and everything and then here instead of calling loss that backward we want to have the manual back
propagation here as we derived It Up Above so go up copy paste all the chunks of code that we've already derived put
them here and drive your own gradients and then optimize this neural nut basically using your own gradients all
the way to the calibration of The Bachelor and the evaluation of the loss and I was able to achieve quite a good
loss basically the same loss you would achieve before and that shouldn't be surprising because all we've done is
we've really gotten to Lost That backward and we've pulled out all the code and inserted it here but those gradients
are identical and everything is identical and the results are identical it's just that we have full visibility
on exactly what goes on under the hood I'll plot that backward in this specific case and this is all of our code this is
the full backward pass using basically the simplified backward pass for the cross entropy loss and the mass
generalization so back propagating through cross entropy the second layer the 10 H nonlinearity the batch
normalization uh through the first layer and through the embedding and so you see that this
is only maybe what is this 20 lines of code or something like that and that's what gives us gradients and now we can
potentially erase losses backward so the way I have the code set up is you should be able to run this entire cell once you
fill this in and this will run for only 100 iterations and then break and it breaks because it gives you an
opportunity to check your gradients against pytorch so here our gradients we see are not
exactly equal they are approximately equal and the differences are tiny wanting negative 9 or so and I don't
exactly know where they're coming from to be honest um so once we have some confidence that the gradients are basically correct we
can take out the gradient tracking we can disable this breaking statement
and then we can basically disable lost of backward we don't need it anymore it feels amazing
to say that and then here when we are doing the update we're not going to use P dot grad
this is the old way of pytorch we don't have that anymore because we're not doing backward we are going to use this
update where we you see that I'm iterating over I've arranged the grads to be in the
same order as the parameters and I'm zipping them up the gradients and the parameters into p and grad and then here
I'm going to step with just the grad that we derived manually so the last piece
um is that none of this now requires gradients from pytorch and so one thing
you can do here um is you can do with no grad and offset
this whole code block and really what you're saying is you're telling Pat George that hey I'm not going to call backward on any of this
and this allows pytorch to be a bit more efficient with all of it and then we should be able to just uh
run this and it's running
and you see that losses backward is commented out and we're optimizing
so we're going to leave this run and uh hopefully we get a good result
okay so I allowed the neural net to finish optimization then here I calibrate the bachelor
parameters because I did not keep track of the running mean and very variants in their training Loop
then here I ran the loss and you see that we actually obtained a pretty good loss very similar to what we've achieved
before and then here I'm sampling from the model and we see some of the name like gibberish that we're sort of used to so
basically the model worked and samples uh pretty decent results compared to
what we were used to so everything is the same but of course the big deal is that we did not use lots of backward we
did not use package Auto grad and we estimated our gradients ourselves by hand and so hopefully you're looking at this
the backward pass of this neural net and you're thinking to yourself actually that's not too complicated
um each one of these layers is like three lines of code or something like that and most of it is fairly straightforward
potentially with the notable exception of the batch normalization backward pass otherwise it's pretty good okay and
outro
that's everything I wanted to cover for this lecture so hopefully you found this interesting and what I liked about it
honestly is that it gave us a very nice diversity of layers to back propagate through and
um I think it gives a pretty nice and comprehensive sense of how these backward passes are implemented and how they work and you'd be able to derive
them yourself but of course in practice you probably don't want to and you want to use the pythonograd but hopefully you
have some intuition about how gradients flow backwards through the neural net starting at the loss and how they flow
through all the variables and all the intermediate results and if you understood a good chunk of it
and if you have a sense of that then you can count yourself as one of these buff doji's on the left instead of the uh
those on the right here now in the next lecture we're actually going to go to recurrent neural nuts lstms and all the
other variants of RNs and we're going to start to complexify the architecture and start to achieve better uh log
likelihoods and so I'm really looking forward to that and I'll see you then





intro
hi everyone today we are continuing our implementation of make more our favorite character level language model
now you'll notice that the background behind me is different that's because I am in Kyoto and it is awesome so I'm in
a hotel room here now over the last few lectures we've built up to this architecture that is a
multi-layer perceptron character level language model so we see that it receives three previous characters and
tries to predict the fourth character in a sequence using a very simple multi perceptron using one hidden layer of
neurons with 10ational neuralities so we'd like to do now in this lecture is I'd like to complexify this
architecture in particular we would like to take more characters in a sequence as an input not just three and in addition
to that we don't just want to feed them all into a single hidden layer because that squashes too much information too
quickly instead we would like to make a deeper model that progressively fuses this information to make its guess about
the next character in a sequence and so we'll see that as we make this architecture more complex we're actually
going to arrive at something that looks very much like a wavenet the witness is this paper published by
the point in 2016 and it is also a language model basically but it tries to
predict audio sequences instead of character level sequences or Word level sequences but fundamentally the modeling
setup is identical it is an auto aggressive model and it tries to predict next character in a sequence and the
architecture actually takes this interesting hierarchical sort of approach to predicting the next
character in a sequence uh with the street-like structure and this is the architecture and we're going to
implement it in the course of this video so let's get started so the starter code for part five is very similar to where
starter code walkthrough
we ended up in in part three recall that part four was the manual black replication exercise that is kind of an
aside so we are coming back to part three copy pasting chunks out of it and that is our starter code for part five
I've changed very few things otherwise so a lot of this should look familiar to if you've gone through part three so in
particular very briefly we are doing Imports we are reading our our data set of words and we are processing their set
of words into individual examples and none of this data generation code has changed and basically we have lots and
lots of examples in particular we have 182 000 examples of three characters try
to predict the fourth one and we've broken up every one of these words into little problems of given three
characters predict the fourth one so this is our data set and this is what we're trying to get the neural lot to do
now in part three we started to develop our code around these layer modules
um that are for example like class linear and we're doing this because we want to think of these modules as
building blocks and like a Lego building block bricks that we can sort of like stack up into neural networks and we can
feed data between these layers and stack them up into a sort of graphs now we also developed these layers to
have apis and signatures very similar to those that are found in pytorch so we
have torch.nn and it's got all these layer building blocks that you would use in practice and we were developing all
of these to mimic the apis of these so for example we have linear so there will also be a torch.nn.linear and its
signature will be very similar to our signature and the functionality will be also quite identical as far as I'm aware
so we have the linear layer with the Bass from 1D layer and the 10h layer that we developed previously
and linear just as a matrix multiply in the forward pass of this module batch
number of course is this crazy layer that we developed in the previous lecture and what's crazy about it is
well there's many things number one it has these running mean and variances that are trained outside of back
propagation they are trained using exponential moving average inside this
layer when we call the forward pass in addition to that there's this training plug because the
behavior of bathroom is different during train time and evaluation time and so suddenly we have to be very careful that bash form is in its correct state that
it's in the evaluation state or training state so that's something to now keep track of something that sometimes introduces bugs
uh because you forget to put it into the right mode and finally we saw that Bachelor couples the statistics or the
the activations across the examples in the batch so normally we thought of the bat as just an efficiency thing but now
we are coupling the computation across batch elements and it's done for the
purposes of controlling the automation statistics as we saw in the previous video so it's a very weird layer at least a
lot of bugs partly for example because you have to modulate the training in eval phase and
so on um in addition for example you have to wait for uh the mean and the variance to
settle and to actually reach a steady state and so um you have to make sure that you basically there's state in this
layer and state is harmful uh usually now I brought out the generator object
previously we had a generator equals g and so on inside these layers I've discarded that in favor of just
initializing the torch RNG outside here use it just once globally just for
Simplicity and then here we are starting to build out some of the neural network elements this should look very familiar we are we
have our embedding table C and then we have a list of players and uh it's a linear feeds to Bachelor feeds to 10h
and then a linear output layer and its weights are scaled down so we are not confidently wrong at the initialization
we see that this is about 12 000 parameters we're telling pytorch that the parameters require gradients
the optimization is as far as I'm aware identical and should look very very familiar
nothing changed here uh loss function looks very crazy we
should probably fix this and that's because 32 batch elements are too few and so you can get very lucky lucky or
unlucky in any one of these batches and it creates a very thick loss function um so we're going to fix that soon
now once we want to evaluate the trained neural network we need to remember because of the bathroom layers to set
all the layers to be training equals false so this only matters for the bathroom layer so far
and then we evaluate we see that currently we have validation
loss of 2.10 which is fairly good but there's still ways to go but even at
2.10 we see that when we sample from the model we actually get relatively name-like results that do not exist in a
training set so for example Yvonne kilo Pros
Alaia Etc so certainly not reasonable not unreasonable I would say
but not amazing and we can still push this validation loss even lower and get much better samples that are even more
name-like so let's improve this model okay first let's fix this graph because
let’s fix the learning rate plot
it is daggers in my eyes and I just can't take it anymore um so last I if you recall is a python
list of floats so for example the first 10 elements
now what we'd like to do basically is we need to average up um some of these values to get a more
sort of Representative uh value along the way so one way to do this is the following
in part torch if I create for example a tensor of the first 10 numbers
then this is currently a one-dimensional array but recall that I can view this array as two-dimensional so for example
I can use it as a two by five array and this is a 2d tensor now two by five and
you see what petroch has done is that the first row of this tensor is the first five elements and the second row
is the second five elements I can also view it as a five by two as an example
and then recall that I can also use negative one in place of one of
these numbers and pytorch will calculate what that number must be in order to make the number of elements work out so this can
be this or like that but it will work of course this would not work
okay so this allows it to spread out some of the consecutive values into rows so that's very helpful because what we
can do now is first of all we're going to create a torshot tensor out of the a
list of floats and then we're going to view it as whatever it is but we're going to
stretch it out into rows of 1000 consecutive elements so the shape of this now becomes 200 by 1000. and each
row is one thousand um consecutive elements in this list so that's very helpful because now we
can do a mean along the rows and the shape of this will just be 200.
and so we've taken basically the mean on every row so plt.plot of that should be something nicer
much better so we see that we basically made a lot of progress and then here this is the
learning rate Decay so here we see that the learning rate Decay subtracted a ton of energy out of the system and allowed
us to settle into sort of the local minimum in this optimization so this is a much nicer plot let me come
up and delete the monster and we're going to be using this going forward now next up what I'm bothered by is that you
pytorchifying our code: layers, containers, torch.nn, fun bugs
see our forward pass is a little bit gnarly and takes way too many lines of code
so in particular we see that we've organized some of the layers inside the layers list but not all of them uh for
no reason so in particular we see that we still have the embedding table a special case outside of the layers and
in addition to that the viewing operation here is also outside of our layers so let's create layers for these
and then we can add those layers to just our list so in particular the two things that we
need is here we have this embedding table and we are indexing at the integers inside uh the batch XB uh
inside the tensor xB so that's an embedding table lookup just done with indexing and then here we see
that we have this view operation which if you recall from the previous video Simply rearranges the character
embeddings and stretches them out into a row and effectively what print that does
is the concatenation operation basically except it's free because viewing is very cheap in pytorch no no memory is being
copied we're just re-representing how we view that tensor so let's create um
modules for both of these operations the embedding operation and flattening operation
so I actually wrote the code in just to save some time so we have a module embedding and a
module pattern and both of them simply do the indexing operation in the forward pass and the flattening operation here
and this C now will just become a salt dot weight inside an embedding module
and I'm calling these layers specifically embedding a platinum because it turns out that both of them actually exist in pi torch so in
phytorch we have n and Dot embedding and it also takes the number of embeddings and the dimensionality of the bedding
just like we have here but in addition python takes in a lot of other keyword arguments that we are not using for our
purposes yet and for flatten that also exists in pytorch and it also takes additional
keyword arguments that we are not using so we have a very simple platform but both of them exist in pytorch
they're just a bit more simpler and now that we have these we can simply take
out some of these special cased um things so instead of C we're just
going to have an embedding and of a cup size and N embed
and then after the embedding we are going to flatten so let's construct those modules and now
I can take out this the and here I don't have to special case anymore because now C is the embeddings
weight and it's inside layers so this should just work
and then here our forward pass simplifies substantially because we don't need to do these now outside of
these layer outside and explicitly they're now inside layers
so we can delete those but now to to kick things off we want this little X which in the beginning is
just XB uh the tensor of integers specifying the identities of these characters at the input
and so these characters can now directly feed into the first layer and this should just work so let me come here and insert a break
because I just want to make sure that the first iteration of this runs and then there's no mistake so that ran
properly and basically we substantially simplified the forward pass here okay I'm sorry I changed my microphone so
hopefully the audio is a little bit better now one more thing that I would like to do in order to pytortify our code even
further is that right now we are maintaining all of our modules in a naked list of layers and we can also
simplify this uh because we can introduce the concept of Pi torch containers so in tors.nn which we are
basically rebuilding from scratch here there's a concept of containers and these containers are basically a way of organizing layers into
lists or dicts and so on so in particular there's a sequential which
maintains a list of layers and is a module class in pytorch and it basically
just passes a given input through all the layers sequentially exactly as we are doing here
so let's write our own sequential I've written a code here and basically
the code for sequential is quite straightforward we pass in a list of layers which we keep here and then given
any input in a forward pass we just call all the layers sequentially and return the result in terms of the parameters
it's just all the parameters of the child modules so we can run this and we can again
simplify this substantially because we don't maintain this naked list of layers we now have a notion of a model which is
a module and in particular is a sequential of all these layers
and now parameters are simply just a model about parameters and so that list comprehension now lives
here and then here we are press here we are doing all the things we used to do
now here the code again simplifies substantially because we don't have to do this forwarding here instead of just
call the model on the input data and the input data here are the integers inside xB so we can simply do logits which are
the outputs of our model are simply the model called on xB
and then the cross entropy here takes the logits and the targets so this simplifies substantially
and then this looks good so let's just make sure this runs that looks good
now here we actually have some work to do still here but I'm going to come back later for now there's no more layers
there's a model that layers but it's not a to access attributes of these classes
directly so we'll come back and fix this later and then here of course this simplifies substantially as well because logits are
the model called on x and then these low Jets come here
so we can evaluate the train and validation loss which currently is terrible because we just initialized the
neural net and then we can also sample from the model and this simplifies dramatically as well because we just want to call the model
onto the context and outcome logits and these logits go into softmax and get
the probabilities Etc so we can sample from this model what did I screw up
okay so I fixed the issue and we now get the result that we expect which is gibberish because the model is not
trained because we re-initialize it from scratch the problem was that when I fixed this cell to be modeled out layers instead of
just layers I did not actually run the cell and so our neural net was in a training mode and what caused the issue
here is the bathroom layer as bathroom layer of the likes to do because Bachelor was in a training mode and here
we are passing in an input which is a batch of just a single example made up of the context
and so if you are trying to pass in a single example into a bash Norm that is in the training mode you're going to end
up estimating the variance using the input and the variance of a single number is is not a number because it is
a measure of a spread so for example the variance of just the single number five you can see is not a number and so
that's what happened in the master basically caused an issue and then that polluted all of the further processing
so all that we have to do was make sure that this runs and we basically made the
issue of again we didn't actually see the issue with the loss we could have evaluated
the loss but we got the wrong result because basharm was in the training mode and uh and so we still get a result it's
just the wrong result because it's using the uh sample statistics of the batch whereas we want to use the running mean
and running variants inside the bachelor and so again an example of introducing a bug
inline because we did not properly maintain the state of what is training or not okay so I Rewritten everything
overview: WaveNet
and here's where we are as a reminder we have the training loss of 2.05 and validation 2.10
now because these losses are very similar to each other we have a sense that we are not overfitting too much on
this task and we can make additional progress in our performance by scaling up the size of the neural network and
making everything bigger and deeper now currently we are using this architecture here where we are taking in
some number of characters going into a single hidden layer and then going to the prediction of the next character
the problem here is we don't have a naive way of making this bigger in a productive way we could of course use
our layers sort of building blocks and materials to introduce additional layers here and make the network deeper but it
is still the case that we are crushing all of the characters into a single layer all the way at the beginning
and even if we make this a bigger layer and add neurons it's still kind of like silly to squash all that information so
fast in a single step so we'd like to do instead is we'd like our Network to look a lot more like this
in the wavenet case so you see in the wavenet when we are trying to make the prediction for the next character in the
sequence it is a function of the previous characters that are feeding that feed in but not all of these
different characters are not just crushed to a single layer and then you have a sandwich they are crushed slowly
so in particular we take two characters and we fuse them into sort of like a diagram representation and we do that
for all these characters consecutively and then we take the bigrams and we fuse those into four character level chunks
and then we fuse that again and so we do that in this like tree-like hierarchical manner so we fuse the information from
the previous context slowly into the network as it gets deeper and so this is
the kind of architecture that we want to implement now in the wave Nets case this is a visualization of a stack of dilated
causal convolution layers and this makes it sound very scary but actually the idea is very simple and the fact that
it's a dilated causal convolution layer is really just an implementation detail to make everything fast we're going to
see that later but for now let's just keep the basic idea of it which is this Progressive Fusion so we want to make
the network deeper and at each level we want to fuse only two consecutive elements two characters then two bigrams
then two four grams and so on so let's unplant this okay so first up let me scroll to where we built the data set
dataset bump the context size to 8
and let's change the block size from 3 to 8. so we're going to be taking eight characters of context to predict the
ninth character so the data set now looks like this we have a lot more context feeding in to predict any next
character in a sequence and these eight characters are going to be processed in this tree like structure
now if we scroll here everything here should just be able to work so we should be able to redefine the network
re-running baseline code on block_size 8
you see the number of parameters has increased by 10 000 and that's because the block size has grown so this first
linear layer is much much bigger our linear layer now takes eight characters into this middle layer so there's a lot
more parameters there but this should just run let me just break right after
the very first iteration so you see that this runs just fine it's just that this network doesn't make too much sense
we're crushing way too much information way too fast so let's now come in and see how we
could try to implement the hierarchical scheme now before we dive into the detail of the re-implementation here I
was just curious to actually run it and see where we are in terms of the Baseline performance of just lazily scaling up the context length so I'll
let it run we get a nice loss curve and then evaluating the loss we actually see quite a bit of improvement just from
increasing the context line length so I started a little bit of a performance log here and previously where we were is
we were getting a performance of 2.10 on the validation loss and now simply scaling up the contact length from 3 to
8 gives us a performance of 2.02 so quite a bit of an improvement here and
also when you sample from the model you see that the names are definitely improving qualitatively as well
so we could of course spend a lot of time here tuning um uh tuning things and making it even bigger and scaling up the network
further even with the simple um sort of setup here but let's continue
and let's Implement here model and treat this as just a rough Baseline performance but there's a lot of
optimization like left on the table in terms of some of the hyper parameters that you're hopefully getting a sense of
now okay so let's scroll up now and come back up and what I've done here
implementing WaveNet
is I've created a bit of a scratch space for us to just like look at the forward pass of the neural net and inspect the
shape of the tensor along the way as the neural net uh forwards so here I'm just
temporarily for debugging creating a batch of just say four examples so four random integers then I'm plucking out
those rows from our training set and then I'm passing into the model the input xB
now the shape of XB here because we have only four examples is four by eight and this eight is now the current block size
so uh inspecting XP we just see that we have four examples each one of them is a
row of xB and we have eight characters here and this integer tensor just contains the
identities of those characters so the first layer of our neural net is the embedding layer so passing XB this
integer tensor through the embedding layer creates an output that is four by eight by ten
so our embedding table has for each character a 10-dimensional vector that
we are trying to learn and so what the embedding layer does here is it plucks out the embedding
Vector for each one of these integers and organizes it all in a four by eight
by ten tensor now so all of these integers are translated into 10 dimensional vectors inside this
three-dimensional tensor now passing that through the flattened layer as you recall what this does is it views
this tensor as just a 4 by 80 tensor and what that effectively does is that all
these 10 dimensional embeddings for all these eight characters just end up being stretched out into a long row
and that looks kind of like a concatenation operation basically so by viewing the tensor differently we now
have a four by eighty and inside this 80 it's all the 10 dimensional uh vectors just uh concatenate next to each
other and then the linear layer of course takes uh 80 and creates 200 channels
just via matrix multiplication so so far so good now I'd like to show you something surprising
let's look at the insides of the linear layer and remind ourselves how it works
the linear layer here in the forward pass takes the input X multiplies it with a weight and then optionally adds
bias and the weight here is two-dimensional as defined here and the bias is one dimensional here
so effectively in terms of the shapes involved what's happening inside this linear layer looks like this right now
and I'm using random numbers here but I'm just illustrating the shapes and what happens
basically a 4 by 80 input comes into the linear layer that's multiplied by this 80 by 200 weight Matrix inside and
there's a plus 200 bias and the shape of the whole thing that comes out of the linear layer is four by two hundred as
we see here now notice here by the way that this here will create a 4x200 tensor and then
plus 200 there's a broadcasting happening here about 4 by 200 broadcasts with 200 uh so everything works here
so now the surprising thing that I'd like to show you that you may not expect is that this input here that is being
multiplied uh doesn't actually have to be two-dimensional this Matrix multiply
operator in pytorch is quite powerful and in fact you can actually pass in higher dimensional arrays or tensors and
everything works fine so for example this could be four by five by eighty and the result in that case will become four
by five by two hundred you can add as many dimensions as you like on the left here
and so effectively what's happening is that the matrix multiplication only works on the last Dimension and the
dimensions before it in the input tensor are left unchanged
so that is basically these um these dimensions on the left are all treated as just a batch Dimension so we can have
multiple batch dimensions and then in parallel over all those Dimensions we are doing the matrix multiplication on
the last dimension so this is quite convenient because we can use that in our Network now
because remember that we have these eight characters coming in and we don't want to now uh flatten all
of it out into a large eight-dimensional vector because we don't want to Matrix multiply
80. into a weight Matrix multiply immediately instead we want to group
these like this so every consecutive two elements
one two and three and four and five and six and seven and eight all of these should be now basically flattened out and multiplied
by weight Matrix but all of these four groups here we'd like to process in parallel so it's kind of like a batch
Dimension that we can introduce and then we can in parallel basically
process all of these uh bigram groups in the four batch dimensions of an
individual example and also over the actual batch dimension of the you know four examples in our example here so
let's see how that works effectively what we want is right now we take a 4 by 80
and multiply it by 80 by 200 to in the linear layer this is what happens
but instead what we want is we don't want 80 characters or 80 numbers to come in we only want two characters to come
in on the very first layer and those two characters should be fused so in other words we just want 20 to
come in right 20 numbers would come in and here we don't want a 4 by 80 to feed
into the linear layer we actually want these groups of two to feed in so instead of four by eighty we want this
to be a 4 by 4 by 20. so these are the four groups of two and
each one of them is ten dimensional vector so what we want is now is we need to change the flattened layer so it doesn't
output a four by eighty but it outputs a four by four by Twenty where basically these um
every two consecutive characters are uh packed in on the very last Dimension and
then these four is the first batch Dimension and this four is the second batch Dimension referring to the four
groups inside every one of these examples and then this will just multiply like this so this is what we want to get to
so we're going to have to change the linear layer in terms of how many inputs it expects it shouldn't expect 80 it
should just expect 20 numbers and we have to change our flattened layer so it doesn't just fully flatten out this
entire example it needs to create a 4x4 by 20 instead of four by eighty so let's
see how this could be implemented basically right now we have an input that is a four by eight by ten that
feeds into the flattened layer and currently the flattened layer just stretches it out so if you remember the
implementation of flatten it takes RX and it just views it as whatever the batch Dimension is and then
negative one so effectively what it does right now is it does e dot view of 4 negative one and
the shape of this of course is 4 by 80. so that's what currently happens and we
instead want this to be a four by four by Twenty where these consecutive ten-dimensional vectors get concatenated
so you know how in Python you can take a list of range of 10
so we have numbers from zero to nine and we can index like this to get all the
even parts and we can also index like starting at one and going in steps up two to get all
the odd parts so one way to implement this it would be as follows we can take e and we can
index into it for all the batch elements and then just even elements in this
Dimension so at indexes 0 2 4 and 8. and then all the parts here from this
last dimension and this gives us the even characters
and then here this gives us all the odd characters and basically what we want to do is we make
sure we want to make sure that these get concatenated in pi torch and then we want to concatenate these two tensors
along the second dimension so this and the shape of it would be
four by four by Twenty this is definitely the result we want we are explicitly grabbing the even parts and
the odd parts and we're arranging those four by four by ten right next to each other and concatenate
so this works but it turns out that what also works is you can simply use a view again and just request the right shape
and it just so happens that in this case those vectors will again end up being arranged in exactly the way we want so
in particular if we take e and we just view it as a four by four by Twenty which is what we want
we can check that this is exactly equal to but let me call this this is the explicit concatenation I suppose
um so explosives dot shape is 4x4 by 20. if you just view it as 4x4 by 20 you can
check that when you compare to explicit uh you got a big this is element wise
operation so making sure that all of them are true that is the truth so basically long story short we don't need
to make an explicit call to concatenate Etc we can simply take this input tensor
to flatten and we can just view it in whatever way we want and in particular you don't want to
stretch things out with negative one we want to actually create a three-dimensional array and depending on
how many vectors that are consecutive we want to um fuse like for example two then we can
just simply ask for this Dimension to be 20. and um use a negative 1 here and python will
figure out how many groups it needs to pack into this additional batch dimension so let's now go into flatten and
implement this okay so I scroll up here to flatten and what we'd like to do is we'd like to change it now so let me
create a Constructor and take the number of elements that are consecutive that we would like to concatenate now in the
last dimension of the output so here we're just going to remember solve.n equals n
and then I want to be careful here because pipe pytorch actually has a torch to flatten and its keyword
arguments are different and they kind of like function differently so R flatten is going to start to depart from patreon
flatten so let me call it flat flatten consecutive or something like that just to make sure that our apis are about
equal so this uh basically flattens only some n consecutive elements and puts them
into the last dimension now here the shape of X is B by T by C
so let me pop those out into variables and recall that in our example down below B was 4 T
was 8 and C was 10. now instead of doing x dot view of B by
negative one right this is what we had before
we want this to be B by um negative 1 by
and basically here we want c times n that's how many consecutive elements we
want and here instead of negative one I don't super love the use of negative one because I like to be very explicit so
that you get error messages when things don't go according to your expectation so what do we expect here we expect this
to become t divide n using integer division here so that's what I expect to happen
and then one more thing I want to do here is remember previously all the way in the beginning n was three and uh
basically we're concatenating um all the three characters that existed there so we basically are concatenated
everything and so sometimes I can create a spurious dimension of one here so if it is the
case that x dot shape at one is one then it's kind of like a spurious dimension
um so we don't want to return a three-dimensional tensor with a one here we just want to return a two-dimensional
tensor exactly as we did before so in this case basically we will just say x equals x dot squeeze that is a
pytorch function and squeeze takes a dimension that it
either squeezes out all the dimensions of a tensor that are one or you can specify the exact Dimension that you
want to be squeezed and again I like to be as explicit as possible always so I expect to squeeze out the First
Dimension only of this tensor this three-dimensional tensor and if
this Dimension here is one then I just want to return B by c times n and so self dot out will be X and then
we return salt dot out so that's the candidate implementation and of course this should be self.n
instead of just n so let's run and let's come here now
and take it for a spin so flatten consecutive
and in the beginning let's just use eight so this should recover the previous Behavior so flagging
consecutive of eight uh which is the current block size we can do this uh that should recover
the previous Behavior so we should be able to run the model and here we can inspect I have a little
code snippet here where I iterate over all the layers I print the name of this
class and the shape and so we see the shapes as we expect
them after every single layer in the top bit so now let's try to restructure it
using our flattened consecutive and do it hierarchically so in particular
we want to flatten consecutive not just not block size but just two and then we want to process this with
linear now then the number of inputs to this linear will not be an embed times block size it will now only be n embed
times two 20. this goes through the first layer and
now we can in principle just copy paste this now the next linear layer should expect and hidden times two
and the last piece of it should expect and it enters 2 again
so this is sort of like the naive version of it um so running this we now have a much much
bigger model and we should be able to basically just forward the model
and now we can inspect uh the numbers in between so four byte by 20
was Platinum consecutively into four by four by Twenty this was projected into four by four by
two hundred and then bash storm just worked out of the box we have to verify that bastron
does the correct thing even though it takes a three-dimensional impedance that are two dimensional input then we have 10h which is element wise
then we crushed it again so if we flatten consecutively and ended up with a four by two by 400 now
then linear brought it back down to 200 batch room 10h and lastly we get a 4 by
400 and we see that the flattened consecutive for the last flatten here uh it squeezed out that dimension of one so
we only ended up with four by four hundred and then linear Bachelor on 10h and uh the last linear layer to get our
logents and so The Lodges end up in the same shape as they were before but now we actually have a nice three layer
neural nut and it basically corresponds to whoops sorry it basically corresponds
exactly to this network now except only this piece here because we only have three layers whereas here in this
example there's uh four layers with the total receptive field size of 16
characters instead of just eight characters so the block size here is 16. so this piece of it's basically
implemented here um now we just have to kind of figure out some good Channel numbers to use
here now in particular I changed the number of hidden units to be 68 in this architecture because when I use 68 the
training the WaveNet: first pass
number of parameters comes out to be 22 000 so that's exactly the same that we had before and we have the same amount
of capacity at this neural net in terms of the number of parameters but the question is whether we are utilizing
those parameters in a more efficient architecture so what I did then is I got rid of a lot of the debugging cells here
and I rerun the optimization and scrolling down to the result we see that we get the identical performance roughly
so our validation loss now is 2.029 and previously it was 2.027 so controlling
for the number of parameters changing from the flat to hierarchical is not giving us anything yet that said there are two things
um to point out number one we didn't really torture the um architecture here very much this is just my first guess
and there's a bunch of hyper parameters search that we could do in order in terms of how we allocate uh our budget
of parameters to what layers number two we still may have a bug inside the
bachelor 1D layer so let's take a look at um uh that because it runs but does it
do the right thing so I pulled up the layer inspector sort of that we have here and printed out the
fixing batchnorm1d bug
shape along the way and currently it looks like the batch form is receiving an input that is 32 by 4 by 68 right and
here on the right I have the current implementation of Bachelor that we have right now now this bachelor assumed in the way we
wrote it and at the time that X is two-dimensional so it was n by D where n
was the batch size so that's why we only reduced uh the mean and the variance over the zeroth dimension but now X will
basically become three-dimensional so what's happening inside the bachelor right now and how come it's working at all and not giving any errors the reason
for that is basically because everything broadcasts properly but the bachelor is not doing what we need what we wanted to
do so in particular let's basically think through what's happening inside the bathroom uh looking at what's what's do
What's Happening Here I have the code here so we're receiving an input of 32 by 4
by 68 and then we are doing uh here x dot mean here I have e instead of X but
we're doing the mean over zero and that's actually giving us 1 by 4 by 68. so we're doing the mean only over the
very first Dimension and it's giving us a mean and a variance that still maintain this Dimension here
so these means are only taking over 32 numbers in the First Dimension and then when we perform this everything
broadcasts correctly still but basically what ends up happening is
when we also look at the running mean
the shape of it so I'm looking at the model that layers at three which is the first bathroom layer and they're looking at whatever the running mean became and
its shape the shape of this running mean now is 1 by 4 by 68.
right instead of it being um you know just a size of dimension
because we have 68 channels we expect to have 68 means and variances that we're maintaining but actually we have an
array of 4 by 68 and so basically what this is telling us is this bash Norm is only
this bachelor is currently working in parallel over
4 times 68 instead of just 68 channels so basically we are maintaining
statistics for every one of these four positions individually and independently
and instead what we want to do is we want to treat this four as a batch Dimension just like the zeroth dimension
so as far as the bachelor is concerned it doesn't want to average we don't want
to average over 32 numbers we want to now average over 32 times four numbers for every single one of these 68
channels and uh so let me now remove this
it turns out that when you look at the documentation of torch.mean
so let's go to torch.me
in one of its signatures when we specify the dimension we see that the dimension here is not
just it can be in or it can also be a tuple of ins so we can reduce over
multiple integers at the same time over multiple Dimensions at the same time so instead of just reducing over zero we
can pass in a tuple 0 1. and here zero one as well and then what's going to happen is the output of
course is going to be the same but now what's going to happen is because we reduce over 0 and 1 if we
look at immin.shape we see that now we've reduced we took
the mean over both the zeroth and the First Dimension so we're just getting 68 numbers and a
bunch of spurious Dimensions here so now this becomes 1 by 1 by 68 and the
running mean and the running variance analogously will become one by one by 68. so even though there are the
spurious Dimensions uh the current the current the correct thing will happen in that we are only maintaining means and
variances for 64 sorry for 68 channels and we're not calculating the mean
variance across 32 times 4 dimensions so that's exactly what we want and let's
change the implementation of bash term 1D that we have so that it can take in two-dimensional or three-dimensional
inputs and perform accordingly so at the end of the day the fix is relatively straightforward basically the dimension
we want to reduce over is either 0 or the Tuple zero and one depending on the dimensionality of X so if x dot and dim
is two so it's a two dimensional tensor then Dimension we want to reduce over is just the integer zero
L if x dot ending is three so it's a three-dimensional tensor then the dims we're going to assume are zero and one
that we want to reduce over and then here we just pass in dim and if the dimensionality of X is
anything else we'll now get an error which is good um so that should be the fix now I want
to point out one more thing we're actually departing from the API of Pi torch here a little bit because when you
come to batch room 1D and pytorch you can scroll down and you can see that the input to this layer can either be n by C
where n is the batch size and C is the number of features or channels or it actually does accept three-dimensional
inputs but it expects it to be n by C by L where LSA like the sequence length or
something like that so um this is problem because you see how C is
nested here in the middle and so when it gets three-dimensional inputs this bash term layer will reduce over zero and two
instead of zero and one so it basically Pi torch batch number one D layer
assumes that c will always be the First Dimension whereas we'll we assume here
that c is the last Dimension and there are some number of batch Dimensions beforehand
um and so it expects n by C or M by C by all we
expect and by C or n by L by C and so it's a deviation
um I think it's okay I prefer it this way honestly so this is the way that we will
keep it for our purposes so I redefined the layers re-initialize the neural net and did a single forward pass with a break just for one step
looking at the shapes along the way they're of course identical all the shapes are the same but the way we see
that things are actually working as we want them to now is that when we look at the bathroom layer the running mean
shape is now one by one by 68. so we're only maintaining 68 means for every one
of our channels and we're treating both the zeroth and the First Dimension as a batch Dimension which is exactly what we
want so let me retrain the neural lot now okay so I retrained the neural net with the bug fix we get a nice curve and
re-training WaveNet with bug fix
when we look at the validation performance we do actually see a slight Improvement so we went from 2.029 to
2.022 so basically the bug inside the bathroom was holding up us back like a
little bit it looks like and we are getting a tiny Improvement now but it's not clear if this is statistical significant
um and the reason we slightly expect an improvement is because we're not maintaining so many different means and
variances that are only estimated using using 32 numbers effectively now we are estimating them using 32 times 4 numbers
so you just have a lot more numbers that go into any one estimate of the mean and variance and it allows things to be a
bit more stable and less Wiggly inside those estimates of those statistics so
scaling up our WaveNet
pretty nice with this more General architecture in place we are now set up to push the performance further by
increasing the size of the network so for example I bumped up the number of embeddings to 24 instead of 10 and also
increased number of hidden units but using the exact same architecture we now have 76 000 parameters and the training
takes a lot longer but we do get a nice curve and then when you actually evaluate the performance we are now
getting validation performance of 1.993 so we've crossed over the 2.0 sort of
territory and right about 1.99 but we are starting to have to wait quite a bit
longer and we're a little bit in the dark with respect to the correct setting of the hyper parameters here and the
learning rates and so on because the experiments are starting to take longer to train and so we are missing sort of like an experimental harness on which we
could run a number of experiments and really tune this architecture very well so I'd like to conclude now with a few
experimental harness
notes we basically improved our performance from a starting of 2.1 down to 1.9 but I don't want that to be the
focus because honestly we're kind of in the dark we have no experimental harness we're just guessing and checking and
this whole thing is terrible we're just looking at the training loss normally you want to look at both the training and the validation loss together and the
whole thing looks different if you're actually trying to squeeze out numbers that said we did implement this
architecture from the wavenet paper but we did not implement this specific uh
forward pass of it where you have a more complicated a linear layer sort of that is this gated linear layer kind of and
there's residual connections and Skip connections and so on so we did not Implement that we just implemented this
WaveNet but with “dilated causal convolutions”
structure I would like to briefly hint or preview how what we've done here relates to convolutional neural networks
as used in the wavenet paper and basically the use of convolutions is strictly for efficiency it doesn't
actually change the model we've implemented so here for example let me look at a specific name to work
with an example so there's a name in our training set and it's DeAndre and it has
seven letters so that is eight independent examples in our model so all these rows here are independent examples
of the Android now you can forward of course any one of these rows independently so I can take
my model and call call it on any individual index notice by the way here
I'm being a little bit tricky the reason for this is that extra at seven that shape is just
um one dimensional array of eight so you can't actually call the model on it you're going to get an error because
there's no batch dimension so when you do extra at
a list of seven then the shape of this becomes one by eight so I get an extra batch dimension of one and then we can
forward the model so that forwards a single example and you
might imagine that you actually may want to forward all of these eight um at the same time
so pre-allocating some memory and then doing a for Loop eight times and forwarding all of those eight here will
give us all the logits in all these different cases now for us with the model as we've implemented it right now this is eight
independent calls to our model but what convolutions allow you to do is it allow you to basically slide this
model efficiently over the input sequence and so this for Loop can be
done not outside in Python but inside of kernels in Cuda and so this for Loop
gets hidden into the convolution so the convolution basically you can cover this it's a for Loop applying a
little linear filter over space of some input sequence and in our case the space
we're interested in is one dimensional and we're interested in sliding these filters over the input data
so this diagram actually is fairly good as well basically what we've done is here they
are highlighting in Black one individ one single sort of like tree of this calculation so just calculating the
single output example here um and so this is basically what we've
implemented here we've implemented a single this black structure we've implemented that and calculated a single
output like a single example but what collusions allow you to do is it allows you to take this black
structure and kind of like slide it over the input sequence here and calculate
all of these orange outputs at the same time or here that corresponds to
calculating all of these outputs of um at all the positions of DeAndre at
the same time and the reason that this is much more efficient is because number one as I
mentioned the for Loop is inside the Cuda kernels in the sliding so that
makes it efficient but number two notice the variable reuse here for example if we look at this circle this node here
this node here is the right child of this node but is also the left child of
the node here and so basically this node and its value is used twice
and so right now in this naive way we'd have to recalculate it but here we are
allowed to reuse it so in the convolutional neural network you think of these linear layers that we
have up above as filters and we take these filters and they're linear filters
and you slide them over input sequence and we calculate the first layer and then the second layer and then the third
layer and then the output layer of the sandwich and it's all done very efficiently using these convolutions
so we're going to cover that in a future video the second thing I hope you took away from this video is you've seen me
torch.nn
basically Implement all of these layer Lego building blocks or module building
blocks and I'm implementing them over here and we've implemented a number of layers together and we've also
implemented these these containers and we've overall pytorchified our code
quite a bit more now basically what we're doing here is we're re-implementing torch.nn which is
the neural networks library on top of torch.tensor and it looks very much like
this except it is much better because because it's in pi torch instead of jingling my Jupyter notebook so I think
going forward I will probably have considered us having unlocked um torch.nn we understand roughly what's
in there how these modules work how they're nested and what they're doing on top of torture tensor so hopefully we'll
just uh we'll just switch over and continue and start using torch.net directly the next thing I hope you got a
the development process of building deep neural nets
bit of a sense of is what the development process of building deep neural networks looks like which I think
was relatively representative to some extent so number one we are spending a lot of time in the documentation page of
pytorch and we're reading through all the layers looking at documentations where the shapes of the inputs what can
they be what does the layer do and so on unfortunately I have to say the patreon's documentation is not are very
good they spend a ton of time on Hardcore engineering of all kinds of distributed Primitives Etc but as far as
I can tell no one is maintaining any documentation it will lie to you it will be wrong it will be incomplete it will
be unclear so unfortunately it is what it is and you just kind of do your best
um with what they've given us um number two
uh the other thing that I hope you got a sense of is there's a ton of trying to make the shapes work and there's a lot
of gymnastics around these multi-dimensional arrays and are they two-dimensional three-dimensional four-dimensional uh what layers take
what shapes is it NCL or NLC and you're promoting and viewing and it just can
get pretty messy and so that brings me to number three I very often prototype these layers and implementations in
jupyter notebooks and make sure that all the shapes work out and I'm spending a lot of time basically babysitting the
shapes and making sure everything is correct and then once I'm satisfied with the functionality in the Jupyter notebook I will take that code and copy
paste it into my repository of actual code that I'm training with and so then
I'm working with vs code on the side so I usually have jupyter notebook and vs code I develop in Jupyter notebook I
paste into vs code and then I kick off experiments from from the reaper of course from the code repository so
that's roughly some notes on the development process of working with neurons lastly I think this lecture unlocks a lot of potential further
going forward
lectures because number one we have to convert our neural network to actually use these dilated causal convolutional
layers so implementing the comnet number two potentially starting to get into
what this means whatever residual connections and Skip connections and why are they useful
number three we as I mentioned we don't have any experimental harness so right now I'm just guessing checking
everything this is not representative of typical deep learning workflows you have to set up your evaluation harness you
can kick off experiments you have lots of arguments that your script can take you're you're kicking off a lot of experimentation you're looking at a lot
of plots of training and validation losses and you're looking at what is working and what is not working and you're working on this like population
level and you're doing all these hyper parameter searches and so we've done none of that so far so how to set that
up and how to make it good I think as a whole another topic number three we
should probably cover recurring neural networks RNs lstm's grooves and of course Transformers so many uh places to
go and we'll cover that in the future for now bye sorry I forgot to say that
improve on my loss! how far can we improve a WaveNet on this data?
if you are interested I think it is kind of interesting to try to beat this number 1.993 because I really haven't
tried a lot of experimentation here and there's quite a bit of fruit potentially to still purchase further so I haven't
tried any other ways of allocating these channels in this neural net maybe the number of dimensions for the embedding
is all wrong maybe it's possible to actually take the original network with just one hidden layer and make it big
enough and actually beat my fancy hierarchical Network it's not obvious that would be kind of embarrassing if
this did not do better even once you torture it a little bit maybe you can read the weight net paper and try to
figure out how some of these layers work and Implement them yourselves using what we have and of course you can always tune some
of the initialization or some of the optimization and see if you can improve it that way so I'd be curious if people
can come up with some ways to beat this and yeah that's it for now bye




intro: ChatGPT, Transformers, nanoGPT, Shakespeare
hi everyone so by now you have probably heard of chat GPT it has taken the world and AI Community by storm and it is a
system that allows you to interact with an AI and give it text based tasks so
for example we can ask chat GPT to write us a small Hau about how important it is that people understand Ai and then they
can use it to improve the world and make it more prosperous so when we run this AI knowledge brings prosperity for all
to see Embrace its power okay not bad and so you could see that chpt went from left to right and
generated all these words SE sort of sequentially now I asked it already the
exact same prompt a little bit earlier and it generated a slightly different outcome ai's power to grow ignorance
holds us back learn Prosperity weights so uh pretty good in both cases and
slightly different so you can see that chat GPT is a probabilistic system and for any one prompt it can give us
multiple answers sort of uh replying to it now this is just one example of a
problem people have come up with many many examples and there are entire websites that index interactions with
chpt and so many of them are quite humorous explain HTML to me like I'm a dog uh write release notes for chess 2
write a note about Elon Musk buying a Twitter and so on so as an example uh
please write a breaking news article about a leaf falling from a tree uh and a shocking turn of events a
leaf has fallen from a tree in the local park Witnesses report that the leaf which was previously attached to a branch of a tree attached itself and
fell to the ground very dramatic so you can see that this is a pretty remarkable system and it is what we call a language
model uh because it um it models the sequence of words or characters or
tokens more generally and it knows how sort of words follow each other in English language and so from its
perspective what it is doing is it is completing the sequence so I give it the start of a sequence and it completes the
sequence with the outcome and so it's a language model in that sense now I would
like to focus on the under the hood of um under the hood components of what makes CH GPT work so what is the neural
network under the hood that models the sequence of these words and that comes
from this paper called attention is all you need in 2017 a landmark paper a
landmark paper in AI that produced and proposed the Transformer architecture so GPT is uh short for
generally generatively pre-trained Transformer so Transformer is the neuron nut that actually does all the heavy
lifting under the hood it comes from this paper in 2017 now if you read this paper this uh reads like a pretty random
machine translation paper and that's because I think the authors didn't fully anticipate the impact that the Transformer would have on the field and
this architecture that they produced in the context of machine translation in their case actually ended up taking over
uh the rest of AI in the next 5 years after and so this architecture with
minor changes was copy pasted into a huge amount of applications in AI in more recent years and that includes at
the core of chat GPT now we are not going to what I'd like to do now is I'd
like to build out something like chat GPT but uh we're not going to be able to of course reproduce chat GPT this is a
very serious production grade system it is trained on uh a good chunk of
internet and then there's a lot of uh pre-training and fine-tuning stages to it and so it's very complicated what I'd
like to focus on is just to train a Transformer based language model and in
our case it's going to be a character level language model I still think that is uh very educational with respect to
how these systems work so I don't want to train on the chunk of Internet we need a smaller data set in this case I
propose that we work with uh my favorite toy data set it's called tiny Shakespeare and um what it is is
basically it's a concatenation of all of the works of sh Shakespeare in my understanding and so this is all of
Shakespeare in a single file uh this file is about 1 megab and it's just all
of Shakespeare and what we are going to do now is we're going to basically model how these characters uh follow each
other so for example given a chunk of these characters like this uh given some
context of characters in the past the Transformer neural network will look at the characters that I've highlighted and
is going to predict that g is likely to come next in the sequence and it's going to do that because we're going to train
that Transformer on Shakespeare and it's just going to try to produce uh character sequences that look like this
and in that process is going to model all the patterns inside this data so once we've trained the system i' just
like to give you a preview we can generate infinite Shakespeare and of course it's a fake thing that looks kind
of like Shakespeare um apologies for there's some Jank that
I'm not able to resolve in in here but um you can see how this is going
character by character and it's kind of like predicting Shakespeare like language so verily my Lord the sites
have left the again the king coming with my curses with precious pale and then
tranos say something else Etc and this is just coming out of the Transformer in a very similar manner as it would come
out in chat GPT in our case character by character in chat GPT uh it's coming out
on the token by token level and tokens are these sort of like little subword pieces so they're not Word level they're
kind of like word chunk level um and now I've already written
this entire code uh to train these Transformers um and it is in a GitHub
repository that you can find and it's called nanog GPT so nanog GPT is a repository that
you can find in my GitHub and it's a repository for training Transformers um on any given text and what I think is
interesting about it because there's many ways to train Transformers but this is a very simple implementation so it's just two files of 300 lines of code each
one file defines the GPT model the Transformer and one file trains it on some given Text data set and here I'm
showing that if you train it on a open web Text data set which is a fairly large data set of web pages then I
reproduce the the performance of gpt2 so gpt2 is an early version of open
AI GPT uh from 2017 if I recall correctly and I've only so far
reproduced the the smallest 124 million parameter model uh but basically this is just proving that the codebase is
correctly arranged and I'm able to load the uh neural network weights that openi
has released later so you can take a look at the finished code here in N GPT
but what I would like to do in this lecture is I would like to basically uh write this repository from scratch so
we're going to begin with an empty file and we're we're going to define a Transformer piece by piece we're going
to train it on the tiny Shakespeare data set and we'll see how we can then uh generate infinite Shakespeare and of
course this can copy paste to any arbitrary Text data set uh that you like uh but my goal really here is to just
make you understand and appreciate uh how under the hood chat GPT works and um
really all that's required is a Proficiency in Python and uh some basic understanding of um calculus and
statistics and it would help if you also see my previous videos on the same YouTube channel in particular my make more
series where I um Define smaller and simpler neural network language models
uh so multi perceptrons and so on it really introduces the language modeling framework and then uh here in this video
we're going to focus on the Transformer neural network itself okay so I created a new Google collab uh jup notebook here
reading and exploring the data
and this will allow me to later easily share this code that we're going to develop together uh with you so you can follow along so this will be in a video
description uh later now here I've just done some preliminaries I downloaded the
data set the tiny Shakespeare data set at this URL and you can see that it's about a 1 Megabyte file then here I open
the input.txt file and just read in all the text of the string and we see that we are working with 1 million characters
roughly and the first 1,000 characters if we just print them out are basically what you would expect this is the first
1,000 characters of the tiny Shakespeare data set roughly up to here so so far so
good next we're going to take this text and the text is a sequence of characters in Python so when I call the set
Constructor on it I'm just going to get the set of all the characters that occur
in this text and then I call list on that to create a list of those characters instead of just a set so that
I have an ordering an arbitrary ordering and then I sort that so basically we get
just all the characters that occur in the entire data set and they're sorted now the number of them is going to be
our vocabulary size these are the possible elements of our sequences and we see that when I print here the
characters there's 65 of them in total there's a space character and then all kinds of special characters and then U
capitals and lowercase letters so that's our vocabulary and that's the sort of like possible uh characters that the
model can see or emit okay so next we will would like to develop some strategy
tokenization, train/val split
to tokenize the input text now when people say tokenize they mean convert
the raw text as a string to some sequence of integers According to some uh notebook According to some vocabulary
of possible elements so as an example here we are going to be building a character level language model so we're
simply going to be translating individual characters into integers so let me show you uh a chunk of code that
sort of does that for us so we're building both the encoder and the decoder and let me just talk through what's
happening here when we encode an arbitrary text like hi there we're going to receive a
list of integers that represents that string so for example 46 47 Etc and then
we also have the reverse mapping so we can take this list and decode it to get
back the exact same string so it's really just like a translation to integers and back for arbitrary string
and for us it is done on a character level now the way this was achieved is we just
iterate over all the characters here and create a lookup table from the character to the integer and vice versa and then
to encode some string we simply translate all the characters individually and to decode it back we
use the reverse mapping and concatenate all of it now this is only one of many possible encodings or many possible sort
of tokenizers and it's a very simple one but there's many other schemas that people have come up with in practice so
for example Google uses a sentence piece uh so sentence piece will also encode text into um integers but in a
different schema and using a different vocabulary and sentence piece is a
subword uh sort of tokenizer and what that means is that um you're not encoding entire words but you're not
also encoding individual characters it's it's a subword unit level and that's
usually what's adopted in practice for example also openai has this Library called tick token that uses a bite pair
encode tokenizer um and that's what GPT uses and you can also just encode words into
like hell world into a list of integers so as an example I'm using the Tik token
Library here I'm getting the encoding for gpt2 or that was used for gpt2
instead of just having 65 possible characters or tokens they have 50,000
tokens and so when they encode the exact same string High there we only get a
list of three integers but those integers are not between 0 and 64 they are between Z and 5,
5,256 so basically you can trade off the code book size and the sequence lengths
so you can have very long sequences of integers with very small vocabularies or we can have short um sequences of
integers with very large vocabularies and so typically people use in practice
these subword encodings but I'd like to keep our token ier very simple so we're using character level tokenizer and that
means that we have very small code books we have very simple encode and decode functions uh but we do get very long
sequences as a result but that's the level at which we're going to stick with this lecture because it's the simplest
thing okay so now that we have an encoder and a decoder effectively a tokenizer we can tokenize the entire
training set of Shakespeare so here's a chunk of code that does that and I'm going to start to use the pytorch
library and specifically the torch. tensor from the pytorch library so we're going to take all of the text in tiny
Shakespeare encode it and then wrap it into a torch. tensor to get the data tensor so here's what the data tensor
looks like when I look at just the first 1,000 characters or the 1,000 elements of it so we see that we have a massive
sequence of integers and this sequence of integers here is basically an identical translation of the first
10,000 characters here so I believe for example that zero is a new line character and maybe one
one is a space not 100% sure but from now on the entire data set of text is re-represented as just it's just
stretched out as a single very large uh sequence of integers let me do one more thing before
we move on here I'd like to separate out our data set into a train and a validation split so in particular we're
going to take the first 90% of the data set and consider that to be the training data for the Transformer and we're going
to withhold the last 10% at the end of it to be the validation data and this will help us understand to what extent
our model is overfitting so we're going to basically hide and keep the validation data on the side because we
don't want just a perfect memorization of this exact Shakespeare we want a neural network that sort of creates
Shakespeare like uh text and so it should be fairly likely for it to produce the actual like stowed away uh
true Shakespeare text um and so we're going to use this to uh get a sense of
the overfitting okay so now we would like to start plugging these text sequences or integer sequences into the
data loader: batches of chunks of data
Transformer so that it can train and learn those patterns now the important thing to realize is we're never going to
actually feed entire text into a Transformer all at once that would be computationally very expensive and
prohibitive so when we actually train a Transformer on a lot of these data sets we only work with chunks of the data set
and when we train the Transformer we basically sample random little chunks out of the training set and train on
just chunks at a time and these chunks have basically some kind of a length and
some maximum length now the maximum length typically at least in the code I usually write is called block size you
can you can uh find it under different names like context length or something like that let's start with the block
size of just eight and let me look at the first train data characters the first block size plus one characters
I'll explain why plus one in a second so this is the first nine characters in the sequence in the
training set now what I'd like to point out is that when you sample a chunk of data like this so say the these nine
characters out of the training set this actually has multiple examples packed into it and uh that's because all of
these characters follow each other and so what this thing is going to say when
we plug it into a Transformer is we're going to actually simultaneously train it to make prediction at every one of
these positions now in the in a chunk of nine characters there's actually eight indiv
ual examples packed in there so there's the example that when 18 when in the
context of 18 47 likely comes next in a context of 18 and 47 56 comes next in a
context of 18 47 56 57 can come next and so on so that's the eight individual
examples let me actually spell it out with code so here's a chunk of code to
illustrate X are the inputs to the Transformer it will just be the first block size characters y will be the uh
next block size characters so it's offset by one and that's because y are the targets for each position in the
input and then here I'm iterating over all the block size of eight and the context is always all the characters in
x uh up to T and including T and the target is always the teth character but
in the targets array y so let me just run this and basically it spells out what I
said in words uh these are the eight examples hidden in a chunk of nine characters that we uh sampled from the
training set I want to mention one more thing we train on all the eight examples
here with context between one all the way up to context of block size and we train on that not just for computational
reasons because we happen to have the sequence already or something like that it's not just done for efficiency it's
also done um to make the Transformer Network be used to seeing contexts all
the way from as little as one all the way to block size and we'd like the transform to be used to seeing
everything in between and that's going to be useful later during inference because while we're sampling we can
start the sampling generation with as little as one character of context and the Transformer knows how to predict the
next character with all the way up to just context of one and so then it can predict everything up to block size and
after block size we have to start truncating because the Transformer will will never um receive more than block
size inputs when it's predicting the next character Okay so we've looked at the time dimension of the tensors that are
going to be feeding into the Transformer there's one more Dimension to care about and that is the batch Dimension and so
as we're sampling these chunks of text we're going to be actually every time we're going to feed them into a
Transformer we're going to have many batches of multiple chunks of text that are all like stacked up in a single
tensor and that's just done for efficiency just so that we can keep the gpus busy uh because they are very good
at parallel processing of um of data and so we just want to process multiple
chunks all at the same time but those chunks are processed completely independently they don't talk to each other and so on so let me basically just
generalize this and introduce a batch Dimension here's a chunk of code let me just run it and then I'm
going to explain what it does so here because we're going to start sampling random locations in the
data set to pull chunks from I am setting the seed so that um in the
random number generator so that the numbers I see here are going to be the same numbers you see later if you try to reproduce this now the batch size here
is how many independent sequences we are processing every forward backward pass of the
Transformer the block size as I explained is the maximum context length to make those predictions so let's say B
size four block size eight and then here's how we get batch for any arbitrary split if the split is a
training split then we're going to look at train data otherwise at valid data that gives us the data array and then
when I Generate random positions to grab a chunk out of I actually grab I
actually generate batch size number of Random offsets so because this is four
we are ex is going to be a uh four numbers that are randomly generated between zero and Len of data minus block
size so it's just random offsets into the training set and then X's as I explained are the
first first block size characters starting at I the Y's are the offset by
one of that so just add plus one and then we're going to get those chunks for
every one of integers I INX and use a torch. stack to take all those uh uh
one-dimensional tensors as we saw here and we're going to um stack them up at
rows and so they all become a row in a 4x8 tensor
so here's where I'm printing then when I sample a batch XB and YB the inputs to
the Transformer now are the input X is the 4x8 tensor four uh rows of eight
columns and each one of these is a chunk of the training set and then the targets here are in the
associated array Y and they will come in to the Transformer all the way at the end uh to um create the loss function
uh so they will give us the correct answer for every single position inside X and then these are the four
independent rows so spelled out as we did before uh this 4x8 array contains a
total of 32 examples and they're completely independent as far as the Transformer is
concerned uh so when the input is 24 the target is 43 or rather 43 here in the Y
array when the input is 2443 the target is 58 uh when the input is 24 43 58 the
target is 5 Etc or like when it is a 52 581 the target is
58 right so you can sort of see this spelled out these are the 32 independent examples packed in to a single batch of
the input X and then the desired targets are in y and so now this integer tensor
of um X is going to feed into the Transformer and that Transformer is
going to simultaneously process all these examples and then look up the correct um integers to predict in every
one of these positions in the tensor y okay so now that we have our batch of input that we'd like to feed into a
simplest baseline: bigram language model, loss, generation
Transformer let's start basically feeding this into neural networks now we're going to start off with the
simplest possible neural network which in the case of language modeling in my opinion is the Byram language model and
we've covered the Byram language model in my make more series in a lot of depth and so here I'm going to sort of go
faster and let's just Implement pytorch module directly that implements the byr language
model so I'm importing the pytorch um NN module uh for
reproducibility and then here I'm constructing a Byram language model which is a subass of NN
module and then I'm calling it and I'm passing it the inputs and the targets
and I'm just printing now when the inputs on targets come here you see that I'm just taking the index uh the inputs
X here which I rename to idx and I'm just passing them into this token embedding table so it's going on here is
that here in the Constructor we are creating a token embedding table and it
is of size vocap size by vocap size and we're using an. embedding which
is a very thin wrapper around basically a tensor of shape voap size by vocab size and what's happening here is that
when we pass idx here every single integer in our input is going to refer to this embedding table and it's going
to pluck out a row of that embedding table corresponding to its index so 24
here will go into the embedding table and we'll pluck out the 24th row and then 43 will go here and pluck out the
43d row Etc and then pytorch is going to arrange all of this into a batch by Time
by channel uh tensor in this case batch is four time is eight and C which is the
channels is vocab size or 65 and so we're just going to pluck out all those rows arrange them in a b by T by C and
now we're going to interpret this as the logits which are basically the scores for the next character in the sequence
and so what's happening here is we are predicting what comes next based on just the individual identity of a single
token and you can do that because um I mean currently the tokens are not talking to each other and they're not
seeing any context except for they're just seeing themselves so I'm a f I'm a token number five and then I can
actually make pretty decent predictions about what comes next just by knowing that I'm token five because some characters uh know um C follow other
characters in in typical scenarios so we saw a lot of this in a lot more depth in the make more series and here if I just
run this then we currently get the predictions the scores the lits for
every one of the 4x8 positions now that we've made predictions about what comes next we'd like to evaluate the loss
function and so in make more series we saw that a good way to measure a loss or like a quality of the predictions is to
use the negative log likelihood loss which is also implemented in pytorch under the name cross entropy so what we'
like to do here is loss is the cross entropy on the predictions and the
targets and so this measures the quality of the logits with respect to the Targets in other words we have the
identity of the next character so how well are we predicting the next character based on the lits and
intuitively the correct um the correct dimension of low jits uh depending on
whatever the target is should have a very high number and all the other dimensions should be very low number
right now the issue is that this won't actually this is what we want we want to basically output the logits and the
loss this is what we want but unfortunately uh this won't actually run
we get an error message but intuitively we want to uh measure this now when we
go to the pytorch um cross entropy documentation here um we're trying to
call the cross entropy in its functional form uh so that means we don't have to create like a module for it but here
when we go to the documentation you have to look into the details of how pitor expects these inputs and basically the
issue here is ptor expects if you have multi-dimensional input which we do because we have a b BYT by C tensor then
it actually really wants the channels to be the second uh Dimension here so if
you um so basically it wants a b by C BYT instead of a b by T by C and so it's
just the details of how P torch treats um these kinds of inputs and so we don't
actually want to deal with that so what we're going to do instead is we need to basically reshape our logits so here's
what I like to do I like to take basically give names to the dimensions so lit. shape is B BYT by C and unpack
those numbers and then let's uh say that logits equals lit. View and we want it
to be a b * c b * T by C so just a two- dimensional
array right so we're going to take all the we're going to take all of these um
positions here and we're going to uh stretch them out in a onedimensional sequence and uh preserve the channel
Dimension as the second dimension so we're just kind of like stretching out the array so it's two- dimensional and in that case it's going
to better conform to what pytorch uh sort of expects in its Dimensions now we
have to do the same to targets because currently targets are um of shape B by T
and we want it to be just B * T so onedimensional now alternatively you
could always still just do minus one because pytor will guess what this should be if you want to lay it out uh
but let me just be explicit and say p * t once we've reshaped this it will match the cross entropy case and then we
should be able to evaluate our loss okay so that R now and we can do
loss and So currently we see that the loss is 4.87 now because our uh we have 65
possible vocabulary elements we can actually guess at what the loss should be and in
particular we covered negative log likelihood in a lot of detail we are expecting log or lawn of um 1 over 65
and negative of that so we're expecting the loss to be about 4.1 17 but we're
getting 4.87 and so that's telling us that the initial predictions are not uh super diffuse they've got a little bit
of entropy and so we're guessing wrong uh so uh yes but actually we're I a we
are able to evaluate the loss okay so now that we can evaluate the quality of the model on some data we'd like to also
be able to generate from the model so let's do the generation now I'm going to go again a little bit faster here
because I covered all this already in previous videos so here's a generate function for the
model so we take some uh we take the the same kind of input idx here and
basically this is the current uh context of some characters in a batch in some
batch so it's also B BYT and the job of generate is to basically take this B BYT
and extend it to be B BYT + 1 plus 2 plus 3 and so it's just basically it continues the generation in all the
batch dimensions in the time Dimension So that's its job and it will do that for Max new tokens so you can see here
on the bottom there's going to be some stuff here but on the bottom whatever is predicted is concatenated on top of the
previous idx along the First Dimension which is the time Dimension to create a b BYT + one so that becomes a new idx so
the job of generate is to take a b BYT and make it a b BYT plus 1 plus 2 plus three as many as we want Max new tokens
so this is the generation from the model now inside the generation what what are we doing we're taking the current
indices we're getting the predictions so we get uh those are in the low jits and
then the loss here is going to be ignored because um we're not we're not using that and we have no targets that
are sort of ground truth targets that we're going to be comparing with then once we get the logits we are only
focusing on the last step so instead of a b by T by C we're going to pluck out
the negative-1 the last element in the time Dimension because those are the predictions for what comes next so that
gives us the logits which we then convert to probabilities via softmax and then we use tor. multinomial to sample
from those probabilities and we ask pytorch to give us one sample and so idx
next will become a b by one because in each uh one of the batch Dimensions
we're going to have a single prediction for what comes next so this num samples equals one will make this be a
one and then we're going to take those integers that come from the sampling process according to the probability
distribution given here and those integers got just concatenated on top of the current sort of like running stream
of integers and this gives us a b BYT + one and then we can return that now one
thing here is you see how I'm calling self of idx which will end up going to
the forward function I'm not providing any Targets So currently this would give an error because targets is uh is uh
sort of like not given so targets has to be optional so targets is none by default and then if targets is none then
there's no loss to create so it's just loss is none but else all of this
happens and we can create a loss so this will make it so um if we have the
targets we provide them and get a loss if we have no targets it will'll just get the loits so this here will generate from
the model um and let's take that for a ride
now oops so I have another code chunk here which will generate for the model
from the model and okay this is kind of crazy so maybe let me let me break this down so these are the idx
right I'm creating a batch will be just one time will be just one so I'm
creating a little one by one tensor and it's holding a zero and the D type the
data type is uh integer so zero is going to be how we kick off the generation and
remember that zero is uh is the element standing for a new line character so
it's kind of like a reasonable thing to to feed in as the very first character in a sequence to be the new
line um so it's going to be idx which we're going to feed in here then we're going to ask for 100 tokens
and then. generate will continue that now because uh generate works on the
level of batches we we then have to index into the zero throw to basically unplug the um the single batch Dimension
that exists and then that gives us a um
time steps just a onedimensional array of all the indices which we will convert to simple python list from pytorch
tensor so that that can feed into our decode function and uh convert those
integers into text so let me bring this back and we're generating 100 tokens let's
run and uh here's the generation that we achieved so obviously it's garbage and
the reason it's garbage is because this is a totally random model so next up we're going to want to train this model
now one more thing I wanted to point out here is this function is written to be General but it's kind of like ridiculous
right now because we're feeding in all this we're building out this context and we're concatenating
it all and we're always feeding it all into the model but that's kind of
ridiculous because this is just a simple Byram model so to make for example this prediction about K we only needed this W
but actually what we fed into the model is we fed the entire sequence and then we only looked at the very last piece
and predicted K so the only reason I'm writing it in this way is because right now this is a byr model but I'd like to
keep keep this function fixed and I'd like it to work um later when our
characters actually um basically look further in the history and so right now
the history is not used so this looks silly uh but eventually the history will be used and so that's why we want to uh
do it this way so just a quick comment on that so now we see that this is um random so let's train the model so it
becomes a bit less random okay let's Now train the model so first what I'm going to do is I'm going to create a pyour
training the bigram model
optimization object so here we are using the optimizer ATM W um now in a make
more series we've only ever use tastic gradi in descent the simplest possible Optimizer which you can get using the
SGD instead but I want to use Adam which is a much more advanced and popular Optimizer and it works extremely well
for uh typical good setting for the learning rate is roughly 3 E4 uh but for
very very small networks like is the case here you can get away with much much higher learning rates R3 or even
higher probably but let me create the optimizer object which will basically take the gradients and uh update the
parameters using the gradients and then here our batch size
up above was only four so let me actually use something bigger let's say 32 and then for some number of steps um
we are sampling a new batch of data we're evaluating the loss uh we're zeroing out all the gradients from the
previous step getting the gradients for all the parameters and then using those gradients to up update our parameters so
typical training loop as we saw in the make more series so let me now uh run
this for say 100 iterations and let's see what kind of losses we're going to
get so we started around 4.7 and now we're getting to down to like 4.6 4.5 Etc so the optimization is
definitely happening but um let's uh sort of try to increase number of
iterations and only print at the end because we probably want train for
longer okay so we're down to 3.6
roughly roughly down to
three this is the most janky
optimization okay it's working let's just do 10,000 and then from here we want to
copy this and hopefully that we're going to get something reason and of course it's not going to be Shakespeare from a
byr model but at least we see that the loss is improving and uh hopefully we're
expecting something a bit more reasonable okay so we're down at about 2.5 is let's see what we get okay
dramatic improvements certainly on what we had here so let me just increase the number of tokens okay so we see that
we're starting to get something at least like reasonable is
um certainly not shakes spear but uh the model is making progress so that is the
simplest possible model so now what I'd like to do
is obviously this is a very simple model because the tokens are not talking to each other so given the previous context
of whatever was generated we're only looking at the very last character to make the predictions about what comes next so now these uh now these tokens
have to start talking to each other and figuring out what is in the context so that they can make better predictions
for what comes next and this is how we're going to kick off the uh Transformer okay so next I took the code
port our code to a script
that we developed in this juper notebook and I converted it to be a script and I'm doing this because I just want to
simplify our intermediate work into just the final product that we have at this point so in the top here I put all the
hyp parameters that we to find I introduced a few and I'm going to speak to that in a little bit otherwise a lot
of this should be recognizable uh reproducibility read data get the encoder and the decoder create the train
into splits uh use the uh kind of like data loader um that gets a batch of the
inputs and Targets this is new and I'll talk about it in a second now this is
the Byram language model that we developed and it can forward and give us a logits and loss and it can
generate and then here we are creating the optimizer and this is the training
Loop so everything here should look pretty familiar now some of the small things that I added number one I added
the ability to run on a GPU if you have it so if you have a GPU then you can this will use Cuda instead of just CPU
and everything will be a lot more faster now when device becomes Cuda then we need to make sure that when we load the
data we move it to device when we create the model we want to move uh the model parameters to
device so as an example here we have the N an embedding table and it's got a
weight inside it which stores the uh sort of lookup table so so that would be moved to the GPU so that all the
calculations here happen on the GPU and they can be a lot faster and then finally here when I'm creating the
context that feeds in to generate I have to make sure that I create it on the device number two what I introduced is
uh the fact that here in the training Loop here I was just printing the um l.
item inside the training Loop but this is a very noisy measurement of the current loss because every batch will be
more or less lucky and so what I want to do usually um is uh I have an estimate
loss function and the estimate loss basically then um goes up here and it
averages up the loss over multiple batches so in particular we're going to iterate eval iter times and we're going
to basically get our loss and then we're going to get the average loss for both splits and so this will be a lot less
noisy so here when we call the estimate loss we're we're going to report the uh pretty accurate train and validation
loss now when we come back up you'll notice a few things here I'm setting the model to evaluation phase and down here
I'm resetting it back to training phase now right now for our model as is this doesn't actually do anything because the
only thing inside this model is this uh nn. embedding and um this this um
Network would behave both would behave the same in both evaluation mode and training mode we have no drop off layers
we have no batm layers Etc but it is a good practice to Think Through what mode your neural network is in because some
layers will have different Behavior Uh at inference time or training time and
there's also this context manager torch up nograd and this is just telling pytorch that everything that happens
inside this function we will not call do backward on and so pytorch can be a lot
more efficient with its memory use because it doesn't have to store all the intermediate variables uh because we're
never going to call backward and so it can it can be a lot more memory efficient in that way so also a good
practice to tpy torch when we don't intend to do back propagation so right now this script is
about 120 lines of code of and that's kind of our starter code I'm calling it
b.p and I'm going to release it later now running this script gives us output in the terminal
and it looks something like this it basically as I ran this code uh it was
giving me the train loss and Val loss and we see that we convert to somewhere around 2.5 with the pyr model and then here's
the sample that we produced at the end and so we have everything packaged up in the script and we're in a good
position now to iterate on this okay so we are almost ready to start writing our very first self attention block for
version 1: averaging past context with for loops, the weakest form of aggregation
processing these uh tokens now before we actually get there I want to get you
used to a mathematical trick that is used in the self attention inside a Transformer and is really just like at
the heart of an an efficient implementation of self attention and so I want to work with this toy example to
just get you used to this operation and then it's going to make it much more clear once we actually get to um to it
uh in the script again so let's create a b BYT by C where BT and C are just 48 and two in the toy
example and these are basically channels and we have uh batches and we have the
time component and we have information at each point in the sequence so
see now what we would like to do is we would like these um tokens so we have up
to eight tokens here in a batch and these eight tokens are currently not talking to each other and we would like
them to talk to each other we'd like to couple them and in particular we don't
we we want to couple them in a very specific way so the token for example at the fifth location it should not
communicate with tokens in the sixth seventh and eighth location because uh those are future tokens in
the sequence the token on the fifth location should only talk to the one in the fourth third second and first so
it's only so information only flows from previous context to the current time step and we cannot get any information
from the future because we are about to try to predict the future so what is the easiest way for
tokens to communicate okay the easiest way I would say is okay if we're up to
if we're a fifth token and I'd like to communicate with my past the simplest way we can do that is to just do a
weight is to just do an average of all the um of all the preceding elements so
for example if I'm the fif token I would like to take the channels uh that make up that are information at my step but
then also the channels from the fourth step third step second step and the first step I'd like to average those up
and then that would become sort of like a feature Vector that summarizes me in the context of my history now of course
just doing a sum or like an average is an extremely weak form of interaction like this communication is uh extremely
lossy we've lost a ton of information about the spatial Arrangements of all those tokens uh but that's okay for now
we'll see how we can bring that information back later for now what we would like to do is for every single
batch element independently for every teeth token in that sequence we'd like
to now calculate the average of all the vectors in all the previous tokens and
also at this token so let's write that out um I have a small snippet here and
instead of just fumbling around let me just copy paste it and talk to it so in other words we're going to
create X and B is short for bag of words because bag of words is um is kind of
like um a term that people use when you are just averaging up things so this is just a bag of words basically there's a
word stored on every one of these eight locations and we're doing a bag of words we're just averaging
so in the beginning we're going to say that it's just initialized at Zero and then I'm doing a for Loop here so we're not being efficient yet that's coming
but for now we're just iterating over all the batch Dimensions independently iterating over time and then the
previous uh tokens are at this uh batch Dimension and then everything up to and
including the teeth token okay so when we slice out X in this way X prev
Becomes of shape um how many T elements there were in the past and then of
course C so all the two-dimensional information from these little tokens so
that's the previous uh sort of chunk of um tokens from my current sequence and
then I'm just doing the average or the mean over the zero Dimension so I'm averaging out the time here and I'm just
going to get a little c one dimensional Vector which I'm going to store in X bag of words so I can run this and and uh
this is not going to be very informative because let's see so this is X of Zer so
this is the zeroth batch element and then expo at zero now you see how the at
the first location here you see that the two are equal and that's because it's we're just doing an average of this one
token but here this one is now an average of these two and now this one is
an average of these three and so on so uh and this last one is the average
of all of these elements so vertical average just averaging up all the tokens now gives this outcome
here so this is all well and good uh but this is very inefficient now the trick
the trick in self-attention: matrix multiply as weighted aggregation
is that we can be very very efficient about doing this using matrix multiplication so that's the
mathematical trick and let me show you what I mean let's work with the toy example here let me run it and I'll
explain I have a simple Matrix here that is a 3X3 of all ones a matrix B of just
random numbers and it's a 3x2 and a matrix C which will be 3x3 multip 3x2
which will give out a 3x2 so here we're just using um matrix multiplication so a
multiply B gives us C okay so how are these numbers in C um
achieved right so this number in the top left is the first row of a dot product
with the First Column of B and since all the the row of a right now is all just
ones then the do product here with with this column of B is just going to do a sum of these of this column so 2 + 6 + 6
is 14 the element here in the output of C is also the first column here the first
row of a multiplied now with the second column of B so 7 + 4 + 5 is 16 now you
see that there's repeating elements here so this 14 again is because this row is again all ones and it's multiplying the
First Column of B so we get 14 and this one is and so on so this last number
here is the last row do product last column now the trick here is uh the
following this is just a boring number of um it's just a boring array of all
ones but torch has this function called Trail which is short for a
triangular uh something like that and you can wrap it in torch up once and it will just return the lower triangular
portion of this okay so now it will basically zero out
uh these guys here so we just get the lower triangular part well what happens if we do
that so now we'll have a like this and B like this and now what are we getting here in C well what is this number well
this is the first row times the First Column and because this is zeros
uh these elements here are now ignored so we just get a two and then this number here is the first row times the
second column and because these are zeros they get ignored and it's just seven this seven multiplies this one but
look what happened here because this is one and then zeros we what ended up happening is we're just plucking out the
row of this row of B and that's what we got now here we have one 1 Z so here 110
do product with these two columns will now give us 2 + 6 which is 8 and 7 + 4 which is 11 and because this is 111 we
ended up with the addition of all of them and so basically depending on how many ones and zeros we have here we are
basically doing a sum currently of a variable number of these rows and that
gets deposited into C So currently we're doing sums because these are ones but we can also do
average right and you can start to see how we could do average uh of the rows of B uh sort of in an incremental
fashion because we don't have to we can basically normalize these rows so that they sum to one and then we're going to
get an average so if we took a and then we did aals aide torch. sum in the um of a in the um
oneth Dimension and then let's keep them as true so so therefore the broadcasting
will work out so if I rerun this you see now that these rows now sum to one so
this row is one this row is 0. 5.5 Z and here we get 1/3 and now when we do a
multiply B what are we getting here we are just getting the first row first row
here now we are getting the average of the first two rows okay so 2 and six average is four
and four and seven average is 5.5 and on the bottom here we are now getting the average of these three rows
so the average of all of elements of B are now deposited here and so you can
see that by manipulating these uh elements of this multiplying Matrix and
then multiplying it with any given Matrix we can do these averages in this
incremental fashion because we just get um and we can manipulate that based on
the elements of a okay so that's very convenient so let's let's swing back up here and see how we can vectorize this
version 2: using matrix multiply
and make it much more efficient using what we've learned so in particular we are going to produce an
array a but here I'm going to call it we short for weights but this is our
a and this is how much of every row we want to average up and it's going to be
an average because you can see that these rows sum to one so this is our a and then our B in
this example of course is X so what's going to happen here now is
that we are going to have an expo 2 and this Expo 2 is going to be way
multiplying RX so let's think this true way is T BYT
and this is Matrix multiplying in pytorch a b by T by C and it's giving us uh different what
shape so pytorch will come here and it will see that these shapes are not the same so it will create a batch Dimension
here and this is a batched matrix multiply and so it will apply this matrix multiplication in all the batch
elements um in parallel and individually and then for each batch element there
will be a t BYT multiplying T by C exactly as we had
below so this will now create B by T by C and Expo 2 will now become identical
to Expo so we can see that torch. all close of
xbo and xbo 2 should be true now so this kind of like convinces us
that uh these are in fact um the same so xbo and xbo 2 if I just print
them uh okay we're not going to be able to okay we're not going to be able to just stare it down but
um well let me try Expo basically just at the zeroth element and Expo two at the zeroth element so just the first
batch and we should see that this and that should be identical which they are right so what happened here the
trick is we were able to use batched Matrix multiply to do this uh
aggregation really and it's a weighted aggregation and the weights are specified in this um T BYT array and
we're basically doing weighted sums and uh these weighted sums are are U according to uh the weights inside here
they take on sort of this triangular form and so that means that a token at the teth dimension will only get uh sort
of um information from the um tokens perceiving it so that's exactly what we
want and finally I would like to rewrite it in one more way and we're going to see why that's useful so this is the
version 3: adding softmax
third version and it's also identical to the first and second but let me talk through it it uses
softmax so Trill here is this Matrix
lower triangular ones way begins as all
zero okay so if I just print way in the beginning it's all zero then I
used masked fill so what this is doing is we. masked fill it's all zeros and
I'm saying for all the elements where Trill is equal equal Z make them be
negative Infinity so all the elements where Trill is zero will become negative Infinity now so this is what we get and
then the final line here is softmax so if I take a softmax along
every single so dim is negative one so along every single row if I do softmax
what is that going to do well softmax is um is also like a
normalization operation right and so spoiler alert you get the exact same
Matrix let me bring back to softmax and recall that in softmax we're going to exponentiate every single one
of these and then we're going to divide by the sum and so if we exponentiate
every single element here we're going to get a one and here we're going to get uh basically zero 0 z0 Z everywhere else
and then when we normalize we just get one here we're going to get one one and then zeros and then softmax will again
divide and this will give us 5.5 and so on and so this is also the uh the same
way to produce uh this mask now the reason that this is a bit more interesting and the reason we're going
to end up using it in self attention is that these weights here
begin uh with zero and you can think of this as like an interaction strength or like an affinity so basically it's
telling us how much of each uh token from the past do we want to Aggregate
and average up and then this line is saying tokens from
the past cannot communicate by setting them to negative Infinity we're saying that we will not aggregate anything from
those tokens and so basically this then goes through softmax and through the weighted
and this is the aggregation through matrix multiplication and so what this is now is you can think of these as um these
zeros are currently just set by us to be zero but a quick preview is that these
affinities between the tokens are not going to be just constant at zero they're going to be data dependent these
tokens are going to start looking at each other and some tokens will find other tokens more or less interesting
and depending on what their values are they're going to find each other interesting to different amounts and I'm
going to call those affinities I think and then here we are saying the future cannot communicate with the past we're
we're going to clamp them and then when we normalize and sum we're going to aggregate uh sort of their values
depending on how interesting they find each other and so that's the preview for self attention and basically long story
short from this entire section is that you can do weighted aggregations of your past
Elements by having by using matrix multiplication of a lower triangular
fashion and then the elements here in the lower triangular part are telling you how much of each element uh fuses
into this position so we're going to use this trick now to develop the self attention block block so first let's get
minor code cleanup
some quick preliminaries out of the way first the thing I'm kind of bothered by is that you see how we're passing in
vocap size into the Constructor there's no need to do that because vocap size is already defined uh up top as a global
variable so there's no need to pass this stuff around next what I want to do is I don't
want to actually create I want to create like a level of indirection here where we don't directly go to the embedding
for the um logits but instead we go through this intermediate phase because we're going to start making that bigger
so let me introduce a new variable n embed it shorted for number of embedding
Dimensions so nbed here will be say 32 that was a
suggestion from GitHub co-pilot by the way um it also suest 32 which is a good number so this is an embedding table and
only 32 dimensional embeddings so then here this is not going to give us logits directly instead
this is going to give us token embeddings that's I'm going to call it and then to go from the token Tings to
the logits we're going to need a linear layer so self. LM head let's call it
short for language modeling head is n and linear from n ined up to vocap size
and then when we swing over here we're actually going to get the loits by exactly what the co-pilot says now we
have to be careful here because this C and this C are not equal um this is nmed
C and this is vocap size so let's just say that n ined is equal to
C and then this just creates one spous layer of interaction through a linear layer but uh this should basically
run so we see that this runs and uh this currently looks kind of spous but uh
we're going to build on top of this now next up so far we've taken these indices
positional encoding
and we've encoded them based on the identity of the uh tokens in inside idx
the next thing that people very often do is that we're not just encoding the identity of these tokens but also their
position so we're going to have a second position uh embedding table here so self. position embedding table is an an
embedding of block size by an embed and so each position from zero to block size minus one will also get its own
embedding vector and then here first let me decode B BYT from idx do
shape and then here we're also going to have a pause embedding which is the positional embedding and these are this
is to arrange so this will be basically just integers from Z to T minus one and
all of those integers from 0 to T minus one get embedded through the table to create a t by
C and then here this gets renamed to just say x and x will be the addition of
the token embeddings with the positional embeddings and here the broadcasting note will work out so B by T by C plus T
by C this gets right aligned a new dimension of one gets added and it gets broadcasted across
batch so at this point x holds not just the token identities but the positions
at which these tokens occur and this is currently not that useful because of course we just have a simple byr model
so it doesn't matter if you're in the fifth position the second position or wherever it's all translation invariant at this stage uh so this information
currently wouldn't help uh but as we work on the self attention block we'll see that this starts to matter
okay so now we get the Crux of self attention so this is probably the most important part of this video to
THE CRUX OF THE VIDEO: version 4: self-attention
understand we're going to implement a small self attention for a single individual head as they're called so we
start off with where we were so all of this code is familiar so right now I'm
working with an example where I Chang the number of channels from 2 to 32 so we have a 4x8 arrangement of tokens and
each to and the information each token is currently 32 dimensional but we just are working with random
numbers now we saw here that the code as we had it before does a uh simple weight
simple average of all the past tokens and the current token so it's just the
previous information and current information is just being mixed together in an average and that's what this code currently achieves and it Doo by
creating this lower triangular structure which allows us to mask out this uh we
uh Matrix that we create so we mask it out and then we normalize it and
currently when we initialize the affinities between all the different sort of tokens or nodes I'm going to use
those terms interchangeably so when we initialize the affinities between all the different tokens to be zero then we see that way
gives us this um structure where every single row has these um uniform numbers
and so that's what that's what then uh in this Matrix multiply makes it so that we're doing a simple
average now we don't actually want this to be all uniform because different uh
tokens will find different other tokens more or less interesting and we want that to be data dependent so for example
if I'm a vowel then maybe I'm looking for consonants in my past and maybe I want to know what those consonants are
and I want that information to flow to me and so I want to now gather information from the past but I want to
do it in the data dependent way and this is the problem that self attention solves now the way self attention solves
this is the following every single node or every single token at each position
will emit two vectors it will emit a query and it will emit a
key now the query Vector roughly speaking is what am I looking for and
the key Vector roughly speaking is what do I contain and then the way we get
affinities between these uh tokens now in a sequence is we basically just do a
do product between the keys and the queries so my query dot products with
all the keys of all the other tokens and that dot product now becomes
wayy and so um if the key and the query are sort of aligned they will interact
to a very high amount and then I will get to learn more about that specific token as opposed to any other token in
the sequence so let's implement this
now we're going to implement a single what's called head of self
attention so this is just one head there's a hyper parameter involved with these heads which is the head size and
then here I'm initializing linear modules and I'm using bias equals false so these are just going to apply a
matrix multiply with some fixed weights and now let me produce a key and
q k and Q by forwarding these modules on X so the size of this will now
become B by T by 16 because that is the head size and the same here B by T by
16 so this being the head size so you see here that when I forward this linear
on top of my X all the tokens in all the positions in the B BYT Arrangement all
of them them in parallel and independently produce a key and a query so no communication has happened
yet but the communication comes now all the queries will do product with all the
keys so basically what we want is we want way now or the affinities between these to be query multiplying key but we
have to be careful with uh we can't Matrix multiply this we actually need to transpose uh K but we have to be also
careful because these are when you have The Bash Dimension so in particular we want to transpose uh the last two
dimensions dimension1 and dimension -2 so
-21 and so this Matrix multiply now will basically do the following B by T by
16 Matrix multiplies B by 16 by T to give us B by T by
T right so for every row of B we're now going to
have a t Square Matrix giving us the affinities and these are now the way so
they're not zeros they are now coming from this dot product between the keys and the queries so this can now run I
can I can run this and the weighted aggregation now is a function in a data
Bandon manner between the keys and queries of these nodes so just inspecting what happened
here the way takes on this form and you see that before way was uh just
a constant so it was applied in the same way to all the batch elements but now every single batch elements will have
different sort of we because uh every single batch element contains different uh tokens at different positions and so
this is not data dependent so when we look at just the zeroth uh Row for
example in the input these are the weights that came out and so you can see now that they're not just exactly
uniform um and in particular as an example here for the last row this was the eighth token and the eighth token
knows what content it has and it knows at what position it's in and now the E token based on that uh creates a query
hey I'm looking for this kind of stuff um I'm a vowel I'm on the E position I'm looking for any consonant at positions
up to four and then all the nodes get to emit keys and maybe one of the channels
could be I am a I am a consonant and I am in a position up to four and that that key would have a high number in
that specific Channel and that's how the query and the key when they do product they can find each other and create a
high affinity and when they have a high Affinity like say uh this token was
pretty interesting to uh to this eighth token when they have a high Affinity
then through the softmax I will end up aggregating a lot of its information into my position and so I'll get to
learn a lot about it now just this we're looking at way
after this has already happened um let me erase this operation as well so let
me erase the masking and the softmax just to show you the under the hood internals and how that works so without
the masking in the softmax Whey comes out like this right this is the outputs of the do products um and these are the
raw outputs and they take on values from negative you know two to positive two Etc so that's the raw interactions and
raw affinities between all the nodes but now if I'm going if I'm a fifth node I will not want to aggregate anything from
the sixth node seventh node and the eighth node so actually we use the upper triangular masking so those are not
allowed to communicate and now we actually want to have a nice uh distribution uh so we
don't want to aggregate negative .11 of this node that's crazy so instead we exponentiate and normalize and now we
get a nice distribution that sums to one and this is telling us now in the data dependent manner how much of information
to aggregate from any of these tokens in the past so that's way and it's not zeros
anymore but but it's calculated in this way now there's one more uh part to a
single self attention head and that is that when we do the aggregation we don't actually aggregate the tokens exactly we
aggregate we produce one more value here and we call that the
value so in the same way that we produced p and query we're also going to create a value
and then here we don't aggregate X we calculate a v which is
just achieved by uh propagating this linear on top of X again and then we
output way multiplied by V so V is the elements that we aggregate or the the
vectors that we aggregate instead of the raw X and now of course uh this will make it
so that the output here of this single head will be 16 dimensional because that is the head
size so you can think of X as kind of like private information to this token if you if you think about it that way so
X is kind of private to this token so I'm a fifth token at some and I have some identity and uh my information is
kept in Vector X and now for the purposes of the single head here's what I'm interested in here's what I have and
if you find me interesting here's what I will communicate to you and that's stored in v and so V is the thing that
gets aggregated for the purposes of this single head between the different notes and that's uh basically the self
attention mechanism this is this is what it does there are a few notes that I would make like to make about attention
note 1: attention as communication
number one attention is a communication mechanism you can really think about it as a communication mechanism where you
have a number of nodes in a directed graph where basically you have edges pointed between noes like
this and what happens is every node has some Vector of information and it gets to aggregate information via a weighted
sum from all of the nodes that point to it and this is done in a data dependent manner so depending on whatever data is
actually stored that you should not at any point in time now our graph doesn't
look like this our graph has a different structure we have eight nodes because the block size is eight and there's
always eight to tokens and uh the first node is only pointed to by itself the second node is
pointed to by the first node and itself all the way up to the eighth node which is pointed to by all the previous nodes
and itself and so that's the structure that our directed graph has or happens happens to have in Auto regressive sort
of scenario like language modeling but in principle attention can be applied to any arbitrary directed graph and it's
just a communication mechanism between the nodes the second note is that notice that there is no notion of space so
note 2: attention has no notion of space, operates over sets
attention simply acts over like a set of vectors in this graph and so by default
these nodes have no idea where they are positioned in the space and that's why we need to encode them positionally and
sort of give them some information that is anchored to a specific position so that they sort of know where they are
and this is different than for example from convolution because if you're run for example a convolution operation over some input there's a very specific sort
of layout of the information in space and the convolutional filters sort of act in space and so it's it's not like
an attention in ATT ention is just a set of vectors out there in space they communicate and if you want them to have
a notion of space you need to specifically add it which is what we've done when we calculated the um relative
the positional encode encodings and added that information to the vectors the next thing that I hope is very clear
note 3: there is no communication across batch dimension
is that the elements across the batch Dimension which are independent examples never talk to each other they're always
processed independently and this is a batched matrix multiply that applies basically a matrix multiplication uh
kind of in parallel across the batch dimension so maybe it would be more accurate to say that in this analogy of
a directed graph we really have because the back size is four we really have four separate pools of eight nodes and
those eight nodes only talk to each other but in total there's like 32 nodes that are being processed uh but there's
um sort of four separate pools of eight you can look at it that way the next note is that here in the case of
note 4: encoder blocks vs. decoder blocks
language modeling uh we have this specific uh structure of directed graph where the future tokens will not
communicate to the Past tokens but this doesn't necessarily have to be the constraint in the general case and in
fact in many cases you may want to have all of the uh noes talk to each other uh fully so as an example if you're doing
sentiment analysis or something like that with a Transformer you might have a number of tokens and you may want to
have them all talk to each other fully because later you are predicting for example the sentiment of the sentence
and so it's okay for these NOS to talk to each other and so in those cases you will use an encoder block of self
attention and uh all it means that it's an encoder block is that you will delete this line of code allowing all the noes
to completely talk to each other what we're implementing here is sometimes called a decoder block and it's called a
decoder because it is sort of like a decoding language and it's got this
autor regressive format where you have to mask with the Triangular Matrix so that uh nodes from the future never talk
to the Past because they would give away the answer and so basically in encoder blocks you
would delete this allow all the noes to talk in decoder blocks this will always be present so that you have this
triangular structure uh but both are allowed and attention doesn't care attention supports arbitrary connectivity between nodes the next
note 5: attention vs. self-attention vs. cross-attention
thing I wanted to comment on is you keep me you keep hearing me say attention self attention Etc there's actually also
something called cross attention what is the difference so basically the reason this attention
is self attention is because because the keys queries and the values are all
coming from the same Source from X so the same Source X produces Keys queries
and values so these nodes are self attending but in principle attention is
much more General than that so for example an encoder decoder Transformers uh you can have a case where the queries
are produced from X but the keys and the values come from a whole separate external source and sometimes from uh
encoder blocks that encode some context that we'd like to condition on and so the keys and the values will
actually come from a whole separate Source those are nodes on the side and here we're just producing queries and
we're reading off information from the side so cross attention is used when there's a separate source of nodes we'd
like to pull information from into our nodes and it's self attention if we just have nodes that would like to look at
each other and talk to each other so this attention here happens to be self
attention but in principle um attention is a lot more General okay and the last
note 6: "scaled" self-attention. why divide by sqrt(head_size)
note at this stage is if we come to the attention is all need paper here we've already implemented attention so given
query key and value we've U multiplied the query and a key we've soft maxed it
and then we are aggregating the values there's one more thing that we're missing here which is the dividing by one / square root of the head size the
DK here is the head size why are they doing this finds this important so they call it the scaled attention and it's
kind of like an important normalization to basically have the problem is if you have unit gsh
and inputs so zero mean unit variance K and Q are unit gashin then if you just do we naively then you see that your we
actually will be uh the variance will be on the order of head size which in our case is 16 but if you multiply by one
over head size square root so this is square root and this is one over then the variance of we will be one
so it will be preserved now why is this important you'll not notice that way
here will feed into softmax and so it's really important especially at initialization that we be
fairly diffuse so in our case here we sort of locked out here and we had a
fairly diffuse numbers here so um like this now the problem is that because of
softmax if weight takes on very positive and very negative numbers inside it softmax will actually converge towards
one hot vectors and so I can illustrate that here um say we are applying softmax
to a tensor of values that are very close to zero then we're going to get a diffuse thing out of softmax but the moment I take the exact
same thing and I start sharpening it making it bigger by multiplying these numbers by eight for example you'll see
that the softmax will start to sharpen and in fact it will sharpen towards the max so it will sharpen towards whatever
number here is the highest and so um basically we don't want these values to be too extreme especially at
initialization otherwise softmax will be way too peaky and um you're basically aggregating um information from like a
single node every node just agregates information from a single other node that's not what we want especially at
initialization and so the scaling is used just to control the variance at initialization okay so having said all
inserting a single self-attention block to our network
that let's now take our self attention knowledge and let's uh take it for a spin so here in the code I created this
head module and it implements a single head of self attention so you give it a head size and then here it creates the
key query and the value linear layers typically people don't use biases in these uh so those are the linear
projections that we're going to apply to all of our nodes now here I'm creating this Trill variable Trill is not a
parameter of the module so in sort of pytorch naming conventions uh this is called a buffer it's not a parameter and
you have to call it you have to assign it to the module using a register buffer so that creates the trill uh the triang
lower triangular Matrix and we're given the input X this should look very familiar now we calculate the keys the
queries we C calculate the attention scores inside way uh we normalize it so
we're using scaled attention here then we make sure that uh future doesn't communicate with the past so this makes
it a decoder block and then softmax and then aggregate the value and
output then here in the language model I'm creating a head in the Constructor and I'm calling it self attention head
and the head size I'm going to keep as the same and embed just for
now and then here once we've encoded the information with the token embeddings
and the position embeddings we're simply going to feed it into the self attention head and then the output of that is
going to go into uh the decoder language modeling head and create the logits so
this the sort of the simplest way to plug in a self attention component uh into our Network right now I had to make
one more change which is that here in the generate uh we have to make sure
that our idx that we feed into the model because now we're using positional embeddings we can never have more than
block size coming in because if idx is more than block size then our position
embedding table is going to run out of scope because it only has embeddings for up to block size and so therefore I
added some uh code here to crop the context that we're going to feed into
self um so that uh we never pass in more than block siiz elements
so those are the changes and let's Now train the network okay so I also came up to the script here and I decreased the
learning rate because uh the self attention can't tolerate very very high learning rates and then I also increased
number of iterations because the learning rate is lower and then I trained it and previously we were only able to get to up to 2.5 and now we are
down to 2.4 so we definitely see a little bit of an improvement from 2.5 to 2.4 roughly uh but the text is still not
amazing so clearly the self attention head is doing some useful communication
but um we still have a long way to go okay so now we've implemented the scale. product attention now next up and the
multi-headed self-attention
attention is all you need paper there's something called multi-head attention and what is multi-head attention it's
just applying multiple attentions in parallel and concatenating their results so they have a little bit of diagram
here I don't know if this is super clear it's really just multiple attentions in
parallel so let's Implement that fairly straightforward if we want a multi-head attention then
we want multiple heads of self attention running in parallel so in pytorch we can
do this by simply creating multiple heads so however heads how however many
heads you want and then what is the head size of each and then we run all of them
in parallel into a list and simply concatenate all of the outputs and we're
concatenating over the channel Dimension so the way this looks now is we don't have just a single ATT
that uh has a hit size of 32 because remember n Ed is 32 instead of having one Communication
channel we now have four communication channels in parallel and each one of these communication channels typically
will be uh smaller uh correspondingly so because we have four communication
channels we want eight dimensional self attention and so from each Communication channel we're going to together eight
dimensional vectors and then we have four of them and that concatenates to give us 32 which is the original and
embed and so this is kind of similar to um if you're familiar with convolutions this is kind of like a group convolution
uh because basically instead of having one large convolution we do convolution in groups and uh that's multi-headed
self attention and so then here we just use essay heads self attention heads instead
now I actually ran it and uh scrolling down I ran the same thing and then we
now get this down to 2.28 roughly and the output is still the generation is
still not amazing but clearly the validation loss is improving because we were at 2.4 just now and so it helps to
have multiple communication channels because obviously these tokens have a lot to talk about they want to find the
consonants the vowels they want to find the vowels just from certain positions uh they want to find any kinds of
different things and so it helps to create multiple independent channels of communication gather lots of different
types of data and then uh decode the output now going back to the paper for a second of course I didn't explain this
feedforward layers of transformer block
figure in full detail but we are starting to see some components of what we've already implemented we have the positional encodings the token encodings
that add we have the masked multi-headed attention implemented now here's another
multi-headed attention which is a cross attention to an encoder which we haven't we're not going to implement in this
case I'm going to come back to that later but I want you to notice that there's a feed forward part here and
then this is grouped into a block that gets repeat it again and again now the feedforward part here is just a simple
uh multi-layer perceptron um so the multi-headed so here position
wise feed forward networks is just a simple little MLP so I want to start basically in a similar fashion also
adding computation into the network and this computation is on a per node level
so I've already implemented it and you can see the diff highlighted on the left here when I've added or changed things
now before we had the self multi-headed self attention that did the communication but we went way too fast
to calculate the logits so the tokens looked at each other but didn't really have a lot of time to think on what they
found from the other tokens and so what I've implemented here is a little feet
forward single layer and this little layer is just a linear followed by a Rel nonlinearity and that's that's it so
it's just a little layer and then I call it feed forward um and embed
and then this feed forward is just called sequentially right after the self attention so we self attend then we feed
forward and you'll notice that the feet forward here when it's applying linear this is on a per token level all the
tokens do this independently so the self attention is the communication and then once they've gathered all the data now
they need to think on that data individually and so that's what feed forward is doing and that's why I've
added it here now when I train this the validation LW actually continues to go down now to 2. 24 which is down from
2.28 uh the output still look kind of terrible but at least we've improved the situation and so as a preview we're
going to now start to intersperse the communication with the computation and
that's also what the Transformer does when it has blocks that communicate and then compute and it groups them and
replicates them okay so let me show you what we'd like to do we'd like to do
residual connections
something like this we have a block and this block is is basically this part here except for the cross
attention now the block basically intersperses communication and then computation the computation the
communication is done using multi-headed selfelf attention and then the computation is done using a feed forward
Network on all the tokens independently now what I've added here
also is you'll notice this takes the number of embeddings in the embedding Dimension
and number of heads that we would like which is kind of like group size in group convolution and and I'm saying
that number of heads we'd like is four and so because this is 32 we calculate that because this is 32 the number of
heads should be four um the head size should be eight so that everything sort
of works out Channel wise um so this is how the Transformer structures uh sort of the uh the sizes typically so the
head size will become eight and then this is how we want to intersperse them and then here I'm trying to create
blocks which is just a sequential application of block block block so that we're interspersing communication feed
forward many many times and then finally we decode now I actually tried to run
this and the problem is this doesn't actually give a very good uh answer and very good result and the reason for that
is we're start starting to actually get like a pretty deep neural net and deep neural Nets uh suffer from optimization
issues and I think that's what we're kind of like slightly starting to run into so we need one more idea that we can borrow from the um Transformer paper
to resolve those difficulties now there are two optimizations that dramatically help with the depth of these networks
and make sure that the networks remain optimizable let's talk about the first one the first one in this diagram is you
see this Arrow here and then this arrow and this Arrow those are skip connections or sometimes called residual
connections they come from this paper uh the presidual learning for image recognition from about
2015 uh that introduced the concept now these are basically what it means is you
transform data but then you have a skip connection with addition from the previous features now the way I like to
visualize it uh that I prefer is the following here the computation happens
from the top to bottom and basically you have this uh residual pathway and you
are free to Fork off from the residual pathway perform some computation and then project back to the residual
pathway via addition and so you go from the the uh inputs to the targets only
via plus and plus plus and the reason this is useful is because during back propagation remember from our microG
grad video earlier addition distributes gradients equally to both of its branches that that fed as the input and
so the supervision or the gradients from the loss basically hop through every
addition node all the way to the input and then also Fork off into the residual
blocks but basically you have this gradient Super Highway that goes directly from the supervision all the
way to the input unimpeded and then these viral blocks are usually initialized in the beginning so they
contribute very very little if anything to the residual pathway they they are initialized that way so in the beginning
they are sort of almost kind of like not there but then during the optimization they come online over time and they uh
start to contribute but at least at the initialization you can go from directly supervision to the input gradient is
unimpeded and just flows and then the blocks over time kick in and so that dramatically helps
with the optimization so let's implement this so coming back to our block here basically what we want to do is we want
to do xal X+ self attention and xal X+ self. feed
forward so this is X and then we Fork off and do some communication and come
back and we Fork off and we do some computation and come back so those are residual connections and then swinging
back up here we also have to introd use this projection so nn.
linear and uh this is going to be from after we concatenate this this is
the prze and embed so this is the output of the self tension itself but then we
actually want the uh to apply the projection and that's the result so the projection is just a
linear transformation of the outcome of this layer so that's the projection back into the virual pathway and then here in a
feet forward it's going to be the same same thing I could have a a self doot projection here as well but let me just
simplify it and let me uh couple it inside the same sequential container and
so this is the projection layer going back into the residual pathway and
so that's uh well that's it so now we can train this so I implemented one more small change when you look into the
paper again you see that the dimensionality of input and output is 512 for them and they're saying that the
inner layer here in the feet forward has dimensionality of 248 so there's a multiplier of four and so the inner
layer of the feet forward Network should be multiplied by four in terms of Channel sizes so I came here and I
multiplied four times embed here for the feed forward and then from four times nmed coming back down to nmed when we go
back to the pro uh to the projection so adding a bit of computation here and growing that layer that is in the
residual block on the side of the residual pathway and then I train this and we
actually get down all the way to uh 2.08 validation loss and we also see that network is starting to get big enough
that our train loss is getting ahead of validation loss so we're starting to see like a little bit of overfitting and um our our
um uh Generations here are still not amazing but at least you see that we can see like is here this now grief syn like
this starts to almost look like English so um yeah we're starting to really get there okay and the second Innovation
layernorm (and its relationship to our previous batchnorm)
that is very helpful for optimizing very deep neural networks is right here so we have this addition now that's the
residual part but this Norm is referring to something called layer Norm so layer Norm is implemented in pytorch it's a
paper that came out a while back here um and layer Norm is very very similar
to bash Norm so remember back to our make more series part three we implemented bash
normalization and uh bash normalization basically just made sure that um Across
The Bash dimension any individual neuron had unit uh Gan um distribution so it
was zero mean and unit standard deviation one standard deviation output
so what I did here is I'm copy pasting the bashor 1D that we developed in our make more series and see here we can
initialize for example this module and we can have a batch of 32 100 dimensional vectors feeding through the
bachor layer so what this does is it guarantees that when we look at just the
zeroth column it's a zero mean one standard deviation so it's normalizing
every single column of this uh input now the rows are not uh going to be
normalized by default because we're just normalizing columns so let's now Implement layer Norm uh it's very
complicated look we come here we change this from zero to one so we don't
normalize The Columns we normalize the rows and now we've implemented layer
Norm so now the columns are not going to be normalized um but the rows are going to
be normalized for every individual example it's 100 dimensional Vector is normalized uh in this way and because
our computation Now does not span across examples we can delete all of this
buffers stuff uh because uh we can always apply this operation and don't
need to maintain any running buffers so we don't need the buffers uh we
don't There's no distinction between training and test time uh and we don't need these running
buffers we do keep gamma and beta we don't need the momentum we don't care if it's training or not and this is now a
layer norm and it normalizes the rows instead of the columns and this here is
identical to basically this here so let's now Implement layer Norm in our
Transformer before I incorporate the layer Norm I just wanted to note that as I said very few details about the
Transformer have changed in the last 5 years but this is actually something that slightly departs from the original paper you see that the ADD and Norm is
applied after the transformation but um in now it is a bit
more uh basically common to apply the layer Norm before the transformation so there's a reshuffling of the layer Norms
uh so this is called the prorm formulation and that's the one that we're going to implement as well so select deviation from the original paper
basically we need two layer Norms layer Norm one is uh NN do layer norm and we
tell it how many um what is the embedding Dimension and we need the second layer norm and then here the
layer Norms are applied immediately on X so self. layer Norm one applied on X and
self. layer Norm two applied on X before it goes into self attention and feed
forward and uh the size of the layer Norm here is an ed so 32 so when the
layer Norm is normalizing our features it is uh the normalization here uh
happens the mean and the variance are taken over 32 numbers so the batch and the time act as batch Dimensions both of
them so this is kind of like a per token um transformation that just normalizes
the features and makes them a unit mean uh unit Gan at
initialization but of course because these layer Norms inside it have these gamma and beta training
parameters uh the layer Norm will U eventually create outputs that might not
be unit gion but the optimization will determine that so for now this is the uh
this is incorporating the layer norms and let's train them on okay so I let it run and we see that we get down to 2.06
which is better than the previous 2.08 so a slight Improvement by adding the layer norms and I'd expect that they
help uh even more if we had bigger and deeper Network one more thing I forgot to add is that there should be a layer
Norm here also typically as at the end of the Transformer and right before the
final uh linear layer that decodes into vocabulary so I added that as well so at
this stage we actually have a pretty complete uh Transformer according to the original paper and it's a decoder only
Transformer I'll I'll talk about that in a second uh but at this stage uh the major pieces are in place so we can try
to scale this up and see how well we can push this number now in order to scale out the model I had to perform some
scaling up the model! creating a few variables. adding dropout
cosmetic changes here to make it nicer so I introduced this variable called n layer which just specifies how many
layers of the blocks we're going to have I created a bunch of blocks and we have a new variable number of heads as well I
pulled out the layer Norm here and uh so this is identical now one thing that I did briefly change is I added a Dropout
so Dropout is something that you can add right before the residual connection back right before the connection back
into the residual pathway so we can drop out that as l layer here we can drop out
uh here at the end of the multi-headed exension as well and we can also drop out here uh when we calculate the um
basically affinities and after the softmax we can drop out some of those so we can randomly prevent some of the
nodes from communicating and so Dropout uh comes from this paper from 2014 or so and
basically it takes your neural nut and it randomly every forward backward pass shuts off some subset of
uh neurons so randomly drops them to zero and trains without them and what
this does effectively is because the mask of what's being dropped out is changed every single forward backward
pass it ends up kind of uh training an ensemble of sub networks and then at
test time everything is fully enabled and kind of all of those sub networks are merged into a single Ensemble if you
can if you want to think about it that way so I would read the paper to get the full detail for now we're just going to
stay on the level of this is a regularization technique and I added it because I'm about to scale up the model
quite a bit and I was concerned about overfitting so now when we scroll up to the top uh we'll see that I changed a
number of hyper parameters here about our neural nut so I made the batch size be much larger now it's 64 I changed the
block size to be 256 so previously it was just eight eight characters of context now it is 256 characters of
context to predict the 257th uh I brought down the learning rate a
little bit because the neural net is now much bigger so I brought down the learning rate the embedding Dimension is
now 384 and there are six heads so 384 divide 6 means that every head is 64
dimensional as it as a standard and then there's going to be six layers of that
and the Dropout will be at 02 so every forward backward pass 20% of all of these um intermediate calculations are
disabled and dropped to zero and then I already trained this and I ran it so uh drum roll how well does it
perform so let me just scroll up here we get a validation loss of
1.48 which is actually quite a bit of an improvement on what we had before which I think was 2.07 so it went from 2.07
all the way down to 1.48 just by scaling up this neural nut with the code that we have and this of course ran for a lot
longer this maybe trained for I want to say about 15 minutes on my a100 GPU so
that's a pretty a GPU and if you don't have a GPU you're not going to be able to reproduce this uh on a CPU this would
be um I would not run this on a CPU or MacBook or something like that you'll have to Brak down the number of uh
layers and the embedding Dimension and so on uh but in about 15 minutes we can get this kind of a result and um I'm
printing some of the Shakespeare here but what I did also is I printed 10,000 characters so a lot more and I wrote
them to a file and so here we see some of the outputs
so it's a lot more recognizable as the input text file so the input text file just for reference looked like this so
there's always like someone speaking in this manner and uh our predictions now
take on that form except of course they're they're nonsensical when you actually read them
so it is every crimp tap be a house oh those
prepation we give heed um you know
Oho sent me you mighty Lord anyway so you can read through this
um it's nonsensical of course but this is just a Transformer trained on a character level for 1 million characters
that come from Shakespeare so there's sort of like blabbers on in Shakespeare like manner but it doesn't of course
make sense at this scale uh but I think I think still a pretty good demonstration of what's
possible so now I think uh that kind of like concludes
the programming section of this video we basically kind of uh did a pretty good job and um of implementing this
Transformer uh but the picture doesn't exactly match up to what we've done so what's going on with all these digital
Parts here so let me finish explaining this architecture and why it looks so funky basically what's happening here is
encoder vs. decoder vs. both (?) Transformers
what we implemented here is a decoder only Transformer so there's no component here this part is called the encoder and
there's no cross attention block here our block only has a self attention and
the feet forward so it is missing this third in between piece here this piece
does cross attention so we don't have it and we don't have the encoder we just have the decoder and the reason we have
a decoder only uh is because we are just uh generating text and it's unconditioned on anything we're just
we're just blabbering on according to a given data set what makes it a decoder is that we are using the Triangular mask
in our uh trans former so it has this Auto regressive property where we can just uh go and sample from it so the
fact that it's using the Triangular triangular mask to mask out the attention makes it a decoder and it can
be used for language modeling now the reason that the original paper had an incoder decoder architecture is because
it is a machine translation paper so it is concerned with a different setting in particular it expects some uh tokens
that encode say for example French and then it is expecting to decode the translation in English so so you
typically these here are special tokens so you are expected to read in this and
condition on it and then you start off the generation with a special token called start so this is a special new
token um that you introduce and always place in the beginning and then the network is expected to Output neural
networks are awesome and then a special end token to finish the generation so this part here will be
decoded exactly as we we've done it neural networks are awesome will be identical to what we did but unlike what
we did they wanton to condition the generation on some additional information and in that case this
additional information is the French sentence that they should be translating so what they do now is they
bring in the encoder now the encoder reads this part here so we're only going
to take the part of French and we're going to uh create tokens from it exactly as we've seen in our video and
we're going to put a Transformer on it but there's going to be no triangular mask and so all the tokens are allowed
to talk to each other as much as they want and they're just encoding whatever's the content of this French uh
sentence once they've encoded it they they basically come out in the top here
and then what happens here is in our decoder which does the uh language modeling there's an additional
connection here to the outputs of the encoder and that is brought in through a cross
attention so the queries are still generated from X but now the keys and the values are coming from the side the
keys and the values are coming from the top generated by the nodes that came outside of the de the encoder and those
tops the keys and the values there the top of it feed in on a side into every
single block of the decoder and so that's why there's an additional cross attention and really what it's doing is
it's conditioning the decoding not just on the past of this current decoding but also on having seen the
full fully encoded French um prompt sort of and so it's an encoder decoder model
which is why we have those two Transformers an additional block and so on so we did not do this because we have
no we have nothing to encode there's no conditioning we just have a text file and we just want to imitate it and that's why we are using a decoder only
Transformer exactly as done in GPT okay okay so now I wanted to do a
super quick walkthrough of nanoGPT, batched multi-headed self-attention
very brief walkthrough of nanog GPT which you can find in my GitHub and uh nanog GPT is basically two files of
Interest there's train.py and model.py train.py is all the boilerplate code for
training the network it is basically all the stuff that we had here it's the training loop it's just that it's a lot
more complicated because we're saving and loading checkpoints and pre-trained weights and we are uh decaying the
learning rate and compiling the model and using distributed training across multiple nodes or GP use so the training
Pi gets a little bit more hairy complicated uh there's more options Etc
but the model.py should look very very um similar to what we've done here in fact the model is is almost identical so
first here we have the causal self attention block and all of this should look very very recognizable to you we're
producing queries Keys values we're doing Dot products we're masking applying soft Maxs optionally dropping
out and here we are pulling the wi the values what is different here is that in
our code I have separated out the multi-headed detention into just a
single individual head and then here I have multiple heads and I explicitly concatenate them whereas here uh all of
it is implemented in a batched manner inside a single causal self attention and so we don't just have a b and a T
and A C Dimension we also end up with a fourth dimension which is the heads and so it just gets a lot more sort of hairy
because we have four dimensional array um tensors now but it is um equivalent
mathematically so the exact same thing is happening as what we have it's just it's a bit more efficient because all
the heads are now treated as a batch Dimension as well then we have the multier perceptron
it's using the Galu nonlinearity which is defined here except instead of Ru and
this is done just because opening I used it and I want to be able to load their checkpoints uh the blocks of the
Transformer are identical to communicate in the compute phase as we saw and then the GPT will be identical we have the
position encodings token encodings the blocks the layer Norm at the end uh the
final linear layer and this should look all very recognizable and there's a bit
more here because I'm loading checkpoints and stuff like that I'm separating out the parameters into those that should be weight decayed and those
that shouldn't um but the generate function should also be very very similar so a
few details are different but you should definitely be able to look at this uh file and be able to understand little
the pieces now so let's now bring things back to chat GPT what would it look like if we wanted to train chat GPT ourselves
back to ChatGPT, GPT-3, pretraining vs. finetuning, RLHF
and how does it relate to what we learned today well to train in chat GPT there are roughly two stages first is
the pre-training stage and then the fine-tuning stage in the pre-training stage uh we are training on a large
chunk of internet and just trying to get a first decoder only Transformer to
babble text so it's very very similar to what we've done ourselves except we've
done like a tiny little baby pre-training step um and so in our case
uh this is how you print a number of parameters I printed it and it's about 10 million so this Transformer that I
created here to create little Shakespeare um Transformer was about 10
million parameters our data set is roughly 1 million uh characters so roughly 1 million tokens but you have to
remember that opening I is different vocabulary they're not on the Character level they use these um subword chunks
of words and so they have a vocabulary of 50,000 roughly elements and so their sequences are a bit more condensed so
our data set the Shakespeare data set would be probably around 300,000 uh tokens in the open AI vocabulary roughly
so we trained about 10 million parameter model on roughly 300,000 tokens now when
you go to the gpt3 paper and you look at the Transformers
that they trained they trained a number of trans Transformers of different sizes but the biggest Transformer here has 175
billion parameters uh so ours is again 10 million they used this number of layers in the Transformer this is the
nmed this is the number of heads and this is the head size and then this is
the batch size uh so ours was 65 and the learning rate is similar now
when they train this Transformer they trained on 300 billion tokens so again remember ours is about 300,000
so this is uh about a millionfold increase and this number would not be even that large by today's standards
you'd be going up uh 1 trillion and above so they are training a
significantly larger model on uh a good chunk of the internet
and that is the pre-training stage but otherwise these hyper parameters should be fairly recognizable to you and the
architecture is actually like nearly identical to what we implemented ourselves but of course it's a massive
infrastructure challenge to train this you're talking about typically thousands of gpus having to you know talk to each
other to train models of this size so that's just a pre-training stage now after you complete the pre-training
stage uh you don't get something that responds to your questions with answers and is not helpful and Etc you get a
document completer right so it babbles but it doesn't Babble Shakespeare it babbles
internet it will create arbitrary news articles and documents and it will try to complete documents because that's
what it's trained for it's trying to complete the sequence so when you give it a question it would just uh potentially just give you more questions
it would follow with more questions it will do whatever it looks like the some close document would do in the training
data on the internet and so who knows you're getting kind of like undefined Behavior it might basically answer with
to questions with other questions it might ignore your question it might just try to complete some news article it's
totally unineed as we say so the second fine-tuning stage is to actually align
it to be an assistant and uh this is the second stage and so this chat GPT block
post from openi talks a little bit about how the stage is achieved we basically
um there's roughly three steps to to this stage uh so what they do here is they start to collect training data that
looks specifically like what an assistant would do so these are documents that have to format where the question is on top and then an answer is
below and they have a large number of these but probably not on the order of the internet uh this is probably on the
of maybe thousands of examples and so they they then fine-tune the model to
basically only focus on documents that look like that and so you're starting to slowly align it so it's going to expect
a question at the top and it's going to expect to complete the answer and uh these very very large models are very
sample efficient during their fine-tuning so this actually somehow works but that's just step one that's
just fine tuning so then they actually have more steps where okay the second step is you let the model respond and
then different Raiders look at the different responses and rank them for their preference as to which one is
better than the other they use that to train a reward model so they can predict uh basically using a different network
how much of any candidate response would be desirable and then
once they have a reward model they run po which is a form of polic policy gradient um reinforcement learning
Optimizer to uh fine-tune this sampling policy uh so that the answers that the
GP chat GPT now generates are expected to score a high reward according to the
reward model and so basically there's a whole aligning stage here or fine-tuning stage it's got multiple steps in between
there as well and it takes the model from being a document completer to a
question answerer and that's like a whole separate stage a lot of this data is not available publicly it is internal
to open AI and uh it's much harder to replicate this stage um and so that's
roughly what would give you a chat GPT and nanog GPT focuses on the pre-training stage okay and that's
conclusions
everything that I wanted to cover today so we trained to summarize a decoder
only Transformer following this famous paper attention is all you need from
2017 and so that's basically a GPT we trained it on Tiny Shakespeare and got
sensible results all of the training code is roughly 200 lines of code I will be
releasing this um code base so also it comes with all the git log commits along
the way as we built it up in addition to this code I'm going to release the um notebook of course the
Google collab and I hope that gave you a sense for how you can train um these
models like say gpt3 that will be um architecturally basically identical to what we have but they are somewhere
between 10,000 and 1 million times bigger depending on how you count and so
uh that's all I have for now uh we did not talk about any of the fine-tuning stages that would typically go on top of
this so if you're interested in something that's not just language modeling but you actually want to you know say perform tasks um or you want
them to be aligned in a specific way or you want um to detect sentiment or anything like that basically anytime you
don't want something that's just a document completer you have to complete further stages of fine tuning which did
not cover uh and that could be simple supervised fine tuning or it can be something more fancy like we see in chat
jpt where we actually train a reward model and then do rounds of Po to uh align it with respect to the reward
model so there's a lot more that can be done on top of it I think for now we're starting to get to about two hours Mark
uh so I'm going to um kind of finish here uh I hope you enjoyed the lecture
uh and uh yeah go forth and transform see you later





Intro
[MUSIC]
ANNOUNCER: Please welcome AI researcher and founding member of OpenAI, Andrej Karpathy.
ANDREJ KARPATHY: Hi, everyone. I'm happy to be here to tell you about the state of GPT and more generally about
the rapidly growing ecosystem of large language models. I would like to partition the talk into two parts.
In the first part, I would like to tell you about how we train GPT Assistance, and then in the second part,
we're going to take a look at how we can use these assistants effectively for your applications.
First, let's take a look at the emerging recipe for how to train these assistants and keep in mind that this is all very new and still rapidly evolving,
but so far, the recipe looks something like this. Now, this is a complicated slide, I'm going to go through it piece by
GPT Assistant training pipeline
piece, but roughly speaking, we have four major stages, pretraining,
supervised finetuning, reward modeling, reinforcement learning, and they follow each other serially.
Now, in each stage, we have a dataset that powers that stage. We have an algorithm that for our purposes will be
a objective and over for training the neural network, and then we have a resulting model,
and then there are some notes on the bottom. The first stage we're going to start with as the pretraining stage. Now, this stage is special in this diagram,
and this diagram is not to scale because this stage is where all of the computational work basically happens. This is 99 percent of the training
compute time and also flops. This is where we are dealing with
Internet scale datasets with thousands of GPUs in the supercomputer and also months of training potentially.
The other three stages are finetuning stages that are much more along the lines of small few number of GPUs and hours or days.
Let's take a look at the pretraining stage to achieve a base model. First, we are going to gather a large amount of data.
Data collection
Here's an example of what we call a data mixture that comes from this paper that was released by
Meta where they released this LLaMA based model. Now, you can see roughly the datasets that
enter into these collections. We have CommonCrawl, which is a web scrape, C4, which is also CommonCrawl,
and then some high quality datasets as well. For example, GitHub, Wikipedia, Books, Archives, Stock Exchange and so on.
These are all mixed up together, and then they are sampled according to some given proportions,
and that forms the training set for the GPT. Now before we can actually train on this data,
we need to go through one more preprocessing step, and that is tokenization. This is basically a translation of
the raw text that we scrape from the Internet into sequences of integers because
that's the native representation over which GPTs function. Now, this is a lossless translation
between pieces of texts and tokens and integers, and there are a number of algorithms for the stage.
Typically, for example, you could use something like byte pair encoding, which iteratively merges text chunks
and groups them into tokens. Here, I'm showing some example chunks of these tokens,
and then this is the raw integer sequence that will actually feed into a transformer. Now, here I'm showing
2 example models
two examples for hybrid parameters that govern this stage.
GPT-4, we did not release too much information about how it was trained and so on, I'm using GPT-3s numbers,
but GPT-3 is of course a little bit old by now, about three years ago. But LLaMA is a fairly recent model from Meta.
These are roughly the orders of magnitude that we're dealing with when we're doing pretraining. The vocabulary size is usually a couple 10,000 tokens.
The context length is usually something like 2,000, 4,000, or nowadays even 100,000,
and this governs the maximum number of integers that the GPT will look at when it's trying to
predict the next integer in a sequence. You can see that roughly the number of parameters say,
65 billion for LLaMA. Now, even though LLaMA has only 65B parameters compared to GPP-3s 175 billion parameters,
LLaMA is a significantly more powerful model, and intuitively, that's because the model is trained for significantly longer.
In this case, 1.4 trillion tokens, instead of 300 billion tokens. You shouldn't judge the power of a model by
the number of parameters that it contains. Below, I'm showing some tables of rough hyperparameters that typically
go into specifying the transformer neural network, the number of heads, the dimension size, number of layers,
and so on, and on the bottom I'm showing some training hyperparameters. For example, to train the 65B model,
Meta used 2,000 GPUs, roughly 21 days of training and a roughly several million dollars.
That's the rough orders of magnitude that you should have in mind for the pre-training stage.
Now, when we're actually pre-training, what happens? Roughly speaking, we are going to take our tokens,
and we're going to lay them out into data batches. We have these arrays that will feed into the transformer,
and these arrays are B, the batch size and these are all independent examples stocked up in rows and B by T,
T being the maximum context length. In my picture I only have 10 the context lengths, so this could be 2,000, 4,000, etc.
These are extremely long rows. What we do is we take these documents, and we pack them into rows,
and we delimit them with these special end of texts tokens, basically telling the transformer where a new document begins.
Here, I have a few examples of documents and then I stretch them out into this input.
Now, we're going to feed all of these numbers into transformer. Let me just focus on a single particular cell,
but the same thing will happen at every cell in this diagram. Let's look at the green cell. The green cell is going to take
a look at all of the tokens before it, so all of the tokens in yellow, and we're going to feed that entire context
into the transforming neural network, and the transformer is going to try to predict the next token in
a sequence, in this case in red. Now the transformer, I don't have too much time to, unfortunately, go into the full details of this
neural network architecture is just a large blob of neural net stuff for our purposes, and it's got several,
10 billion parameters typically or something like that. Of course, as I tune these parameters, you're getting slightly different predicted distributions
for every single one of these cells. For example, if our vocabulary size is 50,257 tokens,
then we're going to have that many numbers because we need to specify a probability distribution for what comes next.
Basically, we have a probability for whatever may follow. Now, in this specific example, for this specific cell,
513 will come next, and so we can use this as a source of supervision to update our transformers weights.
We're applying this basically on every single cell in the parallel, and we keep swapping batches, and we're trying to get the transformer to make
the correct predictions over what token comes next in a sequence. Let me show you more concretely what this looks
like when you train one of these models. This is actually coming from the New York Times, and they trained a small GPT on Shakespeare.
Here's a small snippet of Shakespeare, and they train their GPT on it. Now, in the beginning, at initialization,
the GPT starts with completely random weights. You're getting completely random outputs as well. But over time, as you train the GPT longer and longer,
you are getting more and more coherent and consistent samples from the model,
and the way you sample from it, of course, is you predict what comes next, you sample from that distribution and
you keep feeding that back into the process, and you can basically sample large sequences.
By the end, you see that the transformer has learned about words and where to put spaces and where to put commas and so on.
We're making more and more consistent predictions over time. These are the plots that you are looking at when you're doing model pretraining.
Effectively, we're looking at the loss function over time as you train, and low loss means that our transformer
is giving a higher probability to the next correct integer in the sequence.
What are we going to do with model once we've trained it after a month? Well, the first thing that we noticed, we the field,
Base models learn powerful, general representations
is that these models basically in the process of language modeling, learn very powerful general representations,
and it's possible to very efficiently fine tune them for any arbitrary downstream tasks you might be interested in.
As an example, if you're interested in sentiment classification, the approach used to be that you collect a bunch of positives
and negatives and then you train some NLP model for that, but the new approach is:
ignore sentiment classification, go off and do large language model pretraining,
train a large transformer, and then you may only have a few examples and you can very efficiently fine tune
your model for that task. This works very well in practice. The reason for this is that basically
the transformer is forced to multitask a huge amount of tasks in the language modeling task,
because in terms of predicting the next token, it's forced to understand a lot about the structure of the text and all the different concepts therein.
That was GPT-1. Now around the time of GPT-2, people noticed that actually even better than fine tuning,
you can actually prompt these models very effectively. These are language models and they want to complete documents,
you can actually trick them into performing tasks by arranging these fake documents.
In this example, for example, we have some passage and then we like do QA, QA, QA.
This is called Few-shot prompt, and then we do Q, and then as the transformer is tried to complete the document is actually answering our question.
This is an example of prompt engineering based model, making it believe that it's imitating a document and getting it to perform a task.
This kicked off, I think the era of, I would say, prompting over fine tuning and seeing that this
actually can work extremely well on a lot of problems, even without training any neural networks, fine tuning or so on.
Now since then, we've seen an entire evolutionary tree of base models that everyone has trained.
Not all of these models are available. for example, the GPT-4 base model was never released.
The GPT-4 model that you might be interacting with over API is not a base model, it's an assistant model, and we're going to cover how to get those in a bit.
GPT-3 based model is available via the API under the name Devanshi and GPT-2 based model
is available even as weights on our GitHub repo. But currently the best available base model
probably is the LLaMA series from Meta, although it is not commercially licensed.
Now, one thing to point out is base models are not assistants. They don't want to make answers to your questions,
they want to complete documents. If you tell them to write a poem about the bread and cheese,
it will answer questions with more questions, it's completing what it thinks is a document.
However, you can prompt them in a specific way for base models that is more likely to work.
As an example, here's a poem about bread and cheese, and in that case it will autocomplete correctly. You can even trick base models into being assistants.
The way you would do this is you would create a specific few-shot prompt that makes it look like there's some document between the human and assistant
and they're exchanging information. Then at the bottom, you put your query at the end and the base model
will condition itself into being a helpful assistant and answer,
but this is not very reliable and doesn't work super well in practice, although it can be done. Instead, we have a different path to make
actual GPT assistants not base model document completers. That takes us into supervised finetuning.
In the supervised finetuning stage, we are going to collect small but high quality data-sets, and in this case,
we're going to ask human contractors to gather data of the form prompt and ideal response.
We're going to collect lots of these typically tens of thousands or something like that. Then we're going to still do language
modeling on this data. Nothing changed algorithmically, we're swapping out a training set. It used to be Internet documents,
which has a high quantity local for basically Q8 prompt response data.
That is low quantity, high quality. We will still do language modeling and then after training,
we get an SFT model. You can actually deploy these models and they are actual assistants and they work to some extent.
Let me show you what an example demonstration might look like. Here's something that a human contractor might come up with.
Here's some random prompt. Can you write a short introduction about the relevance of the term monopsony or something like that?
Then the contractor also writes out an ideal response. When they write out these responses, they are following extensive labeling
documentations and they are being asked to be helpful, truthful, and harmless.
These labeling instructions here, you probably can't read it, neither can I, but they're long and this is people
following instructions and trying to complete these prompts. That's what the dataset looks like. You can train these models. This works to some extent.
Now, you can actually continue the pipeline from here on, and go into RLHF,
reinforcement learning from human feedback that consists of both reward modeling and reinforcement learning.
Let me cover that and then I'll come back to why you may want to go through the extra steps and how that compares to SFT models.
In the reward modeling step, what we're going to do is we're now going to shift our data collection to be of the form of comparisons.
Here's an example of what our dataset will look like. I have the same identical prompt on the top,
RM Dataset
which is asking the assistant to write a program or a function that checks if a given string is a palindrome.
Then what we do is we take the SFT model which we've already trained and we create multiple completions.
In this case, we have three completions that the model has created, and then we ask people to rank these completions.
If you stare at this for a while, and by the way, these are very difficult things to do to compare some of these predictions.
This can take people even hours for a single prompt completion pairs,
but let's say we decided that one of these is much better than the others and so on. We rank them.
Then we can follow that with something that looks very much like a binary classification on all the possible pairs between these completions.
RM Training
What we do now is, we lay out our prompt in rows, and the prompt is identical across all three rows here.
It's all the same prompt, but the completion of this varies. The yellow tokens are coming from the SFT model.
Then what we do is we append another special reward readout token at the end and we basically only
supervise the transformer at this single green token. The transformer will predict some reward
for how good that completion is for that prompt and basically it makes
a guess about the quality of each completion. Then once it makes a guess for every one of them,
we also have the ground truth which is telling us the ranking of them. We can actually enforce that some of
these numbers should be much higher than others, and so on. We formulate this into a loss function and we train our model to make reward predictions
that are consistent with the ground truth coming from the comparisons from all these contractors. That's how we train our reward model.
That allows us to score how good a completion is for a prompt. Once we have a reward model,
we can't deploy this because this is not very useful as an assistant by itself, but it's very useful for the reinforcement
learning stage that follows now. Because we have a reward model, we can score the quality of any arbitrary completion for any given prompt.
What we do during reinforcement learning is we basically get, again, a large collection of prompts and now we do
reinforcement learning with respect to the reward model. Here's what that looks like. We take a single prompt,
we lay it out in rows, and now we use basically the model we'd like to train which
was initialized at SFT model to create some completions in yellow, and then we append the reward token again
and we read off the reward according to the reward model, which is now kept fixed. It doesn't change any more. Now the reward model
tells us the quality of every single completion for all these prompts and so what we can do is we can now just basically apply the same
language modeling loss function, but we're currently training on the yellow tokens, and we are weighing
the language modeling objective by the rewards indicated by the reward model. As an example, in the first row,
the reward model said that this is a fairly high-scoring completion and so all the tokens that we
happen to sample on the first row are going to get reinforced and they're going to get higher probabilities for the future.
Conversely, on the second row, the reward model really did not like this completion, -1.2. Therefore, every single token that we sampled in
that second row is going to get a slightly higher probability for the future. We do this over and over on many prompts on many batches and basically,
we get a policy that creates yellow tokens here. It's basically all the completions here will
score high according to the reward model that we trained in the previous stage.
That's what the RLHF pipeline is. Then at the end, you get a model that you could deploy.
As an example, ChatGPT is an RLHF model, but some other models that you might come across for example,
Vicuna-13B, and so on, these are SFT models. We have base models, SFT models, and RLHF models.
That's the state of things there. Now why would you want to do RLHF? One answer that's not
that exciting is that it works better. This comes from the instruct GPT paper. According to these experiments a while ago now,
these PPO models are RLHF. We see that they are basically preferred in a lot
of comparisons when we give them to humans. Humans prefer basically tokens
that come from RLHF models compared to SFT models, compared to base model that is prompted to be an assistant. It just works better.
But you might ask why does it work better? I don't think that there's a single amazing answer
that the community has really agreed on, but I will offer one reason potentially.
It has to do with the asymmetry between how easy computationally it is to compare versus generate.
Let's take an example of generating a haiku. Suppose I ask a model to write a haiku about paper clips.
If you're a contractor trying to train data, then imagine being a contractor collecting basically data for the SFT stage,
how are you supposed to create a nice haiku for a paper clip? You might not be very good at that, but if I give you a few examples of
haikus you might be able to appreciate some of these haikus a lot more than others. Judging which one of these is good is a much easier task.
Basically, this asymmetry makes it so that comparisons are a better way to potentially leverage
yourself as a human and your judgment to create a slightly better model. Now, RLHF models are not
strictly an improvement on the base models in some cases. In particular, we'd notice for example that they lose some entropy.
That means that they give more peaky results. They can output samples
Mode collapse
with lower variation than the base model. The base model has lots of entropy and will give lots of diverse outputs.
For example, one place where I still prefer to use a base model is in the setup
where you basically have n things and you want to generate more things like it.
Here is an example that I just cooked up. I want to generate cool Pokemon names.
I gave it seven Pokemon names and I asked the base model to complete the document and it gave me a lot more Pokemon names.
These are fictitious. I tried to look them up. I don't believe they're actual Pokemons. This is the task that I think the base model would be
good at because it still has lots of entropy. It'll give you lots of diverse cool more things that look like whatever you give it before.
Having said all that, these are the assistant models that are probably available to you at this point.
There was a team at Berkeley that ranked a lot of the available assistant models and give them basically Elo ratings.
Currently, some of the best models, of course, are GPT-4, by far, I would say, followed by Claude, GPT-3.5, and then a number of models,
some of these might be available as weights, like Vicuna, Koala, etc. The first three rows here are
all RLHF models and all of the other models to my knowledge, are SFT models, I believe.
That's how we train these models on the high level. Now I'm going to switch gears and let's look at how we can
best apply the GPT assistant model to your problems. Now, I would like to work
in setting of a concrete example. Let's work with a concrete example here.
Let's say that you are working on an article or a blog post, and you're going to write this sentence at the end.
"California's population is 53 times that of Alaska." So for some reason, you want to compare the populations of these two states.
Think about the rich internal monologue and tool use and how much work actually goes computationally in
your brain to generate this one final sentence. Here's maybe what that could look like in your brain.
For this next step, let me blog on my blog, let me compare these two populations.
First I'm going to obviously need to get both of these populations. Now, I know that I probably
don't know these populations off the top of my head so I'm aware of what I know or don't know of my self-knowledge.
I go, I do some tool use and I go to Wikipedia and I look up California's population and Alaska's population.
Now, I know that I should divide the two, but again, I know that dividing 39.2 by 0.74 is very unlikely to succeed.
That's not the thing that I can do in my head and so therefore, I'm going to rely on the calculator so I'm going to use a calculator,
punch it in and see that the output is roughly 53. Then maybe I do some reflection and sanity checks in
my brain so does 53 makes sense? Well, that's quite a large fraction, but then California is the most
populous state, so maybe that looks okay. Then I have all the information I might need, and now I get to the creative portion of writing.
I might start to write something like "California has 53x times greater" and then I think to myself,
that's actually like really awkward phrasing so let me actually delete that and let me try again.
As I'm writing, I have this separate process, almost inspecting what I'm writing and judging whether it looks good
or not and then maybe I delete and maybe I reframe it, and then maybe I'm happy with what comes out.
Basically long story short, a ton happens under the hood in terms of your internal monologue when you create sentences like this.
But what does a sentence like this look like when we are training a GPT on it? From GPT's perspective, this
is just a sequence of tokens. GPT, when it's reading or generating these tokens,
it just goes chunk, chunk, chunk, chunk and each chunk is roughly the same amount of computational work for each token.
These transformers are not very shallow networks they have about 80 layers of reasoning,
but 80 is still not like too much. This transformer is going to do its best to imitate,
but of course, the process here looks very different from the process that you took. In particular, in our final artifacts
in the data sets that we create, and then eventually feed to LLMs, all that internal dialogue was completely stripped and unlike you,
the GPT will look at every single token and spend the same amount of compute on every one of them. So, you can't expect it
to do too much work per token and also in particular,
basically these transformers are just like token simulators, they don't know what they don't know.
They just imitate the next token. They don't know what they're good at or not good at. They just tried their best to imitate the next token.
They don't reflect in the loop. They don't sanity check anything. They don't correct their mistakes along the way.
By default, they just are sample token sequences. They don't have separate inner monologue streams
in their head right? They're evaluating what's happening. Now, they do have some cognitive advantages,
I would say and that is that they do actually have a very large fact-based knowledge across a vast number of areas because they have,
say, several, 10 billion parameters. That's a lot of storage for a lot of facts. They also, I think have
a relatively large and perfect working memory. Whatever fits into the context window
is immediately available to the transformer through its internal self attention mechanism and so it's perfect memory,
but it's got a finite size, but the transformer has a very direct access to it and so it can a losslessly remember anything that
is inside its context window. This is how I would compare those two and the reason I bring all of this up is because I
think to a large extent, prompting is just making up for this cognitive difference between
these two architectures like our brains here and LLM brains.
You can look at it that way almost. Here's one thing that people found for example works pretty well in practice.
Especially if your tasks require reasoning, you can't expect the transformer to do too much reasoning per token.
You have to really spread out the reasoning across more and more tokens. For example, you can't give a transformer
a very complicated question and expect it to get the answer in a single token. There's just not enough time for it. "These transformers need tokens to
think," I like to say sometimes. This is some of the things that work well, you may for example have a few-shot prompt that
shows the transformer that it should show its work when it's answering question and if you give a few examples,
the transformer will imitate that template and it will just end up working out better in terms of its evaluation.
Additionally, you can elicit this behavior from the transformer by saying, let things step-by-step.
Because this conditions the transformer into showing its work and because
it snaps into a mode of showing its work, is going to do less computational work per token.
It's more likely to succeed as a result because it's making slower reasoning over time.
Here's another example, this one is called self-consistency. We saw that we had the ability
Ensemble multiple attempts
to start writing and then if it didn't work out, I can try again and I can try multiple times
and maybe select the one that worked best. In these approaches,
you may sample not just once, but you may sample multiple times and then have some process for finding
the ones that are good and then keeping just those samples or doing a majority vote or something like that. Basically these transformers in the process as
they predict the next token, just like you, they can get unlucky and they could sample a not a very good
token and they can go down like a blind alley in terms of reasoning. Unlike you, they cannot recover from that.
They are stuck with every single token they sample and so they will continue the sequence, even if they know that this sequence is not going to work out.
Give them the ability to look back, inspect or try to basically sample around it.
Here's one technique also, it turns out that actually LLMs, they know when they've screwed up,
Ask for reflection
so as an example, say you ask the model to generate a poem that does not
rhyme and it might give you a poem, but it actually rhymes. But it turns out that especially for the bigger models like GPT-4,
you can just ask it "did you meet the assignment?" Actually GPT-4 knows very well that it did not meet the assignment.
It just got unlucky in its sampling. It will tell you, "No, I didn't actually meet the assignment here. Let me try again."
But without you prompting it it doesn't know to revisit and so on.
You have to make up for that in your prompts, and you have to get it to check, if you don't ask it to check,
its not going to check by itself it's just a token simulator.
I think more generally, a lot of these techniques fall into the bucket of what I would say recreating our System 2.
You might be familiar with the System 1 and System 2 thinking for humans. System 1 is a fast automatic process and I
think corresponds to an LLM just sampling tokens. System 2 is the slower deliberate
planning part of your brain. This is a paper actually from
just last week because this space is pretty quickly evolving, it's called Tree of Thought.
The authors of this paper proposed maintaining multiple completions for any given prompt
and then they are also scoring them along the way and keeping the ones that are going well if that makes sense.
A lot of people are really playing around with prompt engineering
to basically bring back some of these abilities that we have in our brain for LLMs.
Now, one thing I would like to note here is that this is not just a prompt. This is actually prompts that are together
used with some Python Glue code because you actually have to maintain multiple prompts and you also have to do
some tree search algorithm here to figure out which prompts to expand, etc. It's a symbiosis of Python Glue code and
individual prompts that are called in a while loop or in a bigger algorithm. I also think there's a really cool
parallel here to AlphaGo. AlphaGo has a policy for placing the next stone when it plays go,
and its policy was trained originally by imitating humans. But in addition to this policy,
it also does Monte Carlo Tree Search. Basically, it will play out a number of possibilities in its head and evaluate all of
them and only keep the ones that work well. I think this is an equivalent of AlphaGo but for text if that makes sense.
Just like Tree of Thought, I think more generally people are starting to really explore
more general techniques of not just the simple question-answer prompts, but something that looks a lot more like
Python Glue code stringing together many prompts. On the right, I have an example from this paper called React where they
structure the answer to a prompt as a sequence of thought-action-observation,
thought-action-observation, and it's a full rollout and a thinking process to answer the query.
In these actions, the model is also allowed to tool use. On the left, I have an example of AutoGPT.
Now AutoGPT by the way is a project that I think got a lot of hype recently,
but I think I still find it inspirationally interesting. It's a project that allows an LLM to keep
the task list and continue to recursively break down tasks. I don't think this currently works very well and I would
not advise people to use it in practical applications. I just think it's something to generally take inspiration
from in terms of where this is going, I think over time. That's like giving our model System 2 thinking.
The next thing I find interesting is, this following serve I would say almost psychological quirk of LLMs,
is that LLMs don't want to succeed, they want to imitate. You want to succeed, and you should ask for it.
What I mean by that is, when transformers are trained, they have training sets and there can be
an entire spectrum of performance qualities in their training data. For example, there could be some kind of a prompt
for some physics question or something like that, and there could be a student's solution that is completely wrong but there can also be an expert
answer that is extremely right. Transformers can't tell the difference between low,
they know about low-quality solutions and high-quality solutions, but by default, they want to imitate all of
it because they're just trained on language modeling. At test time, you actually have to ask for a good performance.
In this example in this paper, they tried various prompts. Let's think step-by-step was very powerful
because it spread out the reasoning over many tokens. But what worked even better is, let's work this out in a step-by-step way
to be sure we have the right answer. It's like conditioning on getting the right answer, and this actually makes the transformer work
better because the transformer doesn't have to now hedge its probability mass on low-quality solutions,
as ridiculous as that sounds. Basically, feel free to ask for a strong solution.
Say something like, you are a leading expert on this topic. Pretend you have IQ 120, etc. But don't try to ask for too much IQ because if
you ask for IQ 400, you might be out of data distribution, or even worse, you could be in data distribution for
something like sci-fi stuff and it will start to take on some sci-fi, or like roleplaying or something like that.
You have to find the right amount of IQ. I think it's got some U-shaped curve there.
Next up, as we saw when we are trying to solve problems, we know what we are good at and what we're not good at,
and we lean on tools computationally. You want to do the same potentially with your LLMs.
Tool use / Plugins
In particular, we may want to give them calculators, code interpreters,
and so on, the ability to do search, and there's a lot of techniques for doing that.
One thing to keep in mind, again, is that these transformers by default may not know what they don't know.
You may even want to tell the transformer in a prompt you are not very good at mental arithmetic. Whenever you need to do very large number addition,
multiplication, or whatever, instead, use this calculator. Here's how you use the calculator, you use this token combination, etc.
You have to actually spell it out because the model by default doesn't know what it's good at or not good at, necessarily, just like you and I might be.
Next up, I think something that is very interesting is we went from a world that was retrieval only all the way,
the pendulum has swung to the other extreme where its memory only in LLMs. But actually, there's this entire space in-between of
these retrieval-augmented models and this works extremely well in practice. As I mentioned, the context window of
a transformer is its working memory. If you can load the working memory with any information that is relevant to the task,
the model will work extremely well because it can immediately access all that memory. I think a lot of people are really interested
in basically retrieval-augment degeneration. On the bottom, I have an example of LlamaIndex which is
one data connector to lots of different types of data. You can index all
of that data and you can make it accessible to LLMs. The emerging recipe there is you take relevant documents,
you split them up into chunks, you embed all of them, and you basically get embedding vectors that represent that data.
You store that in the vector store and then at test time, you make some kind of a query to your vector store and you fetch chunks that
might be relevant to your task and you stuff them into the prompt and then you generate. This can work quite well in practice.
This is, I think, similar to when you and I solve problems. You can do everything from your memory and
transformers have very large and extensive memory, but also it really helps to reference some primary documents.
Whenever you find yourself going back to a textbook to find something, or whenever you find yourself going back to documentation of the library to look something up,
transformers definitely want to do that too. You have some memory over how
some documentation of the library works but it's much better to look it up. The same applies here.
Next, I wanted to briefly talk about constraint prompting. I also find this very interesting.
This is basically techniques for forcing a certain template in the outputs of LLMs.
Guidance is one example from Microsoft actually. Here we are enforcing that the output from the LLM will be JSON.
This will actually guarantee that the output will take on this form because they go in and they mess with the probabilities of
all the different tokens that come out of the transformer and they clamp those tokens and then the transformer is only filling in the blanks here,
and then you can enforce additional restrictions on what could go into those blanks. This might be really helpful, and I think
this constraint sampling is also extremely interesting. I also want to say
a few words about fine tuning. It is the case that you can get really far with prompt engineering, but it's also possible to
think about fine tuning your models. Now, fine tuning models means that you are actually going to change the weights of the model.
It is becoming a lot more accessible to do this in practice, and that's because of a number of techniques that have been
developed and have libraries for very recently. So for example parameter efficient fine tuning techniques like Laura,
make sure that you're only training small, sparse pieces of your model. So most of the model is kept clamped at
the base model and some pieces of it are allowed to change and this still works pretty well empirically and makes
it much cheaper to tune only small pieces of your model. It also means that because most of your model is clamped,
you can use very low precision inference for computing those parts because you are not going to be updated by
gradient descent and so that makes everything a lot more efficient as well. And in addition, we have a number of open source, high-quality base models.
Currently, as I mentioned, I think LLaMa is quite nice, although it is not commercially licensed, I believe right now.
Some things to keep in mind is that basically fine tuning is a lot more technically involved.
It requires a lot more, I think, technical expertise to do right. It requires human data contractors for
datasets and/or synthetic data pipelines that can be pretty complicated. This will definitely slow down
your iteration cycle by a lot, and I would say on a high level SFT is achievable because you're continuing
the language modeling task. It's relatively straightforward, but RLHF, I would say is very much research territory
and is even much harder to get to work, and so I would probably not advise that someone just tries to roll their own RLHF of implementation.
These things are pretty unstable, very difficult to train, not something that is, I think, very beginner friendly right now,
and it's also potentially likely also to change pretty rapidly still.
So I think these are my default recommendations right now. I would break up your task into two major parts.
Default recommendations
Number 1, achieve your top performance, and Number 2, optimize your performance in that order.
Number 1, the best performance will currently come from GPT-4 model. It is the most capable of all by far.
Use prompts that are very detailed. They have lots of task content, relevant information and instructions.
Think along the lines of what would you tell a task contractor if they can't email you back, but then also keep in mind that a task contractor is a
human and they have inner monologue and they're very clever, etc. LLMs do not possess those qualities.
So make sure to think through the psychology of the LLM almost and cater prompts to that.
Retrieve and add any relevant context and information to these prompts. Basically refer to a lot of
the prompt engineering techniques. Some of them I've highlighted in the slides above, but also this is a very large space and I would
just advise you to look for prompt engineering techniques online. There's a lot to cover there.
Experiment with few-shot examples. What this refers to is, you don't just want to tell, you want to show whenever it's possible.
So give it examples of everything that helps it really understand what you mean if you can.
Experiment with tools and plug-ins to offload tasks that are difficult for LLMs natively,
and then think about not just a single prompt and answer, think about potential chains and reflection and how you glue
them together and how you can potentially make multiple samples and so on. Finally, if you think you've squeezed
out prompt engineering, which I think you should stick with for a while, look at some potentially
fine tuning a model to your application, but expect this to be a lot more slower in the vault and then
there's an expert fragile research zone here and I would say that is RLHF, which currently does work a bit
better than SFT if you can get it to work. But again, this is pretty involved, I would say. And to optimize your costs,
try to explore lower capacity models or shorter prompts and so on.
I also wanted to say a few words about the use cases in which I think LLMs are currently well suited for.
In particular, note that there's a large number of limitations to LLMs today, and so I would keep that
definitely in mind for all of your applications. Models, and this by the way could be an entire talk. So I don't have time to cover it in full detail.
Models may be biased, they may fabricate, hallucinate information, they may have reasoning errors, they may struggle in entire classes of applications,
they have knowledge cut-offs, so they might not know any information above, say, September, 2021.
They are susceptible to a large range of attacks which are coming out on Twitter daily,
including prompt injection, jailbreak attacks, data poisoning attacks and so on. So my recommendation right now is
use LLMs in low-stakes applications. Combine them always with human oversight.
Use them as a source of inspiration and suggestions and think co-pilots, instead of completely autonomous agents
that are just like performing a task somewhere. It's just not clear that the models are there right now.
So I wanted to close by saying that GPT-4 is an amazing artifact. I'm very thankful that it exists, and it's beautiful.
It has a ton of knowledge across so many areas. It can do math, code and so on. And in addition, there's this
thriving ecosystem of everything else that is being built and incorporated into the ecosystem. Some of these things I've talked about,
and all of this power is accessible at your fingertips. So here's everything that's needed in terms of
code to ask GPT-4 a question, to prompt it, and get a response. In this case, I said,
can you say something to inspire the audience of Microsoft Build 2023? And I just punched this into Python and verbatim
GPT-4 said the following: And by the way, I did not know that they
used this trick in the keynote. So I thought I was being clever, but it is really good at this.
It says, ladies and gentlemen, innovators and trailblazers Microsoft Build 2023. Welcome to the gathering of brilliant
minds like no other, you are the architects of the future, the visionaries molding the digital realm
in which humanity thrives. Embrace the limitless possibilities of technologies and let your ideas soar as high as your imagination.
Together, let's create a more connected, remarkable, and inclusive world for generations to come. Get ready to unleash your creativity,
canvas the unknown, and turn dreams into reality. Your journey begins today!





intro: Tokenization, GPT-2 paper, tokenization-related issues
hi everyone so in this video I'd like us to cover the process of tokenization in large language models now you see here
that I have a set face and that's because uh tokenization is my least favorite part of working with large
language models but unfortunately it is necessary to understand in some detail because it it is fairly hairy gnarly and
there's a lot of hidden foot guns to be aware of and a lot of oddness with large language models typically traces back to
tokenization so what is tokenization now in my previous video Let's Build GPT from scratch uh we
actually already did tokenization but we did a very naive simple version of tokenization so when you go to the
Google colab for that video uh you see here that we loaded our training set and
our training set was this uh Shakespeare uh data set now in the beginning the Shakespeare data set is just a large
string in Python it's just text and so the question is how do we plug text into
large language models and in this case here we created a vocabulary of 65
possible characters that we saw occur in this string these were the possible characters and we saw that there are 65
of them and then we created a a lookup table for converting from every possible
character a little string piece into a token an integer so here for example we tokenized
the string High there and we received this sequence of tokens and here we took the first 1,000
characters of our data set and we encoded it into tokens and because it is this is character level we received
1,000 tokens in a sequence so token 18 47
Etc now later we saw that the way we plug these tokens into the language
model is by using an embedding table and so basically if we have 65
possible tokens then this embedding table is going to have 65 rows and roughly speaking we're taking the
integer associated with every single sing Le token we're using that as a lookup into this table and we're
plucking out the corresponding row and this row is a uh is trainable parameters
that we're going to train using back propagation and this is the vector that then feeds into the Transformer um and
that's how the Transformer Ser of perceives every single token so here we had a very naive
tokenization process that was a character level tokenizer but in practice in state-ofthe-art uh language
models people use a lot more complicated schemes unfortunately uh for constructing these uh token
vocabularies so we're not dealing on the Character level we're dealing on chunk level and the way these um character
chunks are constructed is using algorithms such as for example the bik pair in coding algorithm which we're
going to go into in detail um and cover in this video I'd like to briefly show
you the paper that introduced a bite level encoding as a mechanism for tokenization in the context of large
language models and I would say that that's probably the gpt2 paper and if you scroll down here to the section
input representation this is where they cover tokenization the kinds of properties that you'd like the tokenization to have and they conclude
here that they're going to have a tokenizer where you have a vocabulary of 50,2 57 possible
tokens and the context size is going to be 1,24 tokens so in the in in the
attention layer of the Transformer neural network every single token is attending to the previous tokens in the sequence and it's
going to see up to 1,24 tokens so tokens are this like fundamental unit um the
atom of uh large language models if you will and everything is in units of tokens everything is about tokens and
tokenization is the process for translating strings or text into sequences of tokens and uh vice versa
when you go into the Llama 2 paper as well I can show you that when you search token you're going to get get 63 hits um
and that's because tokens are again pervasive so here they mentioned that they trained on two trillion tokens of
data and so on so we're going to build our own tokenizer luckily the bite be encoding
algorithm is not uh that super complicated and we can build it from scratch ourselves and we'll see exactly
how this works before we dive into code I'd like to give you a brief Taste of some of the complexities that come from
the tokenization because I just want to make sure that we motivate it sufficiently for why we are doing all
this and why this is so gross so tokenization is at the heart of a lot of weirdness in large language models and I
would advise that you do not brush it off a lot of the issues that may look like just issues with the new network
architecture or the large language model itself are actually issues with the tokenization and fundamentally Trace uh
back to it so if you've noticed any issues with large language models can't
you know not able to do spelling tasks very easily that's usually due to tokenization simple string processing
can be difficult for the large language model to perform natively uh non-english languages can
work much worse and to a large extent this is due to tokenization sometimes llms are bad at
simple arithmetic also can trace be traced to tokenization uh gbt2 specifically would
have had quite a bit more issues with python than uh future versions of it due to tokenization there's a lot of other
issues maybe you've seen weird warnings about a trailing whites space this is a tokenization issue um
if you had asked GPT earlier about solid gold Magikarp and what it is you would see the llm go totally crazy and it
would start going off about a completely unrelated tangent topic maybe you've been told to use yl over Json in
structure data all of that has to do with tokenization so basically tokenization is at the heart of many
issues I will look back around to these at the end of the video but for now let me just um skip over it a little bit and
tokenization by example in a Web UI (tiktokenizer)
let's go to this web app um the Tik tokenizer bell.app so I have it loaded
here and what I like about this web app is that tokenization is running a sort of live in your browser in JavaScript so
you can just type here stuff hello world and the whole string rokenes so here what we see on uh the
left is a string that you put in on the right we're currently using the gpt2 tokenizer we see that this string that I
pasted here is currently tokenizing into 300 tokens and here they are sort of uh
shown explicitly in different colors for every single token so for example uh this word tokenization became two tokens
the token 3,642 and
1,634 the token um space is is token 318
so be careful on the bottom you can show white space and keep in mind that there are spaces and uh sln new line
characters in here but you can hide them for clarity the token space at is token 379
the to the Token space the is 262 Etc so
you notice here that the space is part of that uh token chunk now so this is kind of like how
our English sentence broke up and that seems all well and good now now here I
put in some arithmetic so we see that uh the token 127 Plus and then token six
space 6 followed by 77 so what's happening here is that 127 is feeding in as a single token into the large
language model but the um number 677 will actually feed in as two separate
tokens and so the large language model has to sort of um take account of that
and process it correctly in its Network and see here 804 will be broken up into
two tokens and it's is all completely arbitrary and here I have another example of four-digit numbers and they
break up in a way that they break up and it's totally arbitrary sometimes you have um multiple digits single token
sometimes you have individual digits as many tokens and it's all kind of pretty arbitrary and coming out of the
tokenizer here's another example we have the string egg and you see here that
this became two tokens but for some reason when I say I have an egg you see when it's a space
egg it's two token it's sorry it's a single token so just egg by itself in
the beginning of a sentence is two tokens but here as a space egg is suddenly a single token uh for the exact
same string okay here lowercase egg turns out to be a single token and in
particular notice that the color is different so this is a different token so this is case sensitive and of course
a capital egg would also be different tokens and again um this would be two
tokens arbitrarily so so for the same concept egg depending on if it's in the beginning of a sentence at the end of a
sentence lowercase uppercase or mixed all this will be uh basically very different tokens and different IDs and
the language model has to learn from raw data from all the internet text that it's going to be training on that these are actually all the exact same concept
and it has to sort of group them in the parameters of the neural network and understand just based on the data
patterns that these are all very similar but maybe not almost exactly similar but but very very similar
um after the EG demonstration here I have um an introduction from open a eyes
chbt in Korean so manaso Pang uh Etc uh
so this is in Korean and the reason I put this here is because you'll notice
that um non-english languages work slightly worse in Chachi part of this is
because of course the training data set for Chachi is much larger for English and for everything else but the same is
true not just for the large language model itself but also for the tokenizer so when we train the tokenizer we're
going to see that there's a training set as well and there's a lot more English than non-english and what ends up
happening is that we're going to have a lot more longer tokens for
English so how do I put this if you have a single sentence in English and you tokenize it you might see that it's 10
tokens or something like that but if you translate that sentence into say Korean or Japanese or something else you'll
typically see that the number of tokens used is much larger and that's because the chunks here are a lot more broken up
so we're using a lot more tokens for the exact same thing and what this does is it bloats up the sequence length of all
the documents so you're using up more tokens and then in the attention of the Transformer when these tokens try to
attend each other you are running out of context um in the maximum context length
of that Transformer and so basically all the non-english text is stretched out
from the perspective of the Transformer and this just has to do with the um trainings that used for the tokenizer
and the tokenization itself so it will create a lot bigger tokens and a lot larger groups in English and it will
have a lot of little boundaries for all the other non-english text um so if we
translated this into English it would be significantly fewer tokens the final example I have here is
a little snippet of python for doing FS buuz and what I'd like you to notice is
look all these individual spaces are all separate tokens they are token
220 so uh 220 220 220 220 and then space
if is a single token and so what's going on here is that when the Transformer is going to consume or try to uh create
this text it needs to um handle all these spaces individually they all feed
in one by one into the entire Transformer in the sequence and so this is being extremely wasteful tokenizing
it in this way and so as a result of that gpt2 is not very good with python
and it's not anything to do with coding or the language model itself it's just that if he use a lot of indentation
using space in Python like we usually do uh you just end up bloating out all the
text and it's separated across way too much of the sequence and we are running out of the context length in the
sequence uh that's roughly speaking what's what's happening we're being way too wasteful we're taking up way too much token space now we can also scroll
up here and we can change the tokenizer so note here that gpt2 tokenizer creates a token count of 300 for this string
here we can change it to CL 100K base which is the GPT for tokenizer and we
see that the token count drops to 185 so for the exact same string we are now roughly having the number of tokens and
roughly speaking this is because uh the number of tokens in the GPT 4 tokenizer is roughly double that of the number of
tokens in the gpt2 tokenizer so we went went from roughly 50k to roughly 100K now you can imagine that this is a good
thing because the same text is now squished into half as many tokens so uh
this is a lot denser input to the Transformer and in the Transformer every
single token has a finite number of tokens before it that it's going to pay attention to and so what this is doing is we're roughly able to see twice as
much text as a context for what token to predict next uh because of this change
but of course just increasing the number of tokens is uh not strictly better infinitely uh because as you increase
the number of tokens now your embedding table is um sort of getting a lot larger and also at the output we are trying to
predict the next token and there's the soft Max there and that grows as well we're going to go into more detail later on this but there's some kind of a Sweet
Spot somewhere where you have a just right number of tokens in your vocabulary where everything is
appropriately dense and still fairly efficient now one thing I would like you to note specifically for the gp4
tokenizer is that the handling of the white space for python has improved a
lot you see that here these four spaces are represented as one single token for the three spaces here and then the token
SPF and here seven spaces were all grouped into a single token so we're
being a lot more efficient in how we represent Python and this was a deliberate Choice made by open aai when they designed the gp4 tokenizer and they
group a lot more space into a single character what this does is this densifies Python and therefore we can
attend to more code before it when we're trying to predict the next token in the sequence and so the Improvement in the
python coding ability from gbt2 to gp4 is not just a matter of the language
model and the architecture and the details of the optimization but a lot of the Improvement here is also coming from
the design of the tokenizer and how it groups characters into tokens okay so let's now start writing some code
strings in Python, Unicode code points
so remember what we want to do we want to take strings and feed them into language models for that we need to
somehow tokenize strings into some integers in some fixed vocabulary and
then we will use those integers to make a look up into a lookup table of vectors and feed those vectors into the
Transformer as an input now the reason this gets a little bit tricky of course is that we don't just want to support
the simple English alphabet we want to support different kinds of languages so this is anango in Korean which is hello
and we also want to support many kinds of special characters that we might find on the internet for example
Emoji so how do we feed this text into uh Transformers well how's the what is this
text anyway in Python so if you go to the documentation of a string in Python
you can see that strings are immutable sequences of Unicode code points okay what are Unicode code points
we can go to PDF so Unicode code points are defined by the Unicode Consortium as
part of the Unicode standard and what this is really is that it's just a definition of roughly 150,000 characters
right now and roughly speaking what they look like and what integers um represent
those characters so it says 150,000 characters across 161 scripts as of
right now so if you scroll down here you can see that the standard is very much alive the latest standard 15.1 in
September 2023 and basically this is just a way to
define lots of types of characters like for example all these
characters across different scripts so the way we can access the unic code code Point given Single Character is by using
the or function in Python so for example I can pass in Ord of H and I can see
that for the Single Character H the unic code code point is
104 okay um but this can be arbitr complicated so we can take for example
our Emoji here and we can see that the code point for this one is 128,000 or we can take
un and this is 50,000 now keep in mind you can't plug in strings here because
you uh this doesn't have a single code point it only takes a single uni code code Point character and tells you its
integer so in this way we can look up all the um characters of this
specific string and their code points so or of X forx in this string and we get
this encoding here now see here we've already turned the raw code points
already have integers so why can't we simply just use these integers and not have any tokenization at all why can't
we just use this natively as is and just use the code Point well one reason for that of course is that the vocabulary in
that case would be quite long so in this case for Unicode the this is a vocabulary of
150,000 different code points but more worryingly than that I think the Unicode
standard is very much alive and it keeps changing and so it's not kind of a stable representation necessarily that
we may want to use directly so for those reasons we need something a bit better so to find something better we turn to
Unicode byte encodings, ASCII, UTF-8, UTF-16, UTF-32
encodings so if we go to the Wikipedia page here we see that the Unicode consortion defines three types of
encodings utf8 UTF 16 and UTF 32 these encoding are the way by which we can
take Unicode text and translate it into binary data or by streams utf8 is by far
the most common uh so this is the utf8 page now this Wikipedia page is actually quite long but what's important for our
purposes is that utf8 takes every single Cod point and it translates it to a by
stream and this by stream is between one to four bytes so it's a variable length encoding so depending on the Unicode
Point according to the schema you're going to end up with between 1 to four bytes for each code point on top of that
there's utf8 uh utf16 and UTF 32 UTF 32 is nice because
it is fixed length instead of variable length but it has many other downsides as well so the full kind of spectrum of
pros and cons of all these different three encodings are beyond the scope of this video I just like to point out that
I enjoyed this block post and this block post at the end of it also has a number of references that can be quite useful
uh one of them is uh utf8 everywhere Manifesto um and this Manifesto
describes the reason why utf8 is significantly preferred and a lot nicer
than the other encodings and why it is used a lot more prominently um on the
internet one of the major advantages just just to give you a sense is that utf8 is the only one of these that is
backwards compatible to the much simpler asky encoding of text um but I'm not
going to go into the full detail in this video so suffice to say that we like the utf8 encoding and uh let's try to take
the string and see what we get if we encoded into utf8 the string class in Python actually
has do encode and you can give it the encoding which is say utf8 now we get out of this is not very nice because
this is the bytes is a bytes object and it's not very nice in the way that it's printed so I personally like to take it
through list because then we actually get the raw B of this uh encoding so this is the raw
byes that represent this string according to the utf8 en coding we can
also look at utf16 we get a slightly different by stream and we here we start
to see one of the disadvantages of utf16 you see how we have zero Z something Z something Z something we're starting to
get a sense that this is a bit of a wasteful encoding and indeed for simple asky characters or English characters
here uh we just have the structure of 0 something Z something and it's not exactly nice same for UTF 32 when we
expand this we can start to get a sense of the wastefulness of this encoding for our purposes you see a lot of zeros
followed by something and so uh this is not desirable so suffice it to say that we
would like to stick with utf8 for our purposes however if we just use utf8
naively these are by streams so that would imply a vocabulary length of only
256 possible tokens uh but this this vocabulary size is very very small what
this is going to do if we just were to use it naively is that all of our text would be stretched out over very very
long sequences of bytes and so um what what this does is that certainly
the embeding table is going to be tiny and the prediction at the top at the final layer is going to be very tiny but our sequences are very long and remember
that we have pretty finite um context length and the attention that we can support in a transformer for
computational reasons and so we only have as much context length but now we have very very long sequences and this
is just inefficient and it's not going to allow us to attend to sufficiently long text uh before us for the purposes
of the next token prediction task so we don't want to use the raw bytes of the
utf8 encoding we want to be able to support larger vocabulary size that we
can tune as a hyper but we want to stick with the utf8 encoding of these strings so what do we
do well the answer of course is we turn to the bite pair encoding algorithm which will allow us to compress these
bite sequences um to a variable amount so we'll get to that in a bit but I just
want to briefly speak to the fact that I would love nothing more than to be able to feed raw bite sequences into uh
daydreaming: deleting tokenization
language models in fact there's a paper about how this could potentially be done uh from Summer last last year now the
problem is you actually have to go in and you have to modify the Transformer architecture because as I mentioned
you're going to have a problem where the attention will start to become extremely expensive because the sequences are so
long and so in this paper they propose kind of a hierarchical structuring of
the Transformer that could allow you to just feed in raw bites and so at the end they say together these results
establish the viability of tokenization free autor regressive sequence modeling at scale so tokenization free would
indeed be amazing we would just feed B streams directly into our models but unfortunately I don't know that this has
really been proven out yet by sufficiently many groups and a sufficient scale uh but something like
this at one point would be amazing and I hope someone comes up with it but for now we have to come back and we can't
feed this directly into language models and we have to compress it using the B paare encoding algorithm so let's see
how that works so as I mentioned the B paare encoding algorithm is not all that complicated and the Wikipedia page is
Byte Pair Encoding (BPE) algorithm walkthrough
actually quite instructive as far as the basic idea goes go what we're doing is we have some kind of a input sequence uh
like for example here we have only four elements in our vocabulary a b c and d and we have a sequence of them so
instead of bytes let's say we just have four a vocab size of four the sequence is too long and we'd
like to compress it so what we do is that we iteratively find the pair of uh
tokens that occur the most frequently and then once we've
identified that pair we repl replace that pair with just a single new token
that we append to our vocabulary so for example here the bite pair AA occurs
most often so we mint a new token let's call it capital Z and we replace every
single occurrence of AA by Z so now we have two Z's here so here we took a
sequence of 11 characters with vocabulary size four and we've converted
it to a um sequence of only nine tokens but now with a vocabulary of five
because we have a fifth vocabulary element that we just created and it's Z standing for concatination of AA and we
can again repeat this process so we again look at the sequence and identify
the pair of tokens that are most frequent let's say that that is now AB
well we are going to replace AB with a new token that we meant call Y so y becomes ab and then every single
occurrence of ab is now replaced with y so we end up with this so now we only
have 1 2 3 4 5 6 seven characters in our sequence but we have not just um four
vocabulary elements or five but now we have six and for the final round we
again look through the sequence find that the phrase zy or the pair zy is most common and replace it one more time
with another um character let's say x so X is z y and we replace all curses of zy
and we get this following sequence so basically after we have gone through this process instead of having a um
sequence of 11 uh tokens with a vocabulary length of
four we now have a sequence of 1 2 3 four five tokens but our vocabulary
length now is seven and so in this way we can iteratively compress our sequence
I we Mint new tokens so in the in the exact same way we start we start out with bite sequences so we have 256
vocabulary size but we're now going to go through these and find the bite pairs that occur the most and we're going to
iteratively start minting new tokens appending them to our vocabulary and replacing things and in this way we're
going to end up with a compressed training data set and also an algorithm for taking any arbitrary sequence and
encoding it using this uh vocabul and also decoding it back to Strings so
let's now Implement all that so here's what I did I went to this block post that I enjoyed and I took the first
starting the implementation
paragraph and I copy pasted it here into text so this is one very long line
here now to get the tokens as I mentioned we just take our text and we encode it into utf8 the tokens here at
this point will be a raw bites single stream of bytes and just so that it's
easier to work with instead of just a bytes object I'm going to convert all those bytes to integers and then create
a list of it just so it's easier for us to manipulate and work with in Python and visualize and here I'm printing all
of that so this is the original um this is the original paragraph and its length
is 533 uh code points and then here are the bytes encoded in ut utf8 and we see that
this has a length of 616 bytes at this point or 616 tokens and the reason this
is more is because a lot of these simple asky characters or simple characters
they just become a single bite but a lot of these Unicode more complex characters become multiple bytes up to four and so
we are expanding that size so now what we'd like to do as a first step of the algorithm is we'd like
to iterate over here and find the pair of bites that occur most frequently
because we're then going to merge it so if you are working long on a notebook on a side then I encourage you to basically
click on the link find this notebook and try to write that function yourself otherwise I'm going to come here and
Implement first the function that finds the most common pair okay so here's what I came up with there are many different
counting consecutive pairs, finding most common pair
ways to implement this but I'm calling the function get stats it expects a list of integers I'm using a dictionary to
keep track of basically the counts and then this is a pythonic way to iterate consecutive elements of this list uh
which we covered in the previous video and then here I'm just keeping track of just incrementing by one um for all the
pairs so if I call this on all the tokens here then the stats comes out here so this is the dictionary the keys
are these topples of consecutive elements and this is the count so just
to uh print it in a slightly better way this is one way that I like to do that
where you it's a little bit compound here so you can pause if you like but we iterate all all the items the items
called on dictionary returns pairs of key value and instead I create a list
here of value key because if it's a value key list then I can call sort on
it and by default python will uh use the first element which in this case will be
value to sort by if it's given tles and then reverse so it's descending and
print that so basically it looks like 101 comma 32 was the most commonly
occurring consecutive pair and it occurred 20 times we can double check that that makes reasonable sense so if I
just search 10132 then you see that these are the 20 occurrences of that um pair and if we'd
like to take a look at what exactly that pair is we can use Char which is the opposite of or in Python so we give it a
um unic code Cod point so 101 and of 32 and we see that this is e and space so
basically there's a lot of E space here meaning that a lot of these words seem to end with e so here's eace as an
example so there's a lot of that going on here and this is the most common pair so now that we've identified the most
merging the most common pair
common pair we would like to iterate over this sequence we're going to Mint a new token with the ID of
256 right because these tokens currently go from Z to 255 so when we create a new
token it will have an ID of 256 and we're going to iterate over this
entire um list and every every time we see 101 comma 32 we're going to swap
that out for 256 so let's Implement that now and feel free to uh do that yourself as well so
first I commented uh this just so we don't pollute uh the notebook too much this is a nice way of in Python
obtaining the highest ranking pair so we're basically calling the Max on this
dictionary stats and this will return the maximum key and then the question is how does it
rank keys so you can provide it with a function that ranks keys and that
function is just stats. getet uh stats. getet would basically return the value
and so we're ranking by the value and getting the maximum key so it's 101 comma 32 as we saw now to actually merge
10132 um this is the function that I wrote but again there are many different versions of it so we're going to take a
list of IDs and the the pair that we want to replace and that pair will be replaced with the new index
idx so iterating through IDs if we find the pair swap it out for idx so we
create this new list and then we start at zero and then we go through this entire list sequentially from left to
right and here we are checking for equality at the current position with the
pair um so here we are checking that the pair matches now here is a bit of a tricky condition that you have to append
if you're trying to be careful and that is that um you don't want this here to be out of Bounds at the very last
position when you're on the rightmost element of this list otherwise this would uh give you an autof bounds error
so we have to make sure that we're not at the very very last element so uh this would be false for that so if we find a
match we append to this new list that replacement index and we increment the
position by two so we skip over that entire pair but otherwise if we we haven't found a matching pair we just
sort of copy over the um element at that position and increment by one then
return this so here's a very small toy example if we have a list 566 791 and we
want to replace the occurrences of 67 with 99 then calling this on that will
give us what we're asking for so here the 67 is replaced with
99 so now I'm going to uncomment this for our actual use case where we want to
take our tokens we want to take the top pair here and replace it with 256 to get
tokens to if we run this we get the following so recall that previously we
had a length 616 in this list and now we have a length 596 right so this
decreased by 20 which makes sense because there are 20 occurrences moreover we can try to find 256 here and
we see plenty of occurrences on off it and moreover just double check there should be no occurrence of 10132 so this
is the original array plenty of them and in the second array there are no occurrences of 1032 so we've
successfully merged this single pair and now we just uh iterate this so we are
going to go over the sequence again find the most common pair and replace it so let me now write a y Loop that uses
these functions to do this um sort of iteratively and how many times do we do it four well that's totally up to us as
a hyper parameter the more um steps we take the larger will be our vocabulary and the shorter
will be our sequence and there is some sweet spot that we usually find works the best in practice and so this is kind
of a hyperparameter and we tune it and we find good vocabulary sizes as an example gp4 currently uses roughly
100,000 tokens and um bpark that those are reasonable numbers currently instead
the are large language models so let me now write uh putting putting it all together and uh iterating these steps
training the tokenizer: adding the while loop, compression ratio
okay now before we dive into the Y loop I wanted to add one more cell here where I went to the block post and instead of
grabbing just the first paragraph or two I took the entire block post and I stretched it out in a single line and
basically just using longer text will allow us to have more representative statistics for the bite Pairs and we'll
just get a more sensible results out of it because it's longer text um so here
we have the raw text we encode it into bytes using the utf8 encoding
and then here as before we are just changing it into a list of integers in Python just so it's easier to work with
instead of the raw byes objects and then this is the code that I came up with uh
to actually do the merging in Loop these two functions here are identical to what
we had above I only included them here just so that you have the point of reference here so uh these two are
identical and then this is the new code that I added so the first first thing we want to do is we want to decide on the
final vocabulary size that we want our tokenizer to have and as I mentioned this is a hyper parameter and you set it
in some way depending on your best performance so let's say for us we're going to use 276 because that way we're
going to be doing exactly 20 merges and uh 20 merges because we already have
256 tokens for the raw bytes and to reach 276 we have to do 20 merges uh to
add 20 new tokens here uh this is uh one way in Python to just create a copy of a list
so I'm taking the tokens list and by wrapping it in a list python will construct a new list of all the
individual elements so this is just a copy operation then here I'm creating a merges uh dictionary so this merges
dictionary is going to maintain basically the child one child two mapping to a new uh token and so what
we're going to be building up here is a binary tree of merges but actually it's not exactly a tree because a tree would
have a single root node with a bunch of leaves for us we're starting with the leaves on the bottom which are the
individual bites those are the starting 256 tokens and then we're starting to like merge two of them at a time and so
it's not a tree it's more like a forest um uh as we merge these elements
so for 20 merges we're going to find the most commonly occurring pair we're going
to Mint a new token integer for it so I here will start at zero so we'll going to start at 256 we're going to print
that we're merging it and we're going to replace all of the occurrences of that pair with the new new lied token and
we're going to record that this pair of integers merged into this new
integer so running this gives us the following
output so we did 20 merges and for example the first merge was exactly as
before the 10132 um tokens merging into a new token 2556 now keep in mind that the
individual uh tokens 101 and 32 can still occur in the sequence after merging it's only when they occur
exactly consecutively that that becomes 256 now um and in particular the other thing
to notice here is that the token 256 which is the newly minted token is also eligible for merging so here on the
bottom the 20th merge was a merge of 25 and 259 becoming
275 so every time we replace these tokens they become eligible for merging in the next round of data ration so
that's why we're building up a small sort of binary Forest instead of a single individual tree one thing we can take a look at as
well is we can take a look at the compression ratio that we've achieved so in particular we started off with this
tokens list um so we started off with 24,000 bytes and after merging 20 times
uh we now have only 19,000 um tokens and so therefore the
compression ratio simply just dividing the two is roughly 1.27 so that's the amount of compression we were able to
achieve of this text with only 20 merges um and of course the more
vocabulary elements you add uh the greater the compression ratio here would
be finally so that's kind of like um the training of the tokenizer if you will
tokenizer/LLM diagram: it is a completely separate stage
now 1 Point I wanted to make is that and maybe this is a diagram that can help um
kind of illustrate is that tokenizer is a completely separate object from the large language model itself so
everything in this lecture we're not really touching the llm itself uh we're just training the tokenizer this is a completely separate pre-processing stage
usually so the tokenizer will have its own training set just like a large language model has a potentially
different training set so the tokenizer has a training set of documents on which you're going to train the tokenizer and then and um we're
performing The Bite pair encoding algorithm as we saw above to train the vocabulary of this tokenizer so it has its own training set
it is a pre-processing stage that you would run a single time in the beginning um and the tokenizer is trained using
bipar coding algorithm once you have the tokenizer once it's trained and you have the vocabulary and you have the merges
uh we can do both encoding and decoding so these two arrows here so the
tokenizer is a translation layer between raw text which is as we saw the sequence
of Unicode code points it can take raw text and turn it into a token sequence
and vice versa it can take a token sequence and translate it back into raw
text so now that we have trained uh tokenizer and we have these merges we
are going to turn to how we can do the encoding and the decoding step if you give me text here are the tokens and
vice versa if you give me tokens here's the text once we have that we can translate between these two Realms and
then the language model is going to be trained as a step two afterwards and typically in a in a sort of a
state-of-the-art application you might take all of your training data for the language model and you might run it through the tokenizer and sort of
translate everything into a massive token sequence and then you can throw away the raw text you're just left with
the tokens themselves and those are stored on disk and that is what the large language model is actually reading
when it's training on them so this one approach that you can take as a single massive pre-processing step a
stage um so yeah basically I think the most important thing I want to get across is that this is completely
separate stage it usually has its own entire uh training set you may want to have those training sets be different
between the tokenizer and the logge language model so for example when you're training the tokenizer as I mentioned we don't just care about the
performance of English text we care about uh multi many different languages and we also care about code or not code
so you may want to look into different kinds of mixtures of different kinds of languages and different amounts of code
and things like that because the amount of different language that you have in your tokenizer training set will
determine how many merges of it there will be and therefore that determines the density with which uh this type of
data is um sort of has in the token space and so roughly speaking
intuitively if you add some amount of data like say you have a ton of Japanese data in your uh tokenizer training set
then that means that more Japanese tokens will get merged and therefore Japanese will have shorter sequences uh and that's going to be
beneficial for the large language model which has a finite context length on which it can work on in in the token
space uh so hopefully that makes sense so we're now going to turn to encoding and decoding now that we have trained a
tokenizer so we have our merges and now how do we do encoding and decoding okay
decoding tokens to strings
so let's begin with decoding which is this Arrow over here so given a token sequence let's go through the tokenizer
to get back a python string object so the raw text so this is the function that we' like to implement um we're
given the list of integers and we want to return a python string if you'd like uh try to implement this function yourself it's a fun exercise otherwise
I'm going to start uh pasting in my own solution so there are many different ways to do it um here's one way I will
create an uh kind of pre-processing variable that I will call vocab and vocab is a mapping or a
dictionary in Python for from the token uh ID to the bytes object for that token
so we begin with the raw bytes for tokens from 0 to 255 and then we go in
order of all the merges and we sort of uh populate this vocab list by doing an
addition here so this is the basically the bytes representation of the first
child followed by the second one and remember these are bytes objects so this addition here is an addition of two
bytes objects just concatenation so that's what we get here one tricky thing to be careful with
by the way is that I'm iterating a dictionary in Python using a DOT items and uh it really matters that this runs
in the order in which we inserted items into the merous dictionary luckily starting with python 3.7 this is
guaranteed to be the case but before python 3.7 this iteration may have been out of order with respect to how we
inserted elements into merges and this may not have worked but we are using an um modern python so we're okay and then
here uh given the IDS the first thing we're going to do is get the
tokens so the way I implemented this here is I'm taking I'm iterating over all the IDS I'm using vocap to look up
their bytes and then here this is one way in Python to concatenate all these bytes together to create our tokens and
then these tokens here at this point are raw bytes so I have to decode using UTF
F now back into python strings so previously we called that encode on a
string object to get the bytes and now we're doing it Opposite we're taking the bytes and calling a decode on the bytes
object to get a string in Python and then we can return
text so um this is how we can do it now this actually has a um issue um in the
way I implemented it and this could actually throw an error so try to think figure out why this code could actually
result in an error if we plug in um uh some sequence of IDs that is
unlucky so let me demonstrate the issue when I try to decode just something like 97 I am going to get letter A here back
so nothing too crazy happening but when I try to decode 128 as a single element
the token 128 is what in string or in Python object uni Cod decoder utfa can't
Decode by um 0x8 which is this in HEX in position zero invalid start bite what
does that mean well to understand what this means we have to go back to our utf8 page uh that I briefly showed
earlier and this is Wikipedia utf8 and basically there's a specific schema that
utfa bytes take so in particular if you have a multi-te object for some of the
Unicode characters they have to have this special sort of envelope in how the encoding works and so what's happening
here is that invalid start pite that's because 128 the binary representation of it is
one followed by all zeros so we have one and then all zero and we see here that
that doesn't conform to the format because one followed by all zero just doesn't fit any of these rules so to
speak so it's an invalid start bite which is byte one this one must have a
one following it and then a zero following it and then the content of your uni codee in x here so basically we
don't um exactly follow the utf8 standard and this cannot be decoded and so the way to fix this um is to
use this errors equals in bytes. decode function of python and by default errors
is strict so we will throw an error if um it's not valid utf8 bytes encoding
but there are many different things that you could put here on error handling this is the full list of all the errors
that you can use and in particular instead of strict let's change it to replace and that will replace uh with
this special marker this replacement character so errors equals replace and
now we just get that character back so basically not every single by
sequence is valid utf8 and if it happens that your large language model for example predicts your
tokens in a bad manner then they might not fall into valid utf8 and then we
won't be able to decode them so the standard practice is to basically uh use
errors equals replace and this is what you will also find in the openai um code that they released as well but basically
whenever you see um this kind of a character in your output in that case uh something went wrong and the LM output
not was not valid uh sort of sequence of tokens okay and now we're going to go
encoding strings to tokens
the other way so we are going to implement this Arrow right here where we are going to be given a string and we want to
encode it into tokens so this is the signature of the function that we're interested in and um
this should basically print a list of integers of the tokens so again uh try to maybe implement this yourself if
you'd like a fun exercise uh and pause here otherwise I'm going to start putting in my solution so again there are many ways to
do this so um this is one of the ways that sort of I came came up with so the
first thing we're going to do is we are going to uh take our text encode it into utf8
to get the raw bytes and then as before we're going to call list on the bytes object to get a list of integers of
those bytes so those are the starting tokens those are the raw bytes of our sequence but now of course according to
the merges dictionary above and recall this was the merges some of the bytes may be merged
according to this lookup in addition to that remember that the merges was built from top to bottom and this is sort of
the order in which we inserted stuff into merges and so we prefer to do all these merges in the beginning before we
do these merges later because um for example this merge over here relies on the 256 which got merged here so we have
to go in the order from top to bottom sort of if we are going to be merging anything now we expect to be doing a few
merges so we're going to be doing W true um and now we want to find a pair
of byes that is consecutive that we are allowed to merge according to this in
order to reuse some of the functionality that we've already written I'm going to reuse the function uh get
stats so recall that get stats uh will give us the we'll basically count up how
many times every single pair occurs in our sequence of tokens and return that as a dictionary and the dictionary was a
mapping from all the different uh by pairs to the number of times that they
occur right um at this point we don't actually care how many times they occur in the sequence we only care what the
raw pairs are in that sequence and so I'm only going to be using basically the keys of the dictionary I only care about
the set of possible merge candidates if that makes sense now we want to identify the pair
that we're going to be merging at this stage of the loop so what do we want we want to find the pair or like the a key
inside stats that has the lowest index in the merges uh dictionary because we
want to do all the early merges before we work our way to the late merges so again there are many different
ways to implement this but I'm going to do something a little bit fancy
here so I'm going to be using the Min over an iterator in Python when you call
Min on an iterator and stats here as a dictionary we're going to be iterating the keys of this dictionary in Python so
we're looking at all the pairs inside stats um which are all the consecutive
Pairs and we're going to be taking the consecutive pair inside tokens that has
the minimum what the Min takes a key which gives us the function that is
going to return a value over which we're going to do the Min and the one we care about is we're we care about taking
merges and basically getting um that pairs
index so basically for any pair inside stats we are going to be looking into
merges at what index it has and we want to get the pair with the Min number so
as an example if there's a pair 101 and 32 we definitely want to get that pair uh we want to identify it here and
return it and pair would become 10132 if it occurs and the reason that I'm putting a
float INF here as a fall back is that in the get function when we call uh when we
basically consider a pair that doesn't occur in the merges then that pair is not eligible to be merged right so if in
the token sequence there's some pair that is not a merging pair it cannot be merged then uh it doesn't actually occur
here and it doesn't have an index and uh it cannot be merged which we will denote as float INF and the reason Infinity is
nice here is because for sure we're guaranteed that it's not going to participate in the list of candidates when we do the men so uh so this is one
way to do it so B basically long story short this Returns the most eligible merging candidate pair uh that occurs in
the tokens now one thing to be careful with here is this uh function here might
fail in the following way if there's nothing to merge then uh uh then there's
nothing in merges um that satisfi that is satisfied anymore there's nothing to merge everything just returns float imps
and then the pair I think will just become the very first element of stats
um but this pair is not actually a mergeable pair it just becomes the first pair inside stats arbitrarily because
all of these pairs evaluate to float in for the merging Criterion so basically
it could be that this this doesn't look succeed because there's no more merging pairs so if this pair is not in merges
that was returned then this is a signal for us that actually there was nothing to merge no single pair can be merged
anymore in that case we will break out um nothing else can be
merged you may come up with a different implementation by the way this is kind of like really trying hard in
Python um but really we're just trying to find a pair that can be merged with the lowest index
here now if we did find a pair that is inside merges with the lowest index then
we can merge it so we're going to look into the merger
dictionary for that pair to look up the index and we're going to now merge that
into that index so we're going to do tokens equals and we're going to replace the original tokens we're going
to be replacing the pair pair and we're going to be replacing it with index idx and this returns a new list of tokens
where every occurrence of pair is replaced with idx so we're doing a merge and we're going to be continuing this
until eventually nothing can be merged we'll come out here and we'll break out and here we just return
tokens and so that that's the implementation I think so hopefully this runs okay cool um yeah and this looks uh
reasonable so for example 32 is a space in asky so that's here um so this looks
like it worked great okay so let's wrap up this section of the video at least I wanted to point out that this is not
quite the right implementation just yet because we are leaving out a special case so in particular if uh we try to do
this this would give us an error and the issue is that um if we only have a single character or an empty string then
stats is empty and that causes an issue inside Min so one way to fight this is if L of tokens is at least two because
if it's less than two it's just a single token or no tokens then let's just uh there's nothing to merge so we just
return so that would fix uh that case Okay and then second I have a few
test cases here for us as well so first let's make sure uh about or let's note
the following if we take a string and we try to encode it and then decode it back
you'd expect to get the same string back right is that true for all
strings so I think uh so here it is the case and I think in general this is probably the case um but notice that
going backwards is not is not you're not going to have an identity going backwards because as I mentioned us not
all token sequences are valid utf8 uh sort of by streams and so so therefore
you're some of them can't even be decodable um so this only goes in One
Direction but for that one direction we can check uh here if we take the training text which is the text that we
train to tokenizer around we can make sure that when we encode and decode we get the same thing back which is true
and here I took some validation data so I went to I think this web page and I grabbed some text so this is text that
the tokenizer has not seen and we can make sure that this also works um okay so that gives us some confidence that
this was correctly implemented so those are the basics of the bite pair encoding algorithm we saw how we can uh
take some training set train a tokenizer the parameters of this tokenizer really are just this dictionary of merges and
that basically creates the little binary Forest on top of raw bites once we have this the merges table
we can both encode and decode between raw text and token sequences so that's the the simplest setting of The
tokenizer what we're going to do now though is we're going to look at some of the St the art lar language models and
the kinds of tokenizers that they use and we're going to see that this picture complexifies very quickly so we're going
to go through the details of this comp complexification one at a time so let's
regex patterns to force splits across categories
kick things off by looking at the GPD Series so in particular I have the gpt2 paper here um and this paper is from
2019 or so so 5 years ago and let's scroll down to input representation this
is where they talk about the tokenizer that they're using for gpd2 now this is all fairly readable so I encourage you
to pause and um read this yourself but this is where they motivate the use of the bite pair encoding algorithm on the
bite level representation of utf8 encoding so this is where they motivate it and they talk about the vocabulary
sizes and everything now everything here is exactly as we've covered it so far but things start to depart around here
so what they mention is that they don't just apply the naive algorithm as we have done it and in particular here's a
example suppose that you have common words like dog what will happen is that dog of course occurs very frequently in
the text and it occurs right next to all kinds of punctuation as an example so doc dot dog exclamation mark dog
question mark Etc and naively you might imagine that the BP algorithm could merge these to be single tokens and then
you end up with lots of tokens that are just like dog with a slightly different punctuation and so it feels like you're
clustering things that shouldn't be clustered you're combining kind of semantics with uation and this uh feels suboptimal and
indeed they also say that this is suboptimal according to some of the experiments so what they want to do is
they want to top down in a manual way enforce that some types of um characters
should never be merged together um so they want to enforce these merging rules
on top of the bite PA encoding algorithm so let's take a look um at their code
and see how they actually enforce this and what kinds of mergy they actually do perform so I have to to tab open here
for gpt2 under open AI on GitHub and when we go to Source there is an encoder thatp now I
don't personally love that they call it encoder dopy because this is the tokenizer and the tokenizer can do both
encode and decode uh so it feels kind of awkward to me that it's called encoder but that is the tokenizer and there's a
lot going on here and we're going to step through it in detail at one point for now I just want to focus on this
part here the create a rigix pattern here that looks very complicated and we're going to go through it in a bit uh
but this is the core part that allows them to enforce rules uh for what parts
of the text Will Never Be merged for sure now notice that re. compile here is a little bit misleading because we're
not just doing import re which is the python re module we're doing import reex as re and reex is a python package that
you can install P install r x and it's basically an extension of re so it's a bit more powerful
re um so let's take a look at this pattern and
what it's doing and why this is actually doing the separation that they are looking for okay so I've copy pasted the
pattern here to our jupit notebook where we left off and let's take this pattern for a spin so in the exact same way that
their code does we're going to call an re. findall for this pattern on any
arbitrary string that we are interested so this is the string that we want to encode into tokens um to feed into n llm
like gpt2 so what exactly is this doing well re. findall will take this pattern
and try to match it against a string um the way this works is that you
are going from left to right in the string and you're trying to match the pattern and R.F find all will get all
the occurrences and organize them into a list now when you look at the um when
you look at this pattern first of all notice that this is a raw string um and then these are three double quotes just
to start the string so really the string itself this is the pattern itself right and notice that it's made up of a
lot of ores so see these vertical bars those are ores in reg X and so you go
from left to right in this pattern and try to match it against the string wherever you are so we have hello and
we're going to try to match it well it's not apostrophe s it's not apostrophe t or any of these but it is an optional
space followed by- P of uh sorry SL P of L one or more times what is/ P of L it
is coming to some documentation that I found um there might be other sources as
well uh SLP is a letter any kind of letter from any language and hello is
made up of letters h e l Etc so optional space followed by a bunch of letters one
or more letters is going to match hello but then the match ends because a white
space is not a letter so from there on begins a new sort of attempt to match
against the string again and starting in here we're going to skip over all of these again until we get to the exact
same Point again and we see that there's an optional space this is the optional space followed by a bunch of letters one
or more of them and so that matches so when we run this we get a list of two
elements hello and then space world so how are you if we add more letters we
would just get them like this now what is this doing and why is this important we are taking our string and instead of
directly encoding it um for tokenization we are first splitting it
up and when you actually step through the code and we'll do that in a bit more detail what really is doing on a high
level is that it first splits your text into a list of texts just like this one
and all these elements of this list are processed independently by the tokenizer and all of the results of that
processing are simply concatenated so hello world oh I I
missed how hello world how are you we have five elements of list all of these
will independent independently go from text to a token
sequence and then that token sequence is going to be concatenated it's all going to be joined up and roughly speaking
what that does is you're only ever finding merges between the elements of this list so you can only ever consider
merges within every one of these elements in individually and um after you've done
all the possible merging for all of these elements individually the results of all that will be joined um by
concatenation and so you are basically what what you're doing effectively is you are never going to be merging this e
with this space because they are now parts of the separate elements of this list and so you are saying we are never
going to merge eace um because we're breaking it up in this way so basically using this regx
pattern to Chunk Up the text is just one way of enforcing that some merges are
not to happen and we're going to go into more of this text and we'll see that what this is trying to do on a high level is we're trying to not merge
across letters across numbers across punctuation and so on so let's see in
more detail how that works so let's continue now we have/ P ofn if you go to the documentation SLP of n is any kind
of numeric character in any script so it's numbers so we have an optional space followed by numbers and those
would be separated out so letters and numbers are being separated so if I do Hello World 123 how are you then world
will stop matching here because one is not a letter anymore but one is a number so this group will match for that and
we'll get it as a separate entity uh let's see how these apostrophes work
so here if we have um uh Slash V or I mean apostrophe V as
an example then apostrophe here is not a letter or a number so hello will stop matching and
then we will exactly match this with that so that will come out as a separate
thing so why are they doing the apostrophes here honestly I think that these are just like very common
apostrophes p uh that are used um typically I don't love that they've done
this because uh let me show you what happens when you have uh some Unicode
apostrophes like for example you can have if you have house then this will be
separated out because of this matching but if you use the Unicode apostrophe like
this then suddenly this does not work and so this apostrophe will actually
become its own thing now and so so um it's basically hardcoded for this specific kind of apostrophe and uh
otherwise they become completely separate tokens in addition to this you can go to the gpt2 docs and here when
they Define the pattern they say should have added re. ignore case so BP merges can happen for capitalized versions of
contractions so what they're pointing out is that you see how this is apostrophe and then lowercase letters
well because they didn't do re. ignore case then then um these rules will not
separate out the apostrophes if it's uppercase so house would be like this but if I did
house if I'm uppercase then notice suddenly the apostrophe comes by
itself so the tokenization will work differently in uppercase and lower case
inconsistently separating out these apostrophes so it feels extremely gnarly and slightly gross um but that's that's
how that works okay so let's come back after trying to match a bunch of apostrophe Expressions by the way the
other issue here is that these are quite language specific probably so I don't know that all the languages for example
use or don't use apostrophes but that would be inconsistently tokenized as a result then we try to match letters then
we try to match numbers and then if that doesn't work we fall back to here and
what this is saying is again optional space followed by something that is not a letter number or a space in one or
more of that so what this is doing effectively is this is trying to match punctuation roughly speaking not letters
and not numbers so this group will try to trigger for that so if I do something like this then these parts here are not
letters or numbers but they will actually they are uh they will actually get caught here and so they become its
own group so we've separated out the punctuation and finally this um this is
also a little bit confusing so this is matching white space but this is using a
negative look ahead assertion in regex so what this is doing is it's matching
wh space up to but not including the last Whit space character why is this important um this
is pretty subtle I think so you see how the white space is always included at the beginning of the word so um space r
space u Etc suppose we have a lot of spaces here what's going to happen here is that
these spaces up to not including the last character will get caught by this
and what that will do is it will separate out the spaces up to but not including the last character so that the
last character can come here and join with the um space you and the reason
that's nice is because space you is the common token so if I didn't have these Extra Spaces here you would just have
space you and if I add tokens if I add spaces we still have a space view but
now we have all this extra white space so basically the GB to tokenizer really likes to have a space letters or numbers
um and it it preens these spaces and this is just something that it is consistent about so that's what that is
for and then finally we have all the the last fallback is um whites space characters uh so um that would be
just um if that doesn't get caught then this thing will catch any trailing
spaces and so on I wanted to show one more real world example here so if we have this string which is a piece of
python code and then we try to split it up then this is the kind of output we get so you'll notice that the list has
many elements here and that's because we are splitting up fairly often uh every time sort of a category
changes um so there will never be any merges Within These elements and um that's what you are
seeing here now you might think that in order to train the tokenizer uh open AI has used this to
split up text into chunks and then run just a BP algorithm within all the chunks but that is not exactly what
happened and the reason is the following notice that we have the spaces here uh
those Spaces end up being entire elements but these spaces never actually
end up being merged by by open Ai and the way you can tell is that if you copy paste the exact same chunk here into Tik
token U Tik tokenizer you see that all the spaces are kept independent and
they're all token 220 so I think opena at some point Point en Force some rule that these spaces
would never be merged and so um there's some additional rules on top of just
chunking and bpe that open ey is not uh clear about now the training code for
the gpt2 tokenizer was never released so all we have is uh the code that I've already shown you but this code here
that they've released is only the inference code for the tokens so this is not the training code you can't give it
a piece of text and training tokenizer this is just the inference code which Tak takes the merges that we have up
above and applies them to a new piece of text and so we don't know exactly how opening ey trained um train the
tokenizer but it wasn't as simple as chunk it up and BP it uh whatever it was
tiktoken library intro, differences between GPT-2/GPT-4 regex
next I wanted to introduce you to the Tik token library from openai which is the official library for tokenization
from openai so this is Tik token bip install P to Tik token and then um you
can do the tokenization in inference this is again not training code this is only inference code for
tokenization um I wanted to show you how you would use it quite simple and running this just gives us the gpt2
tokens or the GPT 4 tokens so this is the tokenizer use for GPT 4 and so in
particular we see that the Whit space in gpt2 remains unmerged but in GPT 4 uh these Whit spaces merge as we also saw
in this one where here they're all unmerged but if we go down to GPT 4 uh
they become merged um now in the
gp4 uh tokenizer they changed the regular expression that they use to
Chunk Up text so the way to see this is that if you come to your the Tik token uh library and then you go to this file
Tik token X openi public this is where sort of like the definition of all these different tokenizers that openi
maintains is and so uh necessarily to do the inference they had to publish some of the details about the strings
so this is the string that we already saw for gpt2 it is slightly different but it is actually equivalent uh to what
we discussed here so this pattern that we discussed is equivalent to this pattern this one just executes a little
bit faster so here you see a little bit of a slightly different definition but otherwise it's the same we're going to
go into special tokens in a bit and then if you scroll down to CL 100k this is
the GPT 4 tokenizer you see that the pattern has changed um and this is kind
of like the main the major change in addition to a bunch of other special tokens which I'll go into in a bit again
now some I'm not going to actually go into the full detail of the pattern change because honestly this is my
numbing uh I would just advise that you pull out chat GPT and the regex documentation and just step through it
but really the major changes are number one you see this eye here that means
that the um case sensitivity this is case insensitive match and so the
comment that we saw earlier on oh we should have used re. uppercase uh basically we're now going to be matching
these apostrophe s apostrophe D apostrophe M Etc uh we're going to be
matching them both in lowercase and in uppercase so that's fixed there's a bunch of different like handling of the
whites space that I'm not going to go into the full details of and then one more thing here is you will notice that
when they match the numbers they only match one to three numbers so so they will never merge
numbers that are in low in more than three digits only up to three digits of
numbers will ever be merged and uh that's one change that they made as well
to prevent uh tokens that are very very long number sequences uh but again we don't really
know why they do any of this stuff uh because none of this is documented and uh it's just we just get the pattern so
um yeah it is what it is but those are some of the changes that gp4 has made and of course the vocabulary size went
from roughly 50k to roughly 100K the next thing I would like to do very briefly is to take you through the
GPT-2 encoder.py released by OpenAI walkthrough
gpt2 encoder dopy that openi has released uh this is the file that I
already mentioned to you briefly now this file is uh fairly short and should
be relatively understandable to you at this point um starting at the bottom
here they are loading two files encoder Json and vocab bpe and they do some
light processing on it and then they call this encoder object which is the tokenizer now if you'd like to inspect
these two files which together constitute their saved tokenizer then you can do that with a piece of code
like this um this is where you can download these two files and you can inspect them if you'd like and what you will find is
that this encoder as they call it in their code is exactly equivalent to our vocab so remember here where we have
this vocab object which allowed us us to decode very efficiently and basically it took us from the integer to the byes uh
for that integer so our vocab is exactly their encoder and then their vocab bpe
confusingly is actually are merges so their BP merges which is based on the
data inside vocab bpe ends up being equivalent to our merges so uh basically
they are saving and loading the two uh variables that for us are also critical
the merges variable and the vocab variable using just these two variables you can represent a tokenizer and you
can both do encoding and decoding once you've trained this tokenizer now the only thing that um is
actually slightly confusing inside what opening ey does here is that in addition to this encoder and a decoder they also
have something called a bite encoder and a bite decoder and this is actually unfortunately just
kind of a spirous implementation detail and isn't actually deep or interesting in any way so I'm going to skip the
discussion of it but what opening ey does here for reasons that I don't fully understand is that not only have they
this tokenizer which can encode and decode but they have a whole separate layer here in addition that is used serially with the tokenizer and so you
first do um bite encode and then encode and then you do decode and then bite
decode so that's the loop and they are just stacked serial on top of each other
and and it's not that interesting so I won't cover it and you can step through it if you'd like otherwise this file if
you ignore the bite encoder and the bite decoder will be algorithmically very familiar with you and the meat of it
here is the what they call bpe function and you should recognize this Loop here
which is very similar to our own y Loop where they're trying to identify the Byram uh a pair that they should be
merging next and then here just like we had they have a for Loop trying to merge this pair uh so they will go over all of
the sequence and they will merge the pair whenever they find it and they keep repeating that until they run out of
possible merges in the in the text so that's the meat of this file and uh there's an encode and a decode function
just like we have implemented it so long story short what I want you to take away at this point is that unfortunately it's
a little bit of a messy code that they have but algorithmically it is identical to what we've built up above and what
we've built up above if you understand it is algorithmically what is necessary to actually build a BP to organizer
train it and then both encode and decode the next topic I would like to turn to is that of special tokens so in addition
special tokens, tiktoken handling of, GPT-2/GPT-4 differences
to tokens that are coming from you know raw bytes and the BP merges we can insert all kinds of tokens that we are
going to use to delimit different parts of the data or introduced to create a special structure of the token streams
so in uh if you look at this encoder object from open AIS gpd2 right here we
mentioned this is very similar to our vocab you'll notice that the length of this is
50257 and as I mentioned it's mapping uh and it's inverted from the mapping of our vocab our vocab goes from integer to
string and they go the other way around for no amazing reason um but the thing
to note here is that this the mapping table here is 50257 where does that number come from
where what are the tokens as I mentioned there are 256 raw bite token
tokens and then opena actually did 50,000 merges so those become the other tokens
but this would have been 50256 so what is the 57th token and
there is basically one special token and that one special token you can
see is called end of text so this is a special token and it's the very last
token and this token is used to delimit documents ments in the training set so
when we're creating the training data we have all these documents and we tokenize them and we get a stream of tokens those
tokens only range from Z to 50256 and then in between those
documents we put special end of text token and we insert that token in
between documents and we are using this as a signal to the language model that
the document has ended and what follows is going to be unrelated to the document previously that said the language model
has to learn this from data it it needs to learn that this token usually means that it should wipe its sort of memory
of what came before and what came before this token is not actually informative to what comes next but we are expecting
the language model to just like learn this but we're giving it the Special sort of the limiter of these documents
we can go here to Tech tokenizer and um this the gpt2 tokenizer uh our code that
we've been playing with before so we can add here right hello world world how are you and we're getting different tokens
but now you can see what if what happens if I put end of text you see how until I
finished it these are all different tokens end of text still set different tokens and now
when I finish it suddenly we get token 50256 and the reason this works is
because this didn't actually go through the bpe merges instead the code that
actually outposted tokens has special case instructions for handling special
tokens um we did not see these special instructions for handling special tokens in the encoder dopy it's absent there
but if you go to Tech token Library which is uh implemented in Rust you will find all kinds of special case handling
for these special tokens that you can register uh create adds to the vocabulary and then it looks for them
and it uh whenever it sees these special tokens like this it will actually come in and swap in that special token so
these things are outside of the typical algorithm of uh B PA en coding so these special tokens are used
pervasively uh not just in uh basically base language modeling of predicting the next token in the sequence but
especially when it gets to later to the fine tuning stage and all of the chat uh gbt sort of aspects of it uh because we
don't just want to Del limit documents we want to delimit entire conversations between an assistant and a user so if I
refresh this sck tokenizer page the default example that they have here is using not sort of base model encoders
but ftuned model uh sort of tokenizers um so for example using the GPT 3.5
turbo scheme these here are all special tokens I am start I end Etc uh this is
short for Imaginary mcore start by the way but you can see here that there's a
sort of start and end of every single message and there can be many other other tokens lots of tokens um in use to
delimit these conversations and kind of keep track of the flow of the messages here now we can go back to the Tik token
library and here when you scroll to the bottom they talk about how you can extend tick token and I can you can
create basically you can Fork uh the um CL 100K base tokenizers in gp4 and for
example you can extend it by adding more special tokens and these are totally up to you you can come up with any arbitrary tokens and add them with the
new ID afterwards and the tikken library will uh correctly swap them out uh when
it sees this in the strings now we can also go back to this
file which we've looked at previously and I mentioned that the gpt2 in Tik toen open
I.P we have the vocabulary we have the pattern for splitting and then here we are registering the single special token
in gpd2 which was the end of text token and we saw that it has this ID in GPT 4 when they defy this here you
see that the pattern has changed as we've discussed but also the special tokens have changed in this tokenizer so
we of course have the end of text just like in gpd2 but we also see three sorry
four additional tokens here Thim prefix middle and suffix what is fim fim is
short for fill in the middle and if you'd like to learn more about this idea it comes from this paper um and I'm not
going to go into detail in this video it's beyond this video and then there's one additional uh serve token here so
that's that encoding as well so it's very common basically to train a language model and then if you'd like uh
you can add special tokens now when you add special tokens you of course have to
um do some model surgery to the Transformer and all the parameters involved in that Transformer because you
are basically adding an integer and you want to make sure that for example your embedding Matrix for the vocabulary
tokens has to be extended by adding a row and typically this row would be initialized uh with small random numbers
or something like that because we need to have a vector that now stands for that token in addition to that you have
to go to the final layer of the Transformer and you have to make sure that that projection at the very end into the classifier uh is extended by
one as well so basically there's some model surgery involved that you have to couple with the tokenization changes if
you are going to add special tokens but this is a very common operation that people do especially if they'd like to
fine tune the model for example taking it from a base model to a chat model like chat
GPT okay so at this point you should have everything you need in order to build your own gp4 tokenizer now in the
minbpe exercise time! write your own GPT-4 tokenizer
process of developing this lecture I've done that and I published the code under this repository
MBP so MBP looks like this right now as I'm recording but uh the MBP repository
will probably change quite a bit because I intend to continue working on it um in addition to the MBP repository I've
published the this uh exercise progression that you can follow so if you go to exercise. MD here uh this is
sort of me breaking up the task ahead of you into four steps that sort of uh
build up to what can be a gp4 tokenizer and so feel free to follow these steps exactly and follow a little bit of the
guidance that I've laid out here and anytime you feel stuck just reference the MBP repository here so either the
tests could be useful or the MBP repository itself I try to keep the code fairly clean and understandable and so
um feel free to reference it whenever um you get stuck uh in addition to that basically
once you write it you should be able to reproduce this behavior from Tech token so getting the gb4 tokenizer you can
take uh you can encode the string and you should get these tokens and then you can encode and decode the exact same
string to recover it and in addition to all that you should be able to implement your own train function uh which Tik
token Library does not provide it's it's again only inference code but you could write your own train MBP does it as well
and that will allow you to train your own token vocabularies so here are some of the code inside M be mean bpe uh shows the
token vocabularies that you might obtain so on the left uh here we have the GPT 4
merges uh so the first 256 are raw individual bytes and then here I am
visualizing the merges that gp4 performed during its training so the very first merge that gp4 did was merge
two spaces into a single token for you know two spaces and that is a token 256
and so this is the order in which things merged during gb4 training and this is the merge order that um we obtain in MBP
by training a tokenizer and in this case I trained it on a Wikipedia page of Taylor Swift uh not because I'm a Swifty
but because that is one of the longest um Wikipedia Pages apparently that's available but she is pretty cool and
um what was I going to say yeah so you can compare these two uh vocabularies
and so as an example um here GPT for merged I in to become in and we've done
the exact same thing on this token 259 here space t becomes space t and that
happened for us a little bit later as well so the difference here is again to my understanding only a difference of
the training set so as an example because I see a lot of white space I supect that gp4 probably had a lot of
python code in its training set I'm not sure uh for the tokenizer and uh here we see much less
of that of course in the Wikipedia page so roughly speaking they look the same and they look the same because they're
running the same algorithm and when you train your own you're probably going to get something similar depending on what
you train it on okay so we are now going to move on from tick token and the way that open AI tokenizes its strings and
sentencepiece library intro, used to train Llama 2 vocabulary
we're going to discuss one more very commonly used library for working with tokenization inlm
and that is sentence piece so sentence piece is very commonly used in language
models because unlike Tik token it can do both training and inference and is quite efficient at both it supports a
number of algorithms for training uh vocabularies but one of them is the B pair en coding algorithm that we've been
looking at so it supports it now sentence piece is used both by llama and
mistal series and many other models as well it is on GitHub under Google
sentence piece and the big difference with sentence piece and we're going to look at example
because this is kind of hard and subtle to explain is that they think different about the order of operations here so in
the case of Tik token we first take our code points in the string we encode them
using mutf to bytes and then we're merging bytes it's fairly straightforward for sentence piece um it
works directly on the level of the code points themselves so so it looks at whatever code points are available in
your training set and then it starts merging those code points and um the bpe
is running on the level of code points and if you happen to run out of code points so there are maybe some rare
uh code points that just don't come up too often and the Rarity is determined by this character coverage hyper parameter then these uh code points will
either get mapped to a special unknown token like ank or if you have the bite
foldback option turned on then that will take those rare Cod points it will encode them using utf8 and then the
individual bytes of that encoding will be translated into tokens and there are these special bite tokens that basically
get added to the vocabulary so it uses BP on on the code points and then it
falls back to bytes for rare Cod points um and so that's kind of like difference
personally I find the Tik token we significantly cleaner uh but it's kind of like a subtle but pretty major difference between the way they approach
tokenization let's work with with a concrete example because otherwise this is kind of hard to um to get your head
around so let's work with a concrete example this is how we can import sentence piece and then here we're going
to take I think I took like the description of sentence piece and I just created like a little toy data set it
really likes to have a file so I created a toy. txt file with this content now what's kind of a little bit
crazy about sentence piece is that there's a ton of options and configurations and the reason this is so
is because sentence piece has been around I think for a while and it really tries to handle a large diversity of things and um because it's been around I
think it has quite a bit of accumulated historical baggage uh as well and so in
particular there's like a ton of configuration arguments this is not even all of it you can go to here to see all
the training options um and uh there's also quite useful documentation when you look at
the raw Proto buff uh that is used to represent the trainer spec and so on um
many of these options are irrelevant to us so maybe to point out one example Das Das shrinking Factor uh this shrinking
factor is not used in the B pair en coding algorithm so this is just an argument that is irrelevant to us um it
applies to a different training algorithm now what I tried to do here is
I tried to set up sentence piece in a way that is very very similar as far as I can tell to maybe identical hopefully
to the way that llama 2 was strained so the way they trained their own um their
own tokenizer and the way I did this was basically you can take the tokenizer model file that meta released and you
can um open it using the Proto protuff uh sort of file that you can generate
and then you can inspect all the options and I tried to copy over all the options that looked relevant so here we set up
the input it's raw text in this file here's going to be the output so it's going to be for talk 400. model and
vocab we're saying that we're going to use the BP algorithm and we want to Bap size of
400 then there's a ton of configurations here
for um for basically pre-processing and normalization rules as they're called
normalization used to be very prevalent I would say before llms in natural language processing so in machine
translation and uh text classification and so on you want to normalize and simplify the text and you want to turn
it all lowercase and you want to remove all double whites space Etc and in language models we prefer not to
do any of it or at least that is my preference as a deep learning person you want to not touch your data you want to
keep the raw data as much as possible um in a raw form so you're basically trying to turn
off a lot of this if you can the other thing that sentence piece does is that it has this concept of sentences so
sentence piece it's back it's kind of like was developed I think early in the days where there was um an idea that
they you're training a tokenizer on a bunch of independent sentences so it has a lot of like how many sentences you're
going to train on what is the maximum sentence length um shuffling sentences and so for it
sentences are kind of like the individual training examples but again in the context of llms I find that this is like a very spous and weird
distinction like sentences are just like don't touch the raw data sentences
happen to exist but in raw data sets there are a lot of like inet like what exactly is a sentence what isn't a
sentence um and so I think like it's really hard to Define what an actual sentence is if you really like dig into
it and there could be different concepts of it in different languages or something like that so why even
introduce the concept it it doesn't honestly make sense to me I would just prefer to treat a file as a giant uh
stream of bytes it has a lot of treatment around rare word characters and when I say word
I mean code points we're going to come back to this in a second and it has a lot of other rules for um basically
splitting digits splitting white space and numbers and how you deal with that so these are some kind of like merge
rules so I think this is a little bit equivalent to tick token using the regular expression to split up
categories there's like kind of equivalence of it if you squint T it in sentence piece where you can also for
example split up split up the digits uh and uh so
on there's a few more things here that I'll come back to in a bit and then there are some special tokens that you can indicate and it hardcodes the UN
token the beginning of sentence end of sentence and a pad token um and the UN
token must exist for my understanding and then some some things so we can
train and when when I press train it's going to create this file talk 400.
model and talk 400. wab I can then load the model file and I can inspect the
vocabulary off it and so we trained vocab size 400 on this text here and
these are the individual pieces the individual tokens that sentence piece will create so in the beginning we see
that we have the an token uh with the ID zero then we have the beginning of
sequence end of sequence one and two and then we said that the pad ID is negative
1 so we chose not to use it so there's no pad ID here then these are individual bite
tokens so here we saw that bite fallback in llama was turned on so it's true so
what follows are going to be the 256 bite tokens and these are their
IDs and then at the bottom after the bite tokens come the
merges and these are the parent nodes in the merges so we're not seeing the children we're just seeing the parents
and their ID and then after the merges comes eventually the individual
tokens and their IDs and so these are the individual tokens so these are the individual code Point tokens if you will
and they come at the end so that is the ordering with which sentence piece sort of like represents its vocabularies it
starts with special tokens then the bike tokens then the merge tokens and then the individual codo tokens and all these
raw codepoint to tokens are the ones that it encountered in the training set so those individual code points are
all the the entire set of code points that occurred here so those all get put in there and
then those that are extremely rare as determined by character coverage so if a code Point occurred only a single time
out of like a million um sentences or something like that then it would be ignored and it would not be added to our
uh vocabulary once we have a vocabulary we can encode into IDs and we can um sort
of get a list and then here I am also decoding the indiv idual tokens back into little
pieces as they call it so let's take a look at what happened here hello space
on so these are the token IDs we got back and when we look here uh a few
things sort of uh jump to mind number one take a look at these characters the
Korean characters of course were not part of the training set so sentence piece is encountering code points that
it has not seen during training time and those code points do not have a token associated with them so suddenly these
are un tokens unknown tokens but because bite fall back as true instead sentence
piece falls back to bytes and so it takes this it encodes it with utf8 and
then it uses these tokens to represent uh those bytes and that's what we are
getting sort of here this is the utf8 uh encoding and in this shifted by three uh
because of these um special tokens here that have IDs earlier on so that's what
happened here now one more thing that um well first before I go on with respect
to the bitef back let me remove bite foldback if this is false what's going
to happen let's retrain so the first thing that happened is all the bite tokens disappeared right
and now we just have the merges and we have a lot more merges now because we have a lot more space because we're not taking up space in the wab size uh with
all the bytes and now if we encode this we get a zero so this entire string
here suddenly there's no bitef back so this is unknown and unknown is an and so
this is zero because the an token is token zero and you have to keep in mind
that this would feed into your uh language model so what is a language model supposed to do when all kinds of different things that are unrecognized
because they're rare just end up mapping into Unk it's not exactly the property that you want so that's why I think
llama correctly uh used by fallback true uh because we definitely want to feed
these um unknown or rare code points into the model and some uh some manner the next thing I want to show you is the
following notice here when we are decoding all the individual tokens you see how spaces uh space here ends up
being this um bold underline I'm not 100% sure by the way why sentence piece
switches whites space into these bold underscore characters maybe it's for visualization I'm not 100% sure why that
happens uh but notice this why do we have an extra space in the front of
hello um what where is this coming from well it's coming from this option
here um add dummy prefix is true and when you
go to the documentation add D whites space at the beginning of text in order to treat World in world and hello world in the
exact same way so what this is trying to do is the following if we go back to our tick
tokenizer world as uh token by itself has a different ID than space world so
we have this is 1917 but this is 14 Etc so these are two different tokens for
the language model and the language model has to learn from data that they are actually kind of like a very similar concept so to the language model in the
Tik token World um basically words in the beginning of sentences and words in the middle of sentences actually look
completely different um and it has to learned that they are roughly the same
so this add dami prefix is trying to fight that a little bit and the way that works is that it basically
uh adds a dummy prefix so for as a as a
part of pre-processing it will take the string and it will add a space it will do this and that's done in an effort to
make this world and that world the same they will both be space world so that's
one other kind of pre-processing option that is turned on and llama 2 also uh uses this option and that's I think
everything that I want to say for my preview of sentence piece and how it is different um maybe here what I've done
is I just uh put in the Raw protocol buffer representation basically of the
tokenizer the too trained so feel free to sort of Step through this and if you would like uh your tokenization to look
identical to that of the meta uh llama 2 then you would be copy pasting these settings as I tried to do up above and
uh yeah that's I think that's it for this section I think my summary for sentence piece from all of this is
number one I think that there's a lot of historical baggage in sentence piece a lot of Concepts that I think are slightly confusing and I think
potentially um contain foot guns like this concept of a sentence and it's maximum length and stuff like that um
otherwise it is fairly commonly used in the industry um because it is efficient
and can do both training and inference uh it has a few quirks like for example un token must exist and the way the bite
fallbacks are done and so on I don't find particularly elegant and unfortunately I have to say it's not very well documented so it took me a lot
of time working with this myself um and just visualizing things and trying to really understand what is happening here
because uh the documentation unfortunately is in my opion not not super amazing but it is a very nice repo
that is available to you if you'd like to train your own tokenizer right now okay let me now switch gears again as we're starting to slowly wrap up here I
how to set vocabulary set? revisiting gpt.py transformer
want to revisit this issue in a bit more detail of how we should set the vocap size and what are some of the considerations around it so for this I'd
like to go back to the model architecture that we developed in the last video when we built the GPT from
scratch so this here was uh the file that we built in the previous video and we defined the Transformer model and and
let's specifically look at Bap size and where it appears in this file so here we Define the voap size uh at this time it
was 65 or something like that extremely small number so this will grow much larger you'll see that Bap size doesn't
come up too much in most of these layers the only place that it comes up to is in exactly these two places here so when we
Define the language model there's the token embedding table which is this two-dimensional array where the vocap
size is basically the number of rows and uh each vocabulary element each token
has a vector that we're going to train using back propagation that Vector is of size and embed which is number of
channels in the Transformer and basically as voap size increases this embedding table as I mentioned earlier
is going to also grow we're going to be adding rows in addition to that at the end of the Transformer there's this LM
head layer which is a linear layer and you'll notice that that layer is used at the very end to produce the logits uh
which become the probabilities for the next token in sequence and so intuitively we're trying to produce a
probability for every single token that might come next at every point in time of that Transformer and if we have more
and more tokens we need to produce more and more probabilities so every single token is going to introduce an
additional dot product that we have to do here in this linear layer for this final layer in a
Transformer so why can't vocap size be infinite why can't we grow to Infinity
well number one your token embedding table is going to grow uh your linear
layer is going to grow so we're going to be doing a lot more computation here because this LM head layer will become more computational expensive number two
because we have more parameters we could be worried that we are going to be under trining some of these
parameters so intuitively if you have a very large vocabulary size say we have a million uh tokens then every one of
these tokens is going to come up more and more rarely in the training data because there's a lot more other tokens
all over the place and so we're going to be seeing fewer and fewer examples uh for each individual token and you might
be worried that basically the vectors associated with every token will be undertrained as a result because they just don't come up too often and they
don't participate in the forward backward pass in addition to that as your vocab size grows you're going to start shrinking your sequences a lot
right and that's really nice because that means that we're going to be attending to more and more text so that's nice but also you might be
worrying that two large of chunks are being squished into single tokens and so the model just doesn't have as much of
time to think per sort of um some number of characters in the text or you can
think about it that way right so basically we're squishing too much information into a single token and then the forward pass of the Transformer is
not enough to actually process that information appropriately and so these are some of the considerations you're thinking about when you're designing the
vocab size as I mentioned this is mostly an empirical hyperparameter and it seems like in state-of-the-art architectures
today this is usually in the high 10,000 or somewhere around 100,000 today and
the next consideration I want to briefly talk about is what if we want to take a pre-trained model and we want to extend
the vocap size and this is done fairly commonly actually so for example when you're doing fine-tuning for cha GPT um
a lot more new special tokens get introduced on top of the base model to maintain the metadata and all the
structure of conversation objects between a user and an assistant so that takes a lot of special tokens you might
also try to throw in more special tokens for example for using the browser or any other tool and so it's very tempting to
add a lot of tokens for all kinds of special functionality so if you want to be adding a token that's totally
possible Right all we have to do is we have to resize this embedding so we have to add rows we would initialize these uh
parameters from scratch to be small random numbers and then we have to extend the weight inside this linear uh
so we have to start making dot products um with the associated parameters as well to basically calculate the
probabilities for these new tokens so both of these are just a resizing operation it's a very mild
model surgery and can be done fairly easily and it's quite common that basically you would freeze the base model you introduce these new parameters
and then you only train these new parameters to introduce new tokens into the architecture um and so you can
freeze arbitrary parts of it or you can train arbitrary parts of it and that's totally up to you but basically minor
surgery required if you'd like to introduce new tokens and finally I'd like to mention that actually there's an
training new tokens, example of prompt compression
entire design space of applications in terms of introducing new tokens into a vocabulary that go Way Beyond just
adding special tokens and special new functionality so just to give you a sense of the design space but this could be an entire video just by itself uh
this is a paper on learning to compress prompts with what they called uh gist tokens and the rough idea is suppose
that you're using language models in a setting that requires very long prompts while these long prompts just slow
everything down because you have to encode them and then you have to use them and then you're tending over them and it's just um you know heavy to have
very large prompts so instead what they do here in this paper is they introduce
new tokens and um imagine basically having a few new tokens you put them in
a sequence and then you train the model by distillation so you are keeping the
entire model Frozen and you're only training the representations of the new tokens their embeddings and you're
optimizing over the new tokens such that the behavior of the language model is identical uh to the model that has a
very long prompt that works for you and so it's a compression technique of compressing that very long prompt into
those few new gist tokens and so you can train this and then at test time you can discard your old prompt and just swap in
those tokens and they sort of like uh stand in for that very long prompt and have an almost identical performance and
so this is one um technique and a class of parameter efficient fine-tuning techniques where most of the model is
basically fixed and there's no training of the model weights there's no training of Laura or anything like that of new
parameters the the parameters that you're training are now just the uh token embeddings so that's just one
example but this could again be like an entire video but just to give you a sense that there's a whole design space here that is potentially worth exploring
in the future the next thing I want to briefly address is that I think recently there's a lot of momentum in how you
multimodal [image, video, audio] tokenization with vector quantization
actually could construct Transformers that can simultaneously process not just text as the input modality but a lot of
other modalities so be it images videos audio Etc and how do you feed in all
these modalities and potentially predict these modalities from a Transformer uh do you have to change the architecture
in some fundamental way and I think what a lot of people are starting to converge towards is that you're not changing the architecture you stick with the
Transformer you just kind of tokenize your input domains and then call the day and pretend it's just text tokens and
just do everything else identical in an identical manner so here for example there was a early paper that has nice
graphic for how you can take an image and you can chunc at it into integers um and these sometimes uh so
these will basically become the tokens of images as an example and uh these tokens can be uh hard tokens where you
force them to be integers they can also be soft tokens where you uh sort of don't require uh these to be discrete
but you do Force these representations to go through bottlenecks like in Auto encoders uh also in this paper that came
out from open a SORA which I think really um uh blew the mind of many people and inspired a lot of people in
terms of what's possible they have a Graphic here and they talk briefly about how llms have text tokens Sora has
visual patches so again they came up with a way to chunc a videos into basically tokens when they own
vocabularies and then you can either process discrete tokens say with autog regressive models or even soft tokens
with diffusion models and uh all of that is sort of uh being actively worked on
designed on and is beyond the scope of this video but just something I wanted to mention briefly okay now that we have come quite deep into the tokenization
revisiting and explaining the quirks of LLM tokenization
algorithm and we understand a lot more about how it works let's loop back around to the beginning of this video
and go through some of these bullet points and really see why they happen so first of all why can't my llm spell
words very well or do other spell related tasks so fundamentally this is because
as we saw these characters are chunked up into tokens and some of these tokens
are actually fairly long so as an example I went to the gp4 vocabulary and I looked at uh one of the longer tokens
so that default style turns out to be a single individual token so that's a lot of characters for a single token so my
suspicion is that there's just too much crammed into this single token and my suspicion was that the model should not
be very good at tasks related to spelling of this uh single token so I
asked how many letters L are there in the word default style and of course my
prompt is intentionally done that way and you see how default style will be a single token so this is what the model
sees so my suspicion is that it wouldn't be very good at this and indeed it is not it doesn't actually know how many
L's are in there it thinks there are three and actually there are four if I'm not getting this wrong myself so that
didn't go extremely well let's look look at another kind of uh character level task so for example here I asked uh gp4
to reverse the string default style and they tried to use a code interpreter and I stopped it and I said just do it just
try it and uh it gave me jumble so it doesn't actually really know how to
reverse this string going from right to left uh so it gave a wrong result so
again like working with this working hypothesis that maybe this is due to the tokenization I tried a different
approach I said okay let's reverse the exact same string but take the following approach step one just print out every
single character separated by spaces and then as a step two reverse that list and it again Tred to use a tool but when I
stopped it it uh first uh produced all the characters and that was actually correct and then It reversed them and
that was correct once it had this so somehow it can't reverse it directly but when you go just first uh you know
listing it out in order it can do that somehow and then it can once it's uh broken up this way this becomes all
these individual characters and so now this is much easier for it to see these individual tokens and reverse them and
print them out so that is kind of interesting so let's continue now why
are llms worse at uh non-english langu and I briefly covered this already but
basically um it's not only that the language model sees less non-english data during training of the model
parameters but also the tokenizer is not um is not sufficiently trained on
non-english data and so here for example hello how are you is five tokens and its
translation is 15 tokens so this is a three times blow up and so for example
anang is uh just hello basically in Korean and that end up being three tokens I'm actually kind of surprised by
that because that is a very common phrase there just the typical greeting of like hello and that ends up being
three tokens whereas our hello is a single token and so basically everything is a lot more bloated and diffuse and
this is I think partly the reason that the model Works worse on other languages uh coming back why is LM bad
at simple arithmetic um that has to do with the tokenization of numbers and so
um you'll notice that for example addition is very sort of like uh there's an algorithm that is
like character level for doing addition so for example here we would first add the ones and then the tens and then the
hundreds you have to refer to specific parts of these digits but uh these
numbers are represented completely arbitrarily based on whatever happened to merge or not merge during the tokenization process there's an entire
blog post about this that I think is quite good integer tokenization is insane and this person basically
systematically explores the tokenization of numbers in I believe this is gpt2 and
so they notice that for example for the for um four-digit numbers you can take a
look at whether it is uh a single token or whether it is two tokens that is a 1 three or a 2 two or a 31 combination and
so all the different numbers are all the different combinations and you can imagine this is all completely arbitrarily so and the model
unfortunately sometimes sees uh four um a token for for all four digits
sometimes for three sometimes for two sometimes for one and it's in an arbitrary uh Manner and so this is
definitely a headwind if you will for the language model and it's kind of incredible that it can kind of do it and
deal with it but it's also kind of not ideal and so that's why for example we saw that meta when they train the Llama
2 algorithm and they use sentence piece they make sure to split up all the um
all the digits as an example for uh llama 2 and this is partly to improve a
simple arithmetic kind of performance and finally why is gpt2 not
as good in Python again this is partly a modeling issue on in the architecture and the data set and the strength of the
model but it's also partially tokenization because as we saw here with the simple python example the encoding
efficiency of the tokenizer for handling spaces in Python is terrible and every single space is an individual token and
this dramatically reduces the context length that the model can attend to cross so that's almost like a tokenization bug for gpd2 and that was
later fixed with gp4 okay so here's another fun one my llm abruptly halts
when it sees the string end of text so here's um here's a very strange Behavior
print a string end of text is what I told jt4 and it says could you please specify the string and I'm I'm telling
it give me end of text and it seems like there's an issue it's not seeing end of text and then I give it end of text is
the string and then here's a string and then it just doesn't print it so obviously something is breaking here
with respect to the handling of the special token and I don't actually know what open ey is doing under the hood
here and whether they are potentially parsing this as an um as an actual token
instead of this just being uh end of text um as like individual sort of
pieces of it without the special token handling logic and so it might be that someone when they're calling do encode
uh they are passing in the allowed special and they are allowing end of text as a special character in the user
prompt but the user prompt of course is is a sort of um attacker controlled text
so you would hope that they don't really parse or use special tokens or you know
from that kind of input but it appears that there's something definitely going wrong here and um so your knowledge of
these special tokens ends up being in a tax surface potentially and so if you'd like to confuse llms then just um try to
give them some special tokens and see if you're breaking something by chance okay so this next one is a really fun one uh
the trailing whites space issue so if you come to playground and uh we come
here to GPT 3.5 turbo instruct so this is not a chat model this is a completion model so think of it more like it's a
lot more closer to a base model it does completion it will continue the token sequence so here's a tagline for ice
cream shop and we want to continue the sequence and so we can submit and get a bunch of tokens okay no problem but now
suppose I do this but instead of pressing submit here I do here's a tagline for ice cream shop space so I
have a space here before I click submit we get a warning your text ends
in a trail Ling space which causes worse performance due to how API splits text into tokens so what's happening here it
still gave us a uh sort of completion here but let's take a look at what's happening so here's a tagline for an ice
cream shop and then what does this look like in the actual actual training data
suppose you found the completion in the training document somewhere on the internet and the llm trained on this
data so maybe it's something like oh yeah maybe that's the tagline that's a terrible tagline but notice here that
when I create o you see that because there's the the space character is
always a prefix to these tokens in GPT so it's not an O token it's a space o
token the space is part of the O and together they are token 8840 that's
that's space o so what's What's Happening Here is that when I just have it like this and I let it complete the
next token it can sample the space o token but instead if I have this and I
add my space then what I'm doing here when I incode this string is I have
basically here's a t line for an ice cream uh shop and this space at the very end becomes a token
220 and so we've added token 220 and this token otherwise would be part of
the tagline because if there actually is a tagline here so space o is the token
and so this is suddenly a of distribution for the model because this space is part of the next token but
we're putting it here like this and the model has seen very very little data of
actual Space by itself and we're asking it to complete the sequence like add in more tokens but the problem is that
we've sort of begun the first token and now it's been split up and now we're out
of this distribution and now arbitrary bad things happen and it's just a very rare example for it to see something
like that and uh that's why we get the warning so the fundamental issue here is of course that um the llm is on top of
these tokens and these tokens are text chunks they're not characters in a way you and I would think of them they are
these are the atoms of what the LM is seeing and there's a bunch of weird stuff that comes out of it let's go back
to our default cell style I bet you that the model has never in its training set
seen default cell sta without Le in there it's always seen this as a single
group because uh this is some kind of a function in um I'm guess I don't
actually know what this is part of this is some kind of API but I bet you that it's never seen this combination of
tokens uh in its training data because or I think it would be extremely rare so
I took this and I copy pasted it here and I had I tried to complete from it
and the it immediately gave me a big error and it said the model predicted to completion that begins with a stop sequence resulting in no output consider
adjusting your prompt or stop sequences so what happened here when I clicked submit is that immediately the model
emitted and sort of like end of text token I think or something like that it basically predicted the stop sequence
immediately so it had no completion and so this is why I'm getting a warning again because we're off the data
distribution and the model is just uh predicting just totally arbitrary things
it's just really confused basically this is uh this is giving it brain damage it's never seen this before it's shocked
and it's predicting end of text or something I tried it again here and it in this case it completed it but then
for some reason this request May violate our usage policies this was flagged um basically something just like
goes wrong and there's something like Jank you can just feel the Jank because the model is like extremely unhappy with just this and it doesn't know how to
complete it because it's never occurred in training set in a training set it always appears like this and becomes a
single token so these kinds of issues where tokens are either you sort of like complete the
first character of the next token or you are sort of you have long tokens that you then have just some of the
characters off all of these are kind of like issues with partial tokens is how I
would describe it and if you actually dig into the T token repository go to the rust code and
search for unstable and you'll see um en code
unstable native unstable token tokens and a lot of like special case handling none of this stuff about unstable tokens
is documented anywhere but there's a ton of code dealing with unstable tokens and unstable tokens is exactly kind of like
what I'm describing here what you would like out of a completion API is something a lot more fancy like if we're
putting in default cell sta if we're asking for the next token sequence we're not actually trying to append the next
token exactly after this list we're actually trying to append we're trying to consider lots of tokens um
that if we were or I guess like we're trying to search over characters that if
we retened would be of high probability if that makes sense um so that we can actually add a single individual
character uh instead of just like adding the next full token that comes after this partial token list so I this is
very tricky to describe and I invite you to maybe like look through this it ends up being extremely gnarly and hairy kind
of topic it and it comes from tokenization fundamentally so um maybe I can even spend an entire video talking
about unstable tokens sometime in the future okay and I'm really saving the best for last my favorite one by far is
the solid gold Magikarp and it just okay so this comes from this blog post uh solid gold
Magikarp and uh this is um internet famous now for those of us in llms and
basically I I would advise you to uh read this block Post in full but basically what this person was doing is
this person went to the um token embedding stable and clustered the
tokens based on their embedding representation and this person noticed that there's a cluster of tokens that
look really strange so there's a cluster here at rot e stream Fame solid gold Magikarp Signet message like really
weird tokens in uh basically in this embedding cluster and so what are these
tokens and where do they even come from like what is solid gold magikarpet makes no sense and then they found bunch of
these tokens and then they notice that actually the plot thickens here because if you ask the model about these tokens
like you ask it uh some very benign question like please can you repeat back to me the string sold gold Magikarp uh
then you get a variety of basically totally broken llm Behavior so either you get evasion so I'm sorry I can't
hear you or you get a bunch of hallucinations as a response um you can even get back like insults so you ask it
uh about streamer bot it uh tells the and the model actually just calls you names uh or it kind of comes up with
like weird humor like you're actually breaking the model by asking about these very simple strings like at Roth and
sold gold Magikarp so like what the hell is happening and there's a variety of here documented behaviors uh there's a
bunch of tokens not just so good Magikarp that have that kind of a behavior and so basically there's a
bunch of like trigger words and if you ask the model about these trigger words or you just include them in your prompt
the model goes haywire and has all kinds of uh really Strange Behaviors including sort of ones that violate typical safety
guidelines uh and the alignment of the model like it's swearing back at you so what is happening here and how can this
possibly be true well this again comes down to tokenization so what's happening here is that sold gold Magikarp if you
actually dig into it is a Reddit user so there's a u Sol gold
Magikarp and probably what happened here even though I I don't know that this has been like really definitively explored
but what is thought to have happened is that the tokenization data set was very
different from the training data set for the actual language model so in the tokenization data set there was a ton of
redded data potentially where the user solid gold Magikarp was mentioned in the text because solid gold Magikarp was a
very common um sort of uh person who would post a lot uh this would be a string that occurs many times in a
tokenization data set because it occurs many times in a tokenization data set these tokens would end up getting merged
to the single individual token for that single Reddit user sold gold Magikarp so they would have a dedicated token in a
vocabulary of was it 50,000 tokens in gpd2 that is devoted to that Reddit user
and then what happens is the tokenization data set has those strings but then later when you train the model
the language model itself um this data from Reddit was not present and so
therefore in the entire training set for the language model sold gold Magikarp never occurs that token never appears in
the training set for the actual language model later so this token never gets activated it's initialized at random in
the beginning of optimization then you have forward backward passes and updates to the model and this token is just never updated in the embedding table
that row Vector never gets sampled it never gets used so it never gets trained and it's completely untrained it's kind
of like unallocated memory in a typical binary program written in C or something like that that so it's unallocated
memory and then at test time if you evoke this token then you're basically plucking out a row of the embedding
table that is completely untrained and that feeds into a Transformer and creates undefined behavior and that's
what we're seeing here this completely undefined never before seen in a training behavior and so any of these
kind of like weird tokens would evoke this Behavior because fundamentally the model is um is uh uh out of sample out
of distribution okay and the very last thing I wanted to just briefly mention point out although I think a lot of
people are quite aware of this is that different kinds of formats and different representations and different languages
and so on might be more or less efficient with GPD tokenizers uh or any tokenizers for any other L for that
matter so for example Json is actually really dense in tokens and yaml is a lot more efficient in tokens um so for
example this are these are the same in Json and in yaml the Json is
116 and the yaml is 99 so quite a bit of an Improvement and so in the token
economy where we are paying uh per token in many ways and you are paying in the context length and you're paying in um
dollar amount for uh the cost of processing all this kind of structured data when you have to um so prefer to
use theal over Json and in general kind of like the tokenization density is something that you have to um sort of
care about and worry about at all times and try to find efficient encoding schemes and spend a lot of time in tick
tokenizer and measure the different token efficiencies of different formats and settings and so on okay so that
final recommendations
concludes my fairly long video on tokenization I know it's a try I know it's annoying I know it's irritating I
personally really dislike the stage what I do have to say at this point is don't brush it off there's a lot of foot guns
sharp edges here security issues uh AI safety issues as we saw plugging in unallocated memory into uh language
models so um it's worth understanding this stage um that said I will say that
eternal glory goes to anyone who can get rid of it uh I showed you one possible paper that tried to uh do that and I
think I hope a lot more can follow over time and my final recommendations for the application right now are if you can
reuse the GPT 4 tokens and the vocabulary uh in your application then that's something you should consider and just use Tech token because it is very
efficient and nice library for inference for bpe I also really like the bite
level BP that uh Tik toen and openi uses uh if you for some reason want to train
your own vocabulary from scratch um then I would use uh the bpe with sentence
piece um oops as I mentioned I'm not a huge fan of sentence piece I don't like
its uh bite fallback and I don't like that it's doing BP on unic code code points I think it's uh it also has like
a million settings and I think there's a lot of foot gonss here and I think it's really easy to Mis calibrate them and you end up cropping your sentences or
something like that uh because of some type of parameter that you don't fully understand so so be very careful with
the settings try to copy paste exactly maybe where what meta did or basically spend a lot of time looking at all the
hyper parameters and go through the code of sentence piece and make sure that you have this correct um but even if you
have all the settings correct I still think that the algorithm is kind of inferior to what's happening here and
maybe the best if you really need to train your vocabulary maybe the best thing is to just wait for M bpe to
becomes as efficient as possible and uh that's something that maybe I hope to work on and at some point maybe we can
be training basically really what we want is we want tick token but training code and that is the ideal thing that
currently does not exist and MBP is um is in implementation of it but currently
it's in Python so that's currently what I have to say for uh tokenization there might be an advanced video that has even
drier and even more detailed in the future but for now I think we're going to leave things off here and uh I hope
that was helpful bye
and uh they increase this contact size from gpt1 of 512 uh to 1024 and GPT 4
two the next okay next I would like us to briefly walk through the code from open
AI on the gpt2 encoded
ATP I'm sorry I'm gonna sneeze and then what's Happening Here
is this is a spous layer that I will explain in a bit What's Happening Here
is

intro: Let’s reproduce GPT-2 (124M)
hi everyone so today we are going to be continuing our Zero to Hero series and in particular today we are going to
reproduce the gpt2 model the 124 million version of it so when openi released
gpt2 this was 2019 and they released it with this blog post on top of that they
released this paper and on top of that they released this code on GitHub so open a/
gpt2 now when we talk about reproducing gpt2 we have to be careful because in particular in this video we're going to
be reproducing the 124 million parameter model so the thing to realize is that there's always a miniseries when these
are releases are made so there are the gpt2 miniseries made up of models at
different sizes and usually the biggest model is called the gpt2 but basically the reason we do that
is because you can put the model sizes on the x-axis of plots like this and on the Y AIS you put a lot of uh Downstream
metrics that you're interested in like translation summarization question answering and so on and you can chart out these scaling laws so basically as
the model size increases you're getting better and better at Downstream metrics and so in particular for
gpt2 if we scroll down in paper there are four models in the gpt2 miniseries
starting at 124 million all the way up to 1558 million now the reason my
numbers the way I say them disagree with this table is that this table is wrong if you actually go to the uh gpt2 uh
GitHub repo they sort of say that um there was an error in how they added up the parameters but basically this is the
124 million parameter model Etc so the 124 million parameter had 12 layers in
the Transformer and it had 768 channels in the Transformer 768 dimensions and
I'm going to be assuming some familiarity with what these terms mean because I covered all of this in my previous video let's build gpt2 uh let's
build GPT from scratch so I covered that in the previous video in this playlist now if we do everything correctly and
everything works out well by the end of this video we're going to see something like this where we're looking at the
validation loss which basically um measures how good we are at predicting
the next token in a sequence on some validation data that the model has not seen during training and we see that we
go from doing that task not very well because we're initializing from scratch all the way to doing that task quite
well um by the end of the training and hopefully we're going to beat the gpt2 uh 124 M model
now previously when they were working on this this is already 5 years ago so this was probably a fairly complicated
optimization at the time and the gpus and the compute was a lot smaller today you can reproduce this model in roughly
an hour or probably less even and it will cost you about 10 bucks if you want to do this on the cloud uh Cloud Compu a
sort of computer that you can all rent and if you pay $10 for that computer you wait about an hour or less you can
actually achieve a model that is as good as this model that open ey released and
uh one more thing to mention is unlike many other models open ey did release the weights for gpt2 so those weights
are all available in this repository but the gpt2 paper is not always as good
with all of the details of training so in addition to the gpt2 paper we're going to be referencing the gpt3 paper
which is a lot more Concrete in a lot of the hyp parameters and optimization settings and so on um and it's not a
huge departure in the architecture from the GPT 2 uh version of the model so we're going to be referencing both gpt2
and gpt3 as we try to reproduce gpt2 124 M uh so let's go so the first thing I
exploring the GPT-2 (124M) OpenAI checkpoint
would like to do is actually start at the end or at the Target so in other words let's load the GPT to 124 M model
as it was released by openi and maybe take it for a spin let's sample some tokens from it now the issue with that
is when you go into the code base of gpt2 and you go into the source and you click in on the model. pi you'll realize
that actually this is using tensorflow so the original gpt2 code here was written in tensor flow which is
um you know not let's just say not used as much anymore um so we'd like to use
pytorch uh because it's a lot friendlier easier and I just personally like a lot more the problem with that is the
initial code is intenser flow we'd like to use pytorch so instead uh to get the target we're going to use the hugging
face Transformers um code which I like a lot more so when you go into the
Transformers source Transformers models gpt2 modeling gpt2 Pi you will see that
they have the gpt2 implementation of that Transformer here in this file um and it's like medium readable
but not fully readable um but what it does is it did all the work of converting all those weights uh from
tensor flow to pytorch Friendly and so it's much easier to load and work with so in particular we can look at the
gpt2 um model here and we can load it using hugging face Transformers so swinging over this is what that looks
like from Transformers import the DP GT2 LM head model and then from pre-train
gpt2 uh now one awkward thing about this is that when you do gpt2 as the model that we're loading this actually is the
124 million parameter model if you want the actual the gpt2 the 1.5 billion then
you actually want to do- XL so this is the 12 4 M our Target now what we're
doing is when we actually get this we're initializing the uh pytorch NN module as defined here in this
class from it I want to get just the state dict which is just a raw tensors
so we just have um the tensors of that file and by the way here this is a jupyter notebook uh but this is jupyter
notebook running inside vs code uh so I like to work with it all in a single sort of interface so I like to use vs
code so this is the jupyter notebook extension inside the es
code so when we get the state dick this is just a dict so we can print the key
and the value which is the tensor and let's just look at the shapes so these are sort of
the uh different parameters inside the gbt2 model and their shape so the W
weight for token embedding is of size
50257 by 768 where this is coming from is that we have 50257 tokens in the gpt2 vocabulary um
and the tokens by the way these are exactly the tokens that we spoken about in the previous video on my tokenization
Series so the previous videos just before this I go into a ton of detail on tokenization gpt2 tokenizer happens to
have this many tokens for each token we have a 768 dimensional
embedding that is the distributed representation that stands in for that token so each token is a little string
piece and then the 768 numbers are the vector that represents that
token and so this is just our lookup table for tokens and then here we have the lookup table for the positions so
because gbt2 has a maximum sequence length of 1024 we have up to 1,24 positions that
each token can be attending to in the past and every one of those positions in gpd2 has a fixed Vector of
768 that is learned by optimization um and so this is the
position embedding and the token embedding um and then everything here is just the other weights and biases and
everything else of this Transformer so when you just take for example the positional embeddings and
flatten it out and take just the 20 elements you can see that these are just the parameters these are weights floats
just we can take and we can plot them so these are the position embeddings and we
get something like this and you can see that this has structure and it has structure because what we what we have
here really is every Row in this visualization is a different position a
fixed absolute position in um the range from 0 to 1024 and each row here is the
representation of that position and so it has structure because these positional embeddings end up learning
these sinusoids and cosiness um that sort of like represent each of these
positions and uh each row here stands in for that position and is processed by the Transformer to recover all the
relative positions and uh sort of realize which token is where and um
attend to them depending on their position not just their content so when we actually just look
into an individual column inside these and I just grabbed three random columns
you'll see that for example here we are focusing on every every single um
Channel and we're looking at what that channel is doing as a
function of uh position from one from Z to 1223
really and we can see that some of these channels basically like respond more or less to different parts of the position
Spectrum so this green channel uh really likes to fire for everything after 200
uh up to 800 but not less a lot less and has a sharp drop off here near zero so
who knows what these embeddings are doing and why they are the way they are you can tell for example that because they're a bit more Jagged and they're
kind of noisy you can tell that this model was not fully trained and the more trained this model was the more you
would expect to smooth this out and so this is telling you that this is a little bit of an undertrained model um
but in principle actually these curves don't even have to be smooth this should just be totally random noise and in fact
in the beginning of the optimization it is complete random noise because this position embedding table is initialized
completely at random so in the beginning you have jaggedness and the fact that you end up with something smooth is
already kind of impressive um that that just falls out of the optimization because in principle you shouldn't even
be able to get any single graph out of this that makes sense but we actually get something that looks a little bit
noisy but for the most part looks sinusoidal like um in the original Transformer um in the original
Transformer paper the attention is all you need paper the positional embeddings are actually initialized and fixed if I
remember correctly to sinusoids and cosiness of uh different frequencies and that's the positional coding and it's
fixed but in gpt2 these are just parameters and they're trained from scratch just like any other parameter uh
and that seems to work about as well and so what they do is they kind of like recover these sinusoidal like features
during the optimization we can also look at any of the other matrices here so here I took
the first layer of the Transformer and looking at like one of
its weights and just the first block of 300 by 300 and you see some structure
but like again like who knows what any of this is if you're into mechanistic interpretability you might get a real
kick out of trying to figure out like what is going on what is this structure and what does this all mean but we're
not going to be doing that in this video but we definitely see that there's some interesting structure and that's kind of cool what we're mostly interested in is
we've loaded the weights of this model that was released by open Ai and now using the hogging face Transformers we
can not just get all the raw weights but we can also get the um what they call
Pipeline and sample from it so this is the prefix hello I'm a language model
comma and then we're sampling uh 30 tokens and we getting five sequences and
I ran this and this is what it produced um hell language model but what I'm really doing is
making a human readable document there are other languages but those are dot dot dot so you can read through these if
you like but basically these are five different completions of the same prefix from this uh gbt
2124m now uh if I go here I took this example from here and sadly even though
we are fixing the seed we are getting different Generations from the snippet than what they got so presumably the
code changed um but what we see though at this stage that's important is that
we are getting coherent text so we've loaded the model successfully we can look at all its parameters and the keys
tell us where in the model these come from and we want to actually write our
own gpt2 class so that we have full understanding of what's happening there we don't want to be working with something like uh the modeling gpt2 Pi
because it's just too complicated we want to write this from scratch ourselves so we're going to be implementing the GPT model here in
parallel and as our first task let's load the gpt2 124 M into the class that
we're going to develop here from scratch that's going to give us confidence that we can load the open ey model and
therefore there's a setting of Weights that exactly is the 124 model but then of course what we're going to do is
we're going to initialize the model from scratch instead and try try to train it ourselves um on a bunch of documents
that we're going to get and we're going to try to surpass that model so we're going to get different weights and everything's going to look different
hopefully better even um but uh we're going to have a lot of confidence that because we can load the
openi model we are in the same model family and model class and we just have to ReDiscover a good setting of the
weights uh but from scratch so let's now write the gbt2 model and let's load the
weights and make sure that we can also generate text that looks coherent okay so let's now swing over to the attention
SECTION 1: implementing the GPT-2 nn.Module
is all un need paper that started everything and let's scroll over to the model architecture the original
Transformer now remember that gpt2 is slightly modified from the or or Transformer in particular we do not have
uh the encoder gpt2 is a decoder only Transformer as we call it so this entire encoder here is missing in addition to
that this cross attention here that was using that encoder is also missing so we delete this entire part everything else
stays almost the same but there are some differences that we're going to uh sort of look at here so there are two main
differences when we go to the gb2 page under 2.3 model we notice that first
there's a reshuffling of the layer Norms so they change place and second an
additional layer normalization was added here to the final self detention block
so basically all the layer Norms here instead of being after the MLP or after the attention they SN before it and an
additional layer Norm gets added here right before the final classifier so now let's Implement some
of the first sort of skeleton NN module modules here in our GPT NN module and in
particular we're going to try to match up this schema here that is used by hugging face Transformers because that
will make it much easier to load these weights from this state dict so we want something that reflects uh this schema
here so here's what I came up with um basically we see that the main
container here that has all the modules is called Transformer so I'm reflecting that with an NN module dict and this is
basically a module that allows you to index into the subm modules using keys just like a dictionary uh
strings within it we have the weights of the token embeddings WT and that's an N
embedding and the weights of the position embeddings which is also just an N embedding and if you remember n
embedding is really just a fancy little wrapper module around just a single um
single array of numbers a single uh block of numbers just like this it's a
single tensor and an embedding is a glorified um wrapper around a tensor
that allows you to access its elements uh by indexing into the rows now in addition to that we see here
that we have a h and then there's a this is index using numbers instead of
indexed using strings so there's a h. 0 1 2 Etc all the way up till h. 11 and
that's because there are 12 layers here in this Transformer so to reflect that I'm creating also an H I think that
probably stands for hidden and instead of a module dict this is a model list so we can index it using integers exactly
as we see here 01 2 Etc and the modular list has a n layer blocks and the blocks
are yet to be defined in a module in a bit in addition to that following the gpt2 paper we have we need an additional
final layer Norm that we're going to put in there and then we have the final classifier uh the language model head
which um projects from 768 the number of embedding dimensions in this GPT all the
way to the vocab size which is 50257 and gpt2 uses no bias for this
final uh sort of projection so this is the skeleton and you can see that it
reflects this so the wte is the token embeddings here it's called output
embedding but it's really the token embeddings the PE is the positional codings uh those two pieces of
information as we saw previously are going to add and then go into the Transformer the H is the all the blocks
in Gray and the LNF is this new layer that gets added here by the gpt2 model
and LM head is this linear part here so that's the skeleton of the gpt2 we now
have to implement the block okay so let's now recurse to the block itself so
we want to define the block um so I'll start putting them here so the block I
like to write out like this uh these are some of the initializations and then this is the
actual forward pass of what this block computes and notice here that there's a change from the Transformer again that
is mentioned in the gpt2 paper so here the layer normalizations are after the
application of attention or feed forward in addition to that note that the normalizations are inside the residual
stream you see how feed forward is applied and this arrow goes through and through the normalization so that means
that your residual pathway has normalizations inside them and this is not very good or desirable uh you
actually prefer to have a single uh clean residual stream all the way from supervision all the way down to the
inputs the tokens and this is very desirable and nice because the gradients
that flow from the top if you remember from your microad addition just distributes gradients during the
backwards state to both of its branches equally so addition is a branch in the
gradients and so that means that the gradients from the top flows straight to the inputs the tokens through the
residual Pathways unchanged but then in addition to that the gradient also flows through the blocks and the blocks you
know contribute their own contribution over time and kick in and change the optimization over time but basically
clean residual pathway is desirable from an optimization perspective and then the
this is the pre-normalization version where you see that RX first goes through the layer normalization and then the
attention and then goes uh back out to go to the L ration number two and the
multia perceptron sometimes also referred to as a feed forward Network or an FFN and then that goes into the
residual stream again and the one more thing that is kind of interesting to note is that recall that attention is a
communication operation it is where all the tokens and there's 1,24 tokens lined up in a sequence and this is where the
tokens communicate this is where they exchange information so attention is a
um aggregation function it's a pooling function it's a weighted sum function it
is a reduce operation whereas MLP this uh MLP here happens at every single
token individually there's no information being collected or exchanged between the tokens so the attention is
the reduce and the MLP is the map and what you end up with is that the Transformer just ends up just being a
repeated application of map produce if you want to think about it that way so
um this is where they communicate and this is where they think individually about the information that they gathered
and every one of these blocks uh iteratively refines the um representation is at the residual stream
so this is our block um slightly modified from this picture Okay so let's now move on to the MLP so the MLP block
uh I implemented as follows it is relatively straightforward we basically have two linear projections
here that are sandwiched in between the G nonlinearity so nn. G approximate is 10h
now when we swing on uh swing over to the Pyro documentation this is n.g and
it has this format and it has two versions the original version of G which we'll step into into in a bit and the
approximate version of Galo which we can request using 10 so as you can see just as a preview
here G is a basically like a reu except there's no flat exactly Flat Tail here
at exactly zero but otherwise it looks very much like a slightly smoother reu
it comes from this paper here Gan error linear units and uh you can step through
this paper and there's some mathematical calac reasoning that leads to an interpretation that leads to the specific formulation it has to do with
stochastic radial risers and the expectation of a modification to Adaptive dropout so you can read through
all of that if you'd like here and there's a little bit of history as to why there is an an approximate version
of G and that comes from this issue here as far as I can tell and in this issue
Daniel Hendrix mentions that at the time when they developed this nonlinearity
the Earth function which you need to evaluate the exact G was very slow in tensor flow so they ended up basically
developing this approximation and this approximation that then ended up being picked up by Bert and by GP P2 Etc but
today there's no real good reason to use the approximate version you'd prefer to just use the exact version um because I
my expectation is that there's no big difference anymore and this is kind of like a historical um kind of Quirk um
but we are trying to reproduce gpt2 exactly and gpt2 used the 10h
approximate version so we prefer to stick with that um now one other reason to actually
just intuitively use G instead of veru is previously in the in videos in the past we've spoken about the dead reu
neuron problem where in this tale of a reu if it's exactly flat at zero any
activations that fall there will get exactly zero gradient there's no change there's no adaptation there's no
development of the network if any of these activations end in this flat region but the G always contributes a
local gradient and so there's always going to be a change always going to be an adaptation and sort of smoothing it
out ends up empirically working better in practice as demonstrated in this paper and also as demonstrated by it
being picked up by the bird paper gbt2 paper and so on so for that reason we adopt this nonlinearity uh here in the
10 in the gbt2 reproduction now in more modern networks also like llama 3 and so on this nonlinearity also further
changes uh to swiglo and other variants like that uh but for gpt2 they Ed this
approximate G okay and finally we have the attention operation so let me paste in my
attention so I know this is a lot so I'm going to go through this a bit quickly a bit
slowly but not too slowly because we have covered this in the previous video and I would just point you there um so
this is the attention operation now in the previous video you will remember this is not just attention this is um
multi-headed attention right and so in the previous video we had this multi-headed attention module and this
implementation made it obvious that these heads are not actually that complicated uh there's basically
in parallel inside every attention block there's multiple heads and they're all functioning in parallel and uh their
outputs are just being concatenated and that becomes the output of the multi-headed attention so the heads are
just kind of like parallel streams and their outputs get concatenated and so it was very simple
and made the head be kind of like U fairly straightforward in terms of its
implementation what happens here is that instead of having two separate modules and indeed many more modules that get
concatenated all of that is just put into a single uh self attention uh
module and instead I'm being very careful and doing a bunch of transpose
split um tensor gymnastics to make this very efficient in pych but fundamentally
and algorithmically nothing is different from the implementation we saw before um in this uh give
repository so to remind you very briefly and I don't want to go in this uh into
this in too many in too much time but we have these tokens lined up in a sequence and there's 1,20 of them and then each
token at this stage of the attention emits three vectors the query key and the value and first what happens here um
is that the queries and the keys have to multiply each other to get sort of the attention um amount like how interesting
they find each other so they have to interact multiplicatively so what we're doing here is we're calculating the qkv
we splitting it and then there's a bunch of gymnastics as I mentioned here and the way this works is that we're
basically making the number of heads and H into a batch Dimension and so it's a
batch Dimension just like B so that in these operations that follow pytorch treats B and NH as batches and it
applies all the operations on all of them in parallel in both the batch and the
heads and the operations that get applied are number one the queries and the keys intera to give us her attention
this is the autoaggressive mask that makes sure that the tokens only attend to tokens before them and never to
tokens in the future the softmax here normalizes the attention so it sums to one always and
then recall from the previous video that doing the attention Matrix multiply with the values is basically a way to do a
weighted sum of the values of the tokens that we found interesting at every single token and then the final
transpose conf VI and view is just reassembling all of that again and this actually performs the concatenation
operation so you can step through this uh slowly if you'd like um but it is equivalent mathematically to our
previous implementation is just more efficient in P torch so that's why I chose this implementation
instead now in addition to that I'm being careful with how I name my variables so for example cattin is the
same as seaten and so actually our keys should basically exactly follow the schema of the hugging face train
Transformers code and that will make it very easy for us to now Port over all the weights from exactly this sort of
naming conventions because all of our variables are named the same thing but um at this point we have finished the
gpt2 implementation and what that allows us to do is we don't have to basically use uh this file from hugging face which
is fairly long um this is uh 2,000 lines of code um instead we
just have a less than 100 lines of code and this is the complete uh gpd2 implementation so at this stage we
should just be able to take over all the weights set them and then do generation so let's see what that looks like okay
loading the huggingface/GPT-2 parameters
so here I've also changed the GPT config so that the numbers here the H parameters agree with the gpt2 124 M
model so the maximum sequence length which I call block size here is 124 the
number of tokens is 50250 257 which if you watch my tokenizer video know that
this is 50,000 m merges BP merges 256 bite tokens the leaves of the BP tree
and one special end of text token that delimits different documents and can start generation as well and there are
12 layers there are 12 heads in the attention and the dimension of the Transformers was
768 so here's how we can now load the parameters from hugging face to uh our
code here and initialize the GPT class with those parameters so let me just copy paste a bunch of code
here and I'm not going to go through this code too slow too quickly too slowly because um honestly it's not that
interesting it's not that exciting we're just loading the weights so it's kind of dry but as I mentioned there are four
models in this miniseries of gpt2 this is some of the Jupyter code um code that
we had here on the right I'm just pting it over these are the hyper parameters of the gpt2 models uh we're creating the
config object and creating our own model and then what's Happening Here is we're creating the state dict both for our
model and for the hugging face model um and then what we're doing here
is we're going over the hugging face model keys and we're copying over those
tensors and in the process we are kind of ignoring a few of the buffers they're not parameters they're buffers so for
example attention dobias uh that's just used for the autoaggressive mask and so we are ignoring some of those masks and
uh that's it and then then one additional kind of annoyance is that this comes from the tensorflow repo and
I'm not sure how this is a little bit annoying but some of the weights are transposed from what pytorch would want
and so manually I hardcoded the weights that should be transposed and then we transpose them if that is so and then we
return this model so the from pre-trained is a Constructor or class method in Python
that Returns the GPT object if we just give it the model type which in our case
is gpt2 the smallest model that we're interested in so this is the code and this is how you would use it and um we
can pop open the terminal here in vs code and we can python train gbt2 pi and
fingers crossed okay so we didn't crash and so
we can load the weights and the biases and everything else into our Ann module
but now let's also get additional confidence that this is working and let's try to actually generate from this model okay now before we can actually
implementing the forward pass to get logits
generate from this model we have to be able to forward it we didn't actually write that code yet so here's the
forward function so the input to the forward is going to be our indices our tokens uh
token indices and they are always of shape B BYT and so we have batch
dimension of B and then we have the time dimension of up to T and the T can't be
more than the block size the block size is is the maximum sequence length so B BYT indices arranged is sort of like a
two-dimensional layout and remember that basically every single row of this is of size up to uh block size and this is T
tokens that are in a sequence and then we have B independent sequences stacked up in a batch so that this is
efficient now here we are forwarding the position embeddings and the token embeddings and this code should be very
recognizable from the previous lecture so um we basically use uh a range which
is kind of like a version of range but for pytorch uh and we're iterating from Z to T and creating this uh positions uh
sort of uh indices um and then we are making sure that
they're in the same device as idx because we're not going to be training on only CPU that's going to be too inefficient we want to be training on
GPU and that's going to come in in a bit uh then we have the position embeddings and the token embeddings and
the addition operation of those two now notice that the position embed are going to be identical for every single row of
uh of input and so there's broadcasting hidden inside this plus where we have to
create an additional Dimension here and then these two add up because the same position embeddings apply at every
single row of our example stacked up in a batch then we forward the Transformer blocks and finally the last layer norm
and the LM head so what comes out after forward is the logits and if the input
was B BYT indices then at every single B by T we will calculate the uh logits for
what token comes next in the sequence so what is the token B t+1 the one on the
right of this token and B app size here is the number of possible tokens and so
therefore this is the tensor that we're going to obtain and these low jits are just a softmax away from becoming
probabilities so this is the forward pass of the network and now we can get load and so we're going to be able to
generate from the model imminently okay so now we're going to try to set up the identical thing on the
sampling init, prefix tokens, tokenization
left here that matches hug and face on the right so here we've sampled from the pipeline and we sampled five times up to
30 tokens with the prefix of hello I'm a language model and these are the completions that we achieved so we're
going to try to replicate that on the left here so number turn sequences is five max length is 30 so the first thing
we do of course is we initialize our model then we put it into evaluation mode now this is a good practice to put
the model into eval when you're not going to be training it you're just going to be using it and I don't
actually know if this is doing anything right now for the following reason our model up above here contains no modules
or layers that actually have a different uh Behavior at training or evaluation time so for example Dropout batch norm
and a bunch of other layers have this kind of behavior but all of these layers that we've used here should be identical
in both training and evaluation time um so so potentially model that eval does
nothing but then I'm not actually sure if this is the case and maybe pytorch internals uh do some clever things
depending on the evaluation mode uh inside here the next thing we're doing here is we are moving the entire model
to Cuda so we're moving this all of the tensors to GPU so I'm sshed here to a
cloud box and I have a bunch of gpus on this box and here I'm moving the entire
model and all of its members and all of its tensors and everything like that everything gets shipped off to basically
a whole separate computer that is sitting on the GPU and the GPU is connected to the uh CPU and they can
communicate but it's basically a whole separate computer with its own computer architecture and it's really well catered to parallel processing tasks
like those of running neural networks so I'm doing this so that the model lives on the GPU a whole separate computer and
it's just going to make our code a lot more efficient because all of this stuff runs a lot more efficiently on the
gpus so that's the model itself now uh the next thing we want to
do is we want to start with this as the prefix when we do the generation so
let's actually create those prefix tokens so here's the code that I've written we're going to import the tich
token library from open Ai and we're going to get the gpt2 encoding so that's the tokenizer for gpt2 and then we're
going to encode this string and get a list of integers which are the tokens uh
now these integers here should actually be fairly straightforward because we can just copy paste this string and we can
sort of inspect what it is in tick tokenizer so just pasting that in these are the tokens that are going to come
out so this list of integers is what we expect tokens to become and as you
recall if you saw my video of course all the tokens they're just little string chunks right so these are this is the
chunc of this string into gpt2 tokens so once we have those tokens it's
a list of integers we can create a torch tensor out of it in this case it's eight tokens and then we're going to replicate
these eight tokens for five times to get five rows of eight tokens and that is
our initial um input X as I call it here and it lives on the GPU as well so X now
is this idx that we can put into forward to get our logits so that we know what
comes as the sixth token uh sorry as the ninth token in every one
of these five rows okay and we are now ready to generate so let me paste in one more code block
sampling loop
here um so what's happening here in this code block is we have this x which is of
size B BYT right so batch by time and we're going to be in every iteration of
this loop we're going to be adding a column of new indices into each one of these rows right and so these are the
new indices and we're appending them to the the sequence as we're sampling so with each Loop iteration we get one more
column into X and all of the operations happen in the context manager of torch. nograd this is just telling pytorch that
we're not going to be calling that backward on any of this so it doesn't have to cach all the intermediate tensors it's not going to have to
prepare in any way for a potential backward later and this saves a lot of space and also possibly uh some time so
we get our low jits we get the loow jits at only the last location we throw away all the other low jits uh we don't need
them we only care about the last columns low jits so this is being wasteful uh
but uh this is just kind of like an inefficient implementation of sampling um so it's correct but
inefficient so we get the last column of loow jits pass it through soft Max to get our probabilities then here I'm
doing top case sampling of 50 and I'm doing that because this is the hugging face default so just looking at the
hugging face docks here of a pipeline um there's a bunch of
quarks that go into hugging face and I mean it's it's kind of a lot honestly
but I guess the important one that I noticed is that they're using top K by default which is 50 and what that does
is that uh so that's being used here as well and what that does is basically we want to take our probabilities and we
only want to keep the top 50 probabilities and anything that is lower than the 50th probability uh we just
clamp to zero and renormalize and so that way we are never sampling very rare
tokens uh the tokens we're going to be sampling are always in the top 50 of most likely tokens and this helps keep
the model kind of on track and it doesn't blabber on and it doesn't get lost and doesn't go off the rails as easily uh and it kind of like um sticks
in the vicinity of likely tokens a lot better so this is the way to do it in pytorch and you can step through it if
you like I don't think it's super insightful so I'll speed through it but roughly speaking we get this new column
of of tokens we append them on x and basically The Columns of X grow until
this y Loop gets tripped up and then finally we have an entire X of size um 5
by 30 in this case in this example and we can just basically print all those
individual rows so I'm getting all the rows I'm getting all the tokens that were sampled and I'm using the decode
function from Tik tokenizer to get back the string which we can print and so
terminal new terminal and let me python train
gpt2 okay so these are the generations that we're getting hello I'm a language model not a
program um new line new line Etc hello I'm a language model and one of the main
things that bothers me when they create languages is how easy it becomes to create something that I me so this will
just like blabber on right in all these cases now one thing you will notice is that these Generations are not the
generations of hugging face here and I can't find the discrepancy to be honest
and I didn't fully go through all these options but probably there's something else hiding in on addition to the top P
so I'm not able to match it up but just for correctness um down here Below in the juper notebook and using the hugging
face model so this is the hugging face model here I was I replicated the code
and if I do this and I run that then I am getting the same results so basically
the model internals are not wrong it's just I'm not 100% sure what the pipeline does in hugging face and that's why
we're not able to match them up but otherwise the code is correct and we've loaded all the um tensors correctly so
we're initializing the model correctly and everything here works so long story short uh We've Port it all the weights
we initialize the gpt2 this is the exact opening gpt2 and it can generate
sequences and they look sensible and now here of course we're initializing with gbt2 model weights but now we want to
initialize from scratch from random numbers and we want to actually train a model that will give us sequences as
good as or better than these ones in quality and so that's what we turn to
next so it turns out that using the random model is actually fairly straightforward because pytorch already
sample, auto-detect the device
initializes our model randomly and by default so when we create the GPT model
and the Constructor this is all um all of these layers and modules have random
initializers that are there by default so when these linear layers get created and so on there's default Constructors
for example using the Javier initialization that we saw in the past uh to construct the weights of these
layers and so creating a random model instead of a gpt2 model is actually
fairly straightforward and we would just come here and instead we would create model equals GPT and then we want to use
the default config GPT config and the default config uses the 124 M parameters
so this is the random model initialization and we can run
it and we should be able to get uh results now the results here of course
are total garbage carbal and that's because this is random model and so we're just getting all these random token string pieces chunked up totally
at random so that's what we have right now uh now one more thing I wanted to point out by the way is in case you do
not have Cuda available because you don't have a GPU you can still follow along with uh with what we're doing here
uh to some extent uh and probably not to the very end because by the end we're going to be using multiple gpus and
actually doing a serious training run uh but for now you can actually follow along decently okay uh so one thing that
I like to do in pytorch is I like to autod detect the device that is available to you so in particular you
could do that like this so here we are trying to detect a device
to run on that has the highest compute capability you can think about it that way so by default we start with CPU
which of course is available everywhere because every single computer will have a CPU but then we can try to detect do
you have a GPU you so use a Cuda and then if you don't have a Cuda uh do you
at least have MPS MPS is the back end for Apple silicon so if you have a Macbook that is fairly new you probably
have apple silicon on the inside and then that has a GPU that is actually fairly capable uh depending on which
MacBook you have and so you can use MPS which will be potentially faster than CPU and so we can print the device here
now once we have the device we can actually use it in place of Puda so we
just swap it in and notice that here when we call model on X if this x here
is on CPU instead of GPU then it will work fine because here in the forward
which is where P to will come when we create a pose we were careful to use the
device of idx to create this tensor as well and so there won't be any mismatch
where one tensor is on CPU one is on GPU and uh that you can't combine those but here we are um carefully initializing on
the correct device as indicated by the input to this model so this will autod
detect device for me this will be of course GPU so using device
Cuda uh but uh you can also run with um as I mentioned another device and it's
not going to be too much slower so if I override device here oops if I override device equals
CPU then we'll still print Cuda of course but now we're actually using CPU one 2 3
4 5 6 okay about 6 seconds and actually
we're not using torch compile and stuff like that which will speed up everything a lot faster as well but you can follow
even on a CPU I think to a decent extent um so that's note on that okay so I do
want to loop around eventually into what it means to have different devices in pytorch and what it is exactly that
pytorch does in the background for you when you do something like module. 2 device or where you take a torch tensor
and do A2 device and what exactly happens and how that works but for now I'd like to get to training and I'd like
let’s train: data batches (B,T) → logits (B,T,C)
to start training the model and for now let's just say the device makes code go fast um and let's go into how we can
actually train the model so to train the model we're going to need some data set and for me the best debugging simplest
data set that I like to use is the tiny Shakespeare data set um and it's available at this URL so you can W get
it or you can just search tiny Shakespeare data set and so um I have in my file system
as just LS input.txt so I already downloaded it and here I'm
reading the data set getting the first 1,000 characters and printing the first 100
now remember that gpt2 has uh roughly a compression ratio the tokenizer has a
compression ratio of rly 3 to1 so th000 characters is roughly 300 tokens here uh
that will come out of this in the slice that we're currently getting so this is the first few uh
characters and uh if you want to get a few more statistics on this we can do work count on input.txt
so we can see that this is uh 40,000 lines about 200,000 words in this data
set and about 1 million bytes in this file and knowing that this file is only
asky characters there's no crazy unic code here as far as I know and so every asky character is encoded with one bite
and so this is uh the same number roughly a million characters inside this data set so that's the data set size uh
by default very small and minimal data set for debugging to get us off the ground in order to tokenize this data
set we're going to get Tik token encoding for gbt2 encode the data uh the
first um 1,000 characters and then I'm only going to print the first 24 tokens
so these are the tokens as a list of integers and if you can read gpt2 tokens
you will see that 198 here you'll recognize that as the slashing character so that is a new line and then here for
example we have two new lines so that's 198 twice here uh so this is just a tokenization of the first 24 tokens so
what we want to do now is we want to actually process these token sequences and feed them into a Transformer and in
particular we want them we want to rearrange these tokens into this idx
variable that we're going to be feeding into the Transformer so we don't want a single very long onedimensional sequence
we want an entire batch where each sequence is up to uh is basically T
tokens and T cannot be larger than the maximum sequence length and then we have these t uh tlong uh sequences of tokens
and we have B independent examples of sequences so how can we create a b BYT
tensor that we can feed into the forward out of these onedimensional sequences so here's my favorite way to
to achieve this uh so if we take torch and then we create a tensor object out of this list of integers and just the
first 24 tokens my favorite way to do this is basically you do a do view of um
of uh for example 4x6 which multiply to 24 and so it's just a two-dimensional
rearrangement of these tokens and you'll is that when you view this onedimensional sequence as two-dimensional 4x6 here the first six
uh tokens uh up to here end up being the first row the next six tokens here end
up being the second row and so on and so basically it's just going to stack up this the um every six tokens in this
case as independent rows and it creates a batch of tokens in this case and so for example if we are token 25 in the
Transformer when we feed this in and this becomes the idx this token is going to see these three tokens and it's going
to try to predict that 198 comes next so in this way we are able to
create this two-dimensional batch that's that's quite nice now in terms of the label that we're going to need for the
Target to calculate the loss function how do we get that well we could write some code inside the forward pass
because we know that the next uh token in a sequence which is the label is just to the right of us but you'll notice
that actually we for this token at the very end 13 we don't actually have the next correct token because we didn't
load it so uh we actually didn't get enough information here so I'll show you
my favorite way of basically getting these batches and I like to personally have not just the input to the
Transformer which I like to call X but I also like to create the labels uh tensor
which is of the exact same size as X but contains the targets at every single position
and so here's the way that I like to do that I like to make sure that I fetch plus one uh token because we need the
ground Truth for the very last token uh for 13 and then when we're creating the
input we take everything up to the last token not including and view it as 4x6
and when we're creating targets we do the buffer but starting at index one not
index zero so we're skipping the first element and we view it in the exact same size and then when I print this
here's what happens where we see that basically as an example for this token 25 its Target was 198 and that's now
just stored at the exact same position in the Target tensor which is 198 and also this last token 13 now has its
label which is 198 and that's just because we loaded this plus one here so
basically this is the way I like to do it you take long sequences you uh view them in two- dimensional terms so that
you get batch of time and then we make sure to load one additional token so we
basically load a buffer of tokens of B * t+ one and then we sort of offset things
and view them and then we have two tensors one of them is the input to the Transformer and the other exactly is the
labels and so let's now reorganize this code and um create a very simple data
loader object that tries to basically load these tokens and um feed them to the Transformer and calculate the loss
okay so I reshuffled the code here uh accordingly so as you can see here I'm temporarily overwriting U to run a CPU
and importing TI token and all of this should look familiar we're loading a th000 characters I'm setting BT to just
be 4 and 32 right now just because we're debugging we just want to have a single batch that's very small and all of this
should now look familiar and follows what we did on the right and then here we get the we create the model and get
the lits and so so here as you see I already ran this only runs in a few
seconds but because we have a batch of uh 4X 32 our lits are now of size 4X 32x
50257 so those are the lit for what comes next at every position and now we
have the labels which are stored in y so now is the time to calculate the loss and then do the backward pass and then
the optimization so let's first calculate the loss okay so to calculate the loss we're
cross entropy loss
going to adjust the forward function of this NN module in the model and in particular we're not just going to be
returning logits but also we're going to return the loss uh and we're going to not just pass in the input in thees but
also the targets uh in y and now we will
print not Lo just. shape anymore we're actually going to print the loss function and then c. exit of zero so
that we skip some of the sampling logic so now let's swing up to the forward function which gets called there because
now we also have these optional targets and when we get the targets we
can also calculate uh the loss and remember that we want to basically return uh log just loss and loss by
default is none but um let's put this here if uh targets is
not none then we want to calculate loss and co-pilot is already getting excited
here and calculating the what looks to be correct loss it is using the cross entropy loss as is documented here uh so
this is a function in pytorch under the functional now what is actually
happening here because it looks a little bit scary uh basically uh the F that cross entropy does not like
multi-dimensional inputs it can't take a b BYT by vocap size so what's happening here is that we are flattening out this
three-dimensional tensor into just two Dimensions the First Dimension is going to be calculated automatically and it's
going to be B * T and then the last Dimension is vocap size so basically
this is uh flattening out this three-dimensional tensor of logits to just be two- dimensional B * T all
individual examples and vocap size on uh in terms of the length of each row and
then it's also flattening out the targets which are also two- dimensional at this stage but we're going to just
flatten them out so they're just a single tensor of B * T and this can then pass into cross entropy to calculate a
loss which we return so this should basically at this point run because this
is not too complicated so let's run it and let's see if we
should be printing the
loss and here we see that we printed 11 uh roughly and so
um and notice that this is the tensor of a single element which is this number 11
now we also want to be able to calculate a reasonable uh kind of starting point for a random rationalized Network so we
covered this in previous videos but our vocabulary size is 50257 at initialization of the network
you would hope that um every vocab element is getting roughly a uniform
probability uh so that we're not favoring at initialization any token way too much we're not confidently wrong at
initialization so what we're hoping is that the probability of any arbitrary token is roughly 1 over 50,2 57 and now
we can sanity check the loss because remember that the cross entropy loss is just basically the negative um log
likelihood so if we now take this probability and we take it through the natural logarithm and then we do the
negative that is the loss we expect at initialization and we covered this in previous videos so I would expect
something around 10.82 and we're seeing something around 11 so it's not way off this is roughly the probability I expect
at initialization so that tells me that the at initialization or probability distribtion is roughly diffused it's a
good starting point and we can now uh perform the optimization and tell the network which elements you know should
follow correctly in what order so at this point we can do a l step backward calculate the gradients and do an
optimization so let's get to that okay so let's do the optimization now um so
optimization loop: overfit a single batch
here we have the loss is this is how we get the loss but now basically we want a load
for Loop here so 4 I in range let's do 50 steps or something like that uh let's create an Optimizer object in
pytorch um and so here we are using the atom um Optimizer which is an
alternative to the stochastic radian descent Optimizer SGD that we were using so SGD is a lot simpler atom is a bit
more involved and I actually specifically like the atom W variation because in my opinion it kind of just
like fixes a bug um so adom w is a bug fix of atom is what I would say when we
go to the documentation for atom W oh my gosh we see um that it takes a bunch of
hyper parameters and it's a little bit more complicated than the SGD we were looking at before uh because in addition
to basically updating the parameters with the gradient uh scaled by the Learning rate it keeps these buffers
around and it keeps two buffers the m and the V which it calls the first and the second moment so something that
looks a bit like momentum and something that looks a bit like RMS prop if you're familiar with it but you don't have to
be it's just kind of a normalization that happens on each gradient element individually and speeds up the
optimization especially for language models but I'm not going to go into the detail right here we're going to treat
it as a bit of a black box and it just optimizes um the objective faster than
SGD which is what we've seen in the previous lectures so let's use it as a black box in our case uh create the
optimizer object and then go through the optimization
the first thing to always make sure the co-pilot did not forget to zero the gradients so um always remember that you
have to start with a zero gradient then when you get your loss and you do a DOT backward dot backward adds to gradients
so it deposits gradients it it always does a plus equals on whatever the gradients are which is why you must set
them to zero so this accumulates the gradient from this loss and then we call the step function on the optimizer to um
update the parameters and to um decrease the loss and then we print a step and the
loss do item is used here because loss is a tensor with a single element do item will actually uh convert that to a
single float and this float will live not will will live on the CPU so this
gets to some of the internals again of the devices but loss is a is a tensor with a single element and it lifts on
GPU for me because I'm using gpus when you call item P torch behind the scenes
will take that one-dimensional tensor ship it back to the CPU uh memory and convert it into a float that we can just
print so this is the optimization and this should probably just
work let's see what happens actually sorry let me instead of
using CPU override let me delete that so this is a bit faster for me and it runs on Cuda
oh expected all tensors to be on the same device but found at least two devices Cuda zero and CPU so Cuda zero
is the zeroth GPU because I actually have eight gpus on this box uh so the zeroth GPU in my box and CPU and model
we have moved to device but when I was writing this code I actually introduced a bug because buff we never moved to
device and you have to be careful because you can't just do buff dot two of
device um it's not stateful it doesn't convert it to be a device it instead uh
returns pointer to a new memory which is on the device so you see how we can just do model that two a device that does not
apply to tensors you have to do buff equals um b.2 device and then this should work
okay so what do we expect to see we expect to see a reasonable loss in the beginning and then we continue to
optimize just the single batch and so we want to see that we can overfit this single batch we can we can crush this
little batch and we can perfectly predict the indices on just this little batch and indeed that is roughly what
we're seeing here so um we started off at roughly 10.82 11
in this case and then as we continue optimizing on this single batch without loading new examples we are making sure
that we can overfit a single batch and we are getting to very very low loss so the Transformer is memorizing this
single individual batch and one more thing I didn't mention is uh the learning rate here is 3 E4 which is a
pretty good default for most uh optimizations that you want to run at a very early debugging stage so this is
our simple inter Loop and uh we are overfitting a single batch and this looks good so now what uh what comes
next is we don't just want to overfit a single batch we actually want to do an optimization so we actually need to iterate these XY batches and create a
little data loader uh that makes sure that we're always getting a fresh batch and that we're actually optimizing a
reasonable objective so let's do that next okay so this is what I came up with and I wrote a little data loader
data loader lite
light um so what this data loader does is we're importing the token up here
we're reading the entire text file from this single input.txt tokenizing it and then we're just
printing the number of tokens in total and the number of batches in a single Epoch of iterating over this data set so
how many unique batches do we output before we loop back around the beginning of the document and start reading it
again so we start off at position zero and then we simply walk the document in
batches of B * T so we take chunks of B * T and then always Advance by B * T and
um it's important to note that we're always advancing our position by exactly B * T but when we're fetching the tokens
we're actually fetching from current position to B * t + 1 and we need that
plus one because remember uh we need the target token um for the last token in the current
batch and so that way we can do um the XY exactly as we did it before and if we
are to um run out of data we'll just loop back around to zero so this is one
way to write a very very simple data loader um that simply just goes through the file in chunks and is good enough
for us uh for current purposes and we're going to complexify it later and now
we'd like to come back around here and we'd like to actually use our data loader so the import Tik token has moved
up and actually all of this is now useless so instead we just want a train
loader for the training data and we want to use the same hyper parameters for four so B size was four and time was
32 and then here we need to get the XY for the current batch so let's see if
copal gets it because this is simple enough uh so we call the next batch and then we um make sure that we have to
move our tensors from CPU to the device
so here when I converted the tokens notice that I didn't actually move these tokens to the GPU I left them on CPU
which is the default um and that's just because I'm trying not to waste too much memory on the GPU in this case this is a
tiny data set and it would fit uh but it's fine to just uh ship it to GPU right now for for our purposes right now
so we get the next batch we keep the data loader simple CPU class and then here we actually ship it to the GPU and
do all the computation and uh let's see if this runs so python train gbt2 pi and
what do we expect to see before this actually happens what we expect to see is now we're actually getting the next batch so we expect to not overfit a
single batch and so I expect our loss to come down but not too much and that's
because I still expect it to come down because in the 50257 tokens many of those tokens never
occur in our data set so there are some very easy gains to be made here in the optimization by for example taking the
biases of all the loits that never occur and driving them to negative infinity and that would basically just it's just
that all of these crazy unic codes or different languages those tokens never occur so their probability should be
very low and so the gains that we should be seeing are along the lines of basically deleting the usage of tokens
that never occur that's probably most of the loss gain that we're going to see at this scale right now uh but we shouldn't
come to a zero uh because um we are only doing 50 iterations and I don't think
that's enough to do an eoch right now so let's see what we got we um we have 338,000
tokens which makes sense with our 3:1 compression ratio because there are 1 million uh characters so one Epoch with
the current setting of B and T will take 2, 600 batches and we're only doing 50
batches of optimization in here so we start off in a familiar territory as expected and then we seem
to come down to about 6.6 so basically things seem to be working okay right now
with respect to our expectations so that's good okay next I want to actually fix a bug that we have in our code um
parameter sharing wte and lm_head
it's not a major bug but it is a bug with respect to how gpt2 training uh should
happen um so the buck is the following we were not being careful enough when we were
loading the weights from hugging face and we actually missed a little detail so if we come
here notice that um the shape of these two tensors is the same so this one here
is the token embedding at the bottom of the Transformer right so and this one here
is the language modeling head at the top of the Transformer and both of these are
basically two-dimensional tensors and they shape is identical so here the
first one is the output embedding the token embedding and the second one is this linear layer at the very top the
classifier layer both of them are of shape 50257 X
768 um this one here is giving us our token embeddings at the bottom and this
one here is taking the 768 channels of the Transformer and trying to upscale
that to 50, 257 to get the Lis for the next token so they're both the same
shape but more than that actually if you look at um comparing their elements um
in pytorch this is an element wise equality so then we use do all and we see that every single element is
identical and more than that we see that if we actually look at the data pointer
uh this is what this is a way in pytorch to get the actual pointer to the uh data and the storage we see that actually the
pointer is identical so not only are these two separate tensors that happen to have the same shape and elements
they're actually pointing to the identical tensor so what's happening here is that this is a common weight
tying scheme uh that actually comes from the original um from the original attention is all
you need paper and actually even the reference before it so if we come
here um eddings and softmax in the attention
is all you need paper they mentioned that in our model we shared the same weight Matrix between the two embedding
layers and the pre softmax linear transformation similar to 30 um so this
is an awkward way to phrase that these two are shared and they're tied and they're the same Matrix and the 30
reference is this paper um so this came out in 2017 and you can read the full paper but
basically it argues for this weight tying scheme and I think intuitively the
idea for why you might want to do this comes from from this paragraph here and basically you you can observe
that um you actually want these two matrices to behave similar in the
following sense if two tokens are very similar semantically like maybe one of
them is all lowercase and the other one is all uppercase or it's the same token in a different language or something like that if you have similarity between
two tokens presumably you would expect that they are uh nearby in the token embedding space but in the exact same
way you'd expect that if you have two tokens that are similar semantically you'd expect them to get the same
probabilities at the output of a transformer because they are semantically similar and so both
positions in the Transformer at the very bottom and at the top have this property that similar tokens should have similar
embeddings or similar weights and so this is what motivates their exploration
here and they they kind of you know I don't want to go through the entire paper and and uh you can go through it
but this is what they observe they also observe that if you look at the output embeddings they also behave like word
embeddings um if you um if you just kind of try to use those weights as word
embeddings um so they kind of observe this similarity they try to tie them and they observe that they can get much
better performance in that way and so this was adopted and the attention is all need paper and then it was used
again in gpt2 as well so I couldn't find it in the
Transformers implementation I'm not sure where they tie those embeddings but I can find it in the original gpt2 code U
introduced by open aai so this is um openai gpt2 Source model and here where
they are forwarding this model and this is in tensorflow but uh that's okay we see that they get the wte token
embeddings and then here is the incoder of the token embeddings and the
position and then here at the bottom they Ed the WT again to do the lits so
when they get the loits it's a math Mo of uh this output from the Transformer and the wte tensor is
reused um and so the wte tensor basically is used twice on the bottom of the Transformer and on the top of the
Transformer and in the backward pass we'll get gradients contributions from both branches right and these gradients
will add up um on the wte tensor um so we'll get a contribution from the
classifier list and then at the very end of the Transformer we'll get a contribution at the at the bottom of it float floating
again into the wte uh tensor so we want to we are currently not sharing WT and
our code but we want to do that um
so weight sharing scheme um and one way to do this let's see if goil gets it oh
it does okay uh so this is one way to do it
uh basically relatively straightforward what we're doing here is we're taking the wte do weight and we're simply uh
redirecting it to point to the LM head so um this basically copies the data
pointer right it copies the reference and now the wte weight becomes orphaned
uh the old value of it and uh pytorch will clean it up python will clean it up
and so we are only left with a single tensor and it's going to be used twice
in the forward pass and uh this is to my knowledge all that's required so we
should be able to use this and this should probably train uh we're just going to basically be using this exact
same sensor twice and um we weren't being careful with
tracking the likelihoods but uh according to the paper and according to the results you'd actually expect slightly better results doing this and
in addition to that one other reason that this is very very nice for us is that this is a ton of parameters right
uh what is the size here it's 768 * 50257 so This Is 40 million parameters
and this is a 124 million parameter model so 40 divide 124 so this is like
30% of the parameters are being saved using this weight time scheme and so
this might be one of the reasons that this is working slightly better if you're not training the model long enough because of the weight tying uh
you don't have to train as many parameters and so you become more efficient um in terms of the training
process uh because you have fewer parameters and you're putting in this inductive bias that these two embeddings
should share similarities between tokens so this is the way time scheme and we've
saved a ton of parameters and we expect our model to work slightly better because of the scheme okay next I would
model initialization: std 0.02, residual init
like us to be a bit more careful with the initialization and to try to follow the way gpt2 initialized their model now
unfortunately the gpt2 paper and the gpt3 paper are not very explicit about initialization so we kind of have to
read between the lines uh and instead of going to the paper which is quite vague um there's a bit of information in the
code that open I released so when we go to the model.py we see that when they initialize their weights they are using
the standard deviation of 0.02 and that's how they they so this is
a normal distribution for the weights and the standard deviation is 0.02 for the bias they initialize that
with zero and then when we scroll down here why is this not scrolling
um the token embeddings are initialized at 0.02 and position embeddings at 0.01 for
some reason so those are the initializations and we'd like to mirror that in gpt2 uh in our module here so here's a
snippet of code that I sort of came up with very quickly so what's happening here is at
the end of our initializer for the GPT module we're calling the apply function of NN module and that iterates all the
sub modules of this module and uh applies in it weights function on them
and so what's happening here is that we're in we're iterating all the modules here and if they are an nn. linear
module then we're going to make sure to initialize the weight using a normal with the standard deviation of
0.02 if there's a bias in this layer we will make sure to initialize that to zero note that zero initialization for
the bias is not actually the pyto default um by default the bias here is
initialized with a uniform so uh that's interesting so we make sure to use zero
and for the embedding we're just going to use 0.02 and um keep it the same um so we're not going to change it to 0.01
for positional because it's about the same and then if you look through our model the only other layer that requires
initialization and that has parameters is the layer norm and the fighter defer initialization sets the scale in the
layer Norm to be one and the offset in the layer Norm to be zero so that's exactly what we want and so we're just
going to uh keep it that way and so this is the default initialization if we are
following the um where is it the uh gpt2
uh source code that they released I would like to point out by the way that um typically the standard deviation here
on this initialization if you follow the Javier initialization would be one of over the square root of the number of
features that are incoming into this layer but if you'll notice actually 0.02 is basically consistent with that
because the the model sizes inside these Transformers for gpt2 are roughly 768 1600 Etc so 1 over the square root of
for example 768 gives us 0.03 if we plug in 600 1,600 we get
0.02 if we plug in three times that 0.014 Etc so basically 0.02 is roughly
in the vicinity of reasonable values for the for um for these initializations
anyway so so it's not uh completely crazy to be hard coding 0.02 here uh but
you'd like typically uh some something that grows with the model size instead
but we will keep this because that is the gpt2 initialization per their source code but we are not fully done yet on
initialization because there's one more caveat here so here a mod initialization which accounts
for the accumulation on the residual path with model depth is used we scale the weight of residual layers of
initialization by factor of one over squ of n where n is the number of residual layers so this is what gbt2 paper says
so we have not implemented that yet and uh we can do so now now I'd like to actually kind of like motivate a little
bit what they mean here I think um so here's roughly what they
mean if you start out with zeros in your residual stream remember that each
residual stream is a is of this form where we continue adding to it X is X
plus something some kind of contribution so every single block of the residual uh
Network contributes some uh amount and it gets added and so what ends up
happening is that the variance of the activations in the residual stream grows
so here's a small example if we start at zero and then we for 100 times uh we
have sort of this residual stream of of 768 uh zeros and then 100 times we add
um random which is a normal distribution zero mean one standard deviation if we
add to it then by the end the residual stream has grown to have standard deviation of 10 and that's just because
um we're always adding um these numbers
and so this scaling factor that they use here exactly compensates for that growth
so if we take n and we basically um scale down every one of these
contributions into the residual stream by one over theare Ro of n so 1 over theun of n is n to the 0.5
right because n the5 is the square root and then one over the square root is n.5
if we scale it in this way then we see that we actually get um
one so this is a way to control the growth of of activations inside the residual
stream in the forward pass and so we'd like to initialize in the same way where these weights that are at the end of
each block so this C uh layer uh the gbt paper proposes to scale down those
weights by one over the square root of the number of residual layers so one crude way to implement
this is the following I don't know if this is uh pyro sanctioned but it works for me is we'll do in the
initialization see that s that do special nanog GPT uh scale in it is one so we're
setting um kind of like a flag for this module there must be a better way in py torch right but I don't
know okay so we're basically attaching this flag and trying to make sure that it doesn't conflict with anything
previously and then when we come down here this STD should be 0.02 by default
but then if haat um module of this thing
then STD * equals
um copal is not guessing correctly uh so we want one over the square root of the number of layers so
um the number of residual layers here is twice times Salt out config layers and then
this times .5 so we want to scale down that standard deviation and this should
be um correct and Implement that I should clarify by the way that the two times number of layers comes from the
fact that every single one of our layers in the Transformer actually has two blocks that add to the ridal pathway
right we have the attention and then the MLP so that's where the two times comes from and the other thing to mention is
that uh what's slightly awkward but we're not going to fix it is that um
because we are weight sharing the wte and the LM head in this iteration of our
old subm modules we're going to actually come around to that tensor twice so we're going to first initialize it as an
embedding with 0.02 and then we're going to come back around it again in a linear and initialize it again using 0.02 and
it's going to be 0.02 because the LM head is of course not not scaled so it's not going to come here it's just it's
going to be basically initialized twice using the identical same initialization but that's okay and then scrolling over
here I added uh some code here so that we have reproducibility um to set the seeds and
now we should be able to python train gpt2 pi and let this running and as far
as I know this is the gpt2 initialization uh in the way we've implemented it right now so this
looks uh reasonable to me okay so at this point we have the gpt2 model we
SECTION 2: Let’s make it fast. GPUs, mixed precision, 1000ms
have some confidence that it's correctly implemented we've initialized it properly and we have a data loader that's iterating through data batches
and we can train so now comes the fun part I'd like us to speed up the training by a lot so we're getting our
money's worth with respect to the hardware that we are uh using here and uh we're going to speed up the training
by quite a bit uh now you always want to start with what Hardware do you have what does it offer and are you fully
utilizing it so in my case if we go to Nvidia SMI we can see
that I have eight gpus and each one of those gpus is an a100 sxm 80 gb so this
is the GPU that I have available to me in this box now when I look when I use
um to spin up these kinds of Boxes by the way my favorite place to go to is Lambda Labs um they do sponsor my
development and that of my projects uh but I this is my favorite place to go
and this is where you can spin up one of these machines and you pay per hour and it's very very simple so I like to spin them up and then
connect vsod to it and that's how I develop now when we look at the A1 100s that are available here a100 80 GB sxm
is the um GPU that I have here and we have a bunch of numbers here for um how
many calculations you can expect out of this GPU so when I come over here and I break in right after here so
python trity so I'm breaking in right after we calculate the loit and
laws and the interesting thing I'd like you to note is when I do lit. dtype this
prints a torch. FL 32 so by default iny torch when you create tensors um and
this is the case for all the activations and for the parameters of the network and so on by default everything is in float 32 that means that every single
number activation or weight and so on is using a float representation that has 32
bits and uh that's actually quite a bit of memory and it turns out empirically that for deep learning as a
computational workload this is way too much and deep learning and the training of these networks can tolerate
significantly lower precisions um not all computational workflows can tolerate small Precision so for example um if we
go back to to the data sheet you'll see that actually these gpus support up to fp64 and this is quite useful I
understand for a lot of um scientific Computing applications and there really need this uh but we don't need that much
Precision for deep learning training So currently we are here fp32 and with this code as it is right
now we expect to get at at most 19.5 Tera flops of performance that means
we're doing 19.5 trillion operations floating Point operations so this is floating Point multiply add most um most
likely and so these are the floating Point operations
uh now notice that if we are willing to go down in Precision so tf32 is a lower
Precision format we're going to see in a second you can actually get an 8X Improvement here and if you're willing
to go down to float 16 or B float 16 you can actually get time 16x performance
all the way to 312 Tera flops you see here that Nvidia likes to site numbers
that have an asterisk here this asterisk uh says with sparsity uh but we are not going to be using sparsity in R code and
I don't know that this is very widely used in the industry right now so most people look at this number here uh
without sparcity and you'll notice that we could have got even more here but this is int 8 and int 8 is used for
inference not for training uh because int 8 has a um it basically has um
uniform spacing um and uh we actually require a float so that we get a better match to
the uh normal distributions that occur during training of neural networks where
both activations and weights are distributed as a normal distribution and so uh floating points are really
important to to match that uh representation so we're not typically using int 8 uh for training but we are
using it for inference and if we bring down the Precision we can get a lot more Terra flops out of the tensor course
available in the gpus we'll talk about that in a second but in addition to that if all of these numbers have fewer bits
of representation it's going to be much easier to move them around and that's where we start to get into the memory
bandwidth and the memory of the model so not only do we have a finite capacity of the number of bits that our GPU can
store but in addition to that there's a speed with which you can access this memory um and you have a certain memory
bandwidth it's a very precious resource and in fact many of the deep learning uh
work workloads for training are memory bound and what that means is actually that the tensor cores that do all these
extremely fast multiplications most of the time they're waiting around they're idle um because we can't feed them with
data fast enough we can't load the data fast enough from memory so typical utilizations of your Hardware if you're
getting 60% uh utilization you're actually doing extremely well um so half
of the time in a well-tuned application your tensor cores are not doing multiplies because the data is not
available so the memory bandwidth here is extremely important as well and if we come down in the Precision for all the
floats all the numbers weights and activations suddenly require less memory so we can store more and we can access
it faster so everything speeds up and it's amazing and now let's reap the benefits of it um and let's first look
at the tensor float 32 format okay so first of all what are tensor cores well tensor course tensor
Tensor Cores, timing the code, TF32 precision, 333ms
core is just an instruction in the a100 architecture right so so what it does is
it does basically a little 4x4 Matrix multiply so uh this is just matrix
multiplication here of 4x4 matrices and there are multiple configurations as to
what Precision any of these matrices are it in what Precision the internal accumulate happens and then what is the
output Precision input precisions Etc so there's a few switches but it's basically a 4x4 multiply and then
anytime we have any operations that require Magic multiplication uh they get broken up into these into this
instruction of little 4x4 multiply and so everything gets broken up into this instruction because it's the fastest way
to multiply matrices and it turns out that most of the computational work that we're doing up above uh all of it really
is matrix multiplication most of the work computationally happens in the linear layers um linear linear Etc
there's a few things sandwiched in between so there's some additions in residuals there's some G nonlinearities
there's some layer Norms Etc but if you just time them you'll see that these are nothing like basically the in
Transformer is just a bunch of Matrix multiplications really um and especially
at this small scale 124 million parameter model actually the biggest matrix multiplication by far is the
classifier layer at the top that is a massive Matrix multiply of going from 768 to
50257 and that Matrix multiply dominates anything else that happens in that Network roughly speaking so it's Matrix
multiplies that become a lot faster which are hidden inside our linear layers and they're accelerated through
tensor course now the best reference I would say for tensor course is basically just go to the um a 100 architecture
white paper and then it's pretty detailed and but I think people it's
like relatively readable mostly if you half understand what's happening um so
figure 9 tensor float 32 so this is the explanation basically for tf32 and what happens here and you
see that there's many configuration options here available so the input operands and what precisions are they in
the accumulator and um what um basically the um the internal representation
within the instruction when you do the accumulate of this matrix multiplication so the intermediate plus
equals um of the intermediate little vector multiplies here that all happens in
fp32 and then uh this is an aex improvement as I mentioned to the Ops that we get so tf32 specifically we're
looking at this row here and the way this works is
um normally fp32 has 32 bits tf32 is the exact same bits we have one
sign bit we have eight exponent bits except the mantisa bits get cropped in
the float and so basically um we end up with just 19 bits instead of 32 bits
because the last 133 bits get truncated they get dropped um and all this is
internal to the instruction so none of it is visible to anything in our pytorch uh none of our pytorch code will change
all of the numbers will look identical it's just that when you call the tensor core um instruction internally in the
hardware it will crop out these 13 bits and that allows it to uh calculate this
little Matrix multiply significantly faster 8X faster now of course this speed up comes at a cost and the cost is
that we are reducing the Precision our accumulate is still an fp32 our output is fp32 our inputs are fp32 but
internally things get truncated in the operand to perform the operation faster and so our results are starting to be a
bit more approximate but empirically when you actually train with this you basically can't tell the difference
so the reason I like tf32 is because if you can tolerate a little bit of a Precision fudge um then this is free
like none of your codes sees this it's fully internal to the operation and the operation to you just go 8X faster and
it's a bit more approximate and so it's a pretty sweet spot I would say in optimization and uh let's see what that
looks like first so I've set up our Cod to just time the uh iterations so import
time I changed the hyper parameters so that we have something a bit more that reflects uh kind of workload that we
want to run uh because we want to do a fairly large run at the end of this so let's use batch size 16 and let's now
use the actual gpt2 um maximum sequence length of 10,24 tokens uh so this is the
configuration and then for 50 iterations I'm just doing something very lazy here
I'm doing time. time to get the current time and then this is the optimization Loop and now I want to time how long
this takes now one issue with working with gpus is that as your
CPU um when your CPU runs it's just scheduling work on GPU it's ordering
some work right and so it send a request and then it continues running and so we
can actually it can happen sometimes that we sort of um speed through this and we queue up a lot of kernels to run
on the GPU and then the CPU sort of like gets here and takes time at time but actually the GPU is still running
because it takes it time to actually work through the work that was scheduled to run and so you're just building up a
queue for the GPU and so actually if you need to you want to wait toat data synchronize and this will wait for the
GPU to finish all the work that was scheduled to run up above here and then
we can actually take the time so basically we're waiting for the GPU to stop this iteration take time and then
we're going to just print it so so here I'm going to run the training Loop and here on the right I'm watching
Nvidia SMI so we start off at zero um we're not using the GPU and then by
default P will use gpu0 so we see that it gets filled up and we're using 35 GB
out of 80 gabt available and then here on the left we see that because we've cranked up the
batch size now it's only 20 batches to do a single Epoch on our tiny Shakespeare
and we see that we're seeing roughly a th000 milliseconds per iteration here right
so the first iteration sometimes is slower and that's because pytorch might be doing a lot of initializations here
on the very first iteration and so it's probably initializing all these uh tensors and buffers to hold all the
gradients and I'm not 100% sure all the work that happens here but uh this could be a slower iteration when you're timing
your logic you always want to be careful with that but basically we're seeing a th000 milliseconds per iteration
um and so this will run for roughly 50 seconds as we have it right now so
that's our Baseline in flo 32 one more thing I wanted to mention is that if
this doesn't fit into your GPU and you're getting out of memory errors then start decreasing your batch size until
things fit so instead of 16 try eight or four or whatever you need to fit um the
batch into your GPU and if you have a bigger GPU you can actually potentially get away with 32 and so on uh by default
you want to basically max out has Max Max out the batch size that fits on your GPU and you want to keep it nice numbers
so use numbers that have lots of powers of two in them so 16 is a good number 8
24 32 48 These are nice numbers but don't use something like 17 uh because
that will run very inefficiently on a GPU uh and we're going to see that a bit later as well so for now let's just
stick with 16124 and uh the one thing that I added also here and I ran it again is I'm
calculating a tokens per second throughput during training because we might end up changing the
backat size around over time but tokens per second is the objective measure that we actually really care about how many
tokens of data are we training on and what is the throughput of tokens that we're getting in our optimization so
right now we're processing and training on 163,000 tokens per second roughly and
that's a bit more objective metric okay so let's now enable tf32 now
luckily pytorch makes this fairly easy for us and uh to enable tf32 you just
need to do a single line and is this and when we go to the py documentation here
for this function basically this tells pych what kind of kernels to run and by
default I believe it is highest highest Precision for mat M and that means that
everything happens in float 32 just like it did before but if we set it to high as we do right now Matrix
multiplications will not use tensor flow 32 when it's available my GPU is a100 so it's an
ampere series and therefore tf32 is available if you have an older GPU this
might not be available for you but for my GPU it's available and so what I expect P to do is that every single
place where we see an nn. linear inside there there's a matrix multiplication and I expect that matrix multiplication
now to be um running on tensor course utilizing the TF 32%
so this is the single line of change that is I believe necessary and let's rerun this now we saw that um in terms
of the throughput that is promised to us we're supposed to be getting 8X roughly
so let's see what happens and that 8X came from here right
um 8X and it also came from looking at it um here 156 T flops instead of of
19.5 okay so what actually happened uh so we're seeing that our throughput
roughly 3x not aex so we are going we're
from 1,000 milliseconds we're going down to 300 milliseconds and our throughput is now about 50,000 tokens per second so
we have a roughly 3x instead of 8X so what happened and basically What's Happening Here is again a lot of these
workloads are memory bound and so even though the tf32 offers in principle a lot faster
throughput all of these numbers everywhere are still float 32s and it's float 32 numbers that are being shipped
all over the place through the memory system and is just costing us way too much time to shuttle around all this
data and so even though we've made the multiply itself much faster uh we are memory bound and we're not actually
seeing the full benefit uh that would come from uh this napkin math here uh
that said we are getting one a 3X faster throughput and this is free um single
line of code in P torch all your variables are still float 32 everywhere it just runs faster and it's slightly
more approximate but we're not going to notice it basically uh so that's
tf32 okay so let's now continue so we've exercised this row and um we saw that we
float16, gradient scalers, bfloat16, 300ms
can crop out some of the Precision inside the operation itself but we saw that we're still memory bound we're
still moving around all these floats right otherwise and we're paying that cost because of this so let's now
decrease the amount of stuff that we're going to be moving around and we're going to do that by dropping down to B
float 16 so we're only going to be maintaining 16 bits per float and we're
going to use the B flat 16 and I'll explain in a bit uh fp16 difference and uh we're going to be in this row so when
we go back to the documentation here for the a 100 um we see here the precisions that
are are available and this is the original fp32 the tf32 crops out the
Precision and then here in bf16 you see that it is very similar to
tf32 but it's even more aggressive in cropping off of the Precision the
mantisa of this float so the important thing with B float 16 is that the exponent bits and the sign bit of course
remain unchanged so if you're familiar with your float numbers and I think this should should probably be an entire
video by itself the exponent sets the range that you can represent of your numbers and the
Precision is how much Precision you have for your numbers and so the range of
numbers is identical but we can we have fewer possibilities within that range
because we are truncating the Mena so we have less Precision in that range what that means is that things are
actually fairly nice because we have the original range of numbers that are representable in float but we just have
less Precision for it and the difference with fp16 is that they actually touch
and change the range so fp16 cannot represent the full range of fp32 it has
a reduced range and that's where you start to actually run into issues because now you need uh these gradient
scalers and things like that and I'm not going to go into the detail of that in this video because that's a whole video
by itself but fb16 actually historically came first that was available in the Volta series before Amper and so fp16
came first and everyone started to train in fp16 but everyone had to use all these gradient scaling operations which
are kind of annoying and it's an additional source of state and complexity and the reason for that was
because the exponent range was reduced in fp16 so that's the i e fp16 spec and
then they came out with bf16 and the Ampere and they made it much simpler because we're just truncating manessa we
have the exact same range and we do not need gradient scalers so everything is much much simpler now when we do use
bf16 though we are impacting the numbers that we might be seeing in our pytorch
code these this change is not just local to the operation itself so let's see how
that works um there's some documentation here that
so I think this is probably the best best page to explain how to use mixed Precision in pytorch um because there
are many other tutorials and so on even within pitor documentation that are a lot more confusing and so I recommend
specifically this one because there's five other copies that I would not recommend and then when we come
here ignore everything about everything ignore everything about gradient scalers and only look at torch.
AutoCast and basically also this comes to a single line of code at the end so this is the context manager that we
want and we want to use that in our Network when you click into the torch.
AutoCast autocasting it has a few more uh a bit more guideline for you so it's
telling you do not call B flat 16 on any of your tensors just use AutoCast and
only surround the uh forward pass of the model and the loss calculation and that's the only two things that you
should be surrounding leave the backward and the optimizer step alone so that's the guidance that comes from the P team
so we're going to follow that guidance and for us because the L calculation is inside of the model forward pass for us
we are going to be doing this and then we don't want to be using torch Flo 16 because if we do that we
need to start using gradient scalers as well so we are going to be using B float 16 this is only possible to do an ampere
uh but this means that the changes are extremely minimal like basically just this one line of
code um let me first break in to here before we actually run this
so right after logits I'd like to show you that different from the tf32 that we
saw this is actually going to impact our tensors so this Lis tensor if we now look at
this and we look at the dtype we suddenly see that this is now B float 16 uh it's not float 32 anymore so our
activations have been changed the activations tensor is now B FL 16 but not everything has changed so model.
Transformer wte uh this is the weight uh token
embedding table it has a weight inside it and the dtype of this weight this
parameter is still torch float 32 so our parameters seem to still be in float 32
but our activations the loits are now in P 16 so clearly this is why we get the
mixed Precision some things pytorch is keeping inlow 32 some things pytorch is
converting to lower Precision um and what gets converted at what point is not
super clear I remember scrolling down is it
here okay I can't find it I I thought it was here okay there we
go so there are a few docks on when you're using this AutoCast what gets
converted to B FL 16 and and when so for example only these Matrix multiply like
operations get converted to float 16 but a lot of operations remain in float 32 so in particular a lot of normalizations
like layer norms and things like that not all of those layers might be converted um so only some layers
selectively would be running B flat 16 but things like softmax uh layer Norms
uh log um log soft Max so loss function calculations a lot of those things might
remain in float 32 because they are more susceptible to Precision changes major multiplies are fairly um
robust to Precision changes uh so some parts of the network are um impacted
more or less by the Precision change um so basically only some parts
of the of the model are running in reduced Precision let's take it for a spin and let's actually see what kind of
improvement we achieve
here okay so we used to be 333 milliseconds we're now 300
and we used to be somewhere around 50,000 tokens per second we're now at 55 so we're definitely running faster but
maybe not a lot faster and that's because there are still many many bottlenecks in our gbt2 we're just
getting started but we have dropped down the precision as far as we can with my current GPU which is a100 we're using
pytorch AutoCast unfortunately I don't actually exactly know what pytorch AutoCast do uh does I don't actually
know exactly what's in B flat 16 what's in float 32 we could go in and we could start to scrutinize it um but these are the kinds
of rules that pytorch has internally and unfortunately they don't documented very well uh so we're not going to go into
that into in too much detail but for now we are training in B flow 16 we do not need a gradient scaler and the reason
things are running faster is because um we are able to run tensor course in B FL
16 now that means we are in this row but uh we are also paying in Precision for
this uh so um we expect slightly less accurate results with respect to the original fp32 but empirically in many
cases this is a worth it uh kind of tradeoff because it allows you to run faster and you could for example train
longer and make up for the uh for that Precision decrease so um that's b46 for
torch.compile, Python overhead, kernel fusion, 130ms
now okay so as we can see we are currently at about 300 milliseconds uh per iteration and we're now going to
reach for some really heavy weapons in the pie torch Arsenal and in particular we're going to introduce torch. compile
so torch. compile is really quite incredible infrastructure from the pytorch team and it's basically a
compiler for neural networks like it's almost like GCC for CN C++ code this is
just this GCC of neural nuts so came out a while ago and extremely simple to use
um the way to use torch compile is to do this it's a single line of code to compile your model and return it now
this line of code will cost you compilation time but as you might guess it's going to make the code a lot faster
so let's actually run that because this will take some time to run but currently remember we're at 300 milliseconds and
we'll see what happens now while this is running I'd like to explain a little bit of what torch. compile does under the
hood uh so feel free to read this page of P torch but basically there's no real
good reason for you to not use torch compile in your pie torch I kind of feel like you should be using almost by
default if you're not uh unless you're debugging and you want your code to run really fast and there's one line here in
torch compile that I found that actually kind of like gets to why this is faster speed up mainly comes from reducing
python overhead and GPU read wrs so let me unpack that a little bit um okay here
we are okay so we went from 300 milliseconds we're now running at 129
milliseconds so this is uh 300 129 about 2.3x Improvement from a single line of
code in py torch uh so quite incredible so what is happening what's happening under the hood well when you pass the
model to torch compile what we have here in this NN module this is really just the
algorithmic description of what we'd like to happen in our Network and torch
compile will analyze the entire thing and it will look at what operations You' like to use and with the benefit of
knowing exactly what's going to happen it doesn't have to run in What's called the e mode it doesn't have to just kind
of like go layer by layer like the python interpreter normally would start
at the forward and the python interpreter will go okay let's do this operation and then
let's do that operation and it kind of materializes all the operations as it goes through uh so these um calculations
are dispatched and run in this order and the python interpreter and this code doesn't know what kind of operations are
going to happen later but torch compile sees your entire code at the same time and it's able to know what operations
you intend to run and it will kind of optimize that process the first thing it will do is will it will take out the
python interpreter from the forward pass entirely and it will kind of compile this entire neural net as a single
object with no python interpreter involved so it knows exactly what's going to run and we'll just run that and
it's all going to be running in efficient code uh the second thing that happens is
uh this read write that they mentioned very briefly so a good example of that I think is the G nonlinearity that we've
been looking at so here we use the n and G now this here is me uh basically just
breaking up the inang Galu uh which you remember has this formula so this here
is the equivalent implementation to what's happening inside g algorithmic l it's identical Now by default if uh we just
we using this instead of ending. G here what would happen without torch compile
well the python interpreter would make its way here and then it would be okay well there's an input well let me first
let me raise this input to the third power and it's going to dispatch a kernel that takes your input and raises
it to the third power and that kernel will run and when this kernel runs what
ends up happening is this input is stored in the memory of the GPU so here's a helpful example of the layout
of what's happening right you have your CPU this is in every single computer there's a few cores in there and you
have your uh Ram uh your memory and the CPU can talk to the memory and this is
all well known but now we've added the GPU and the GPU is a slightly different architecture of course they can
communicate and it's different in that it's got a lot more course than a CPU
all of those cores are individually a lot simpler too but it also has memory right this high bandwidth memory I'm
sorry if I'm botching it hbm I don't even know what that stands for I'm just realizing that
but uh this is the memory and it's very equivalent to uh RAM basically in the
computer and what's happening is that input is living in the memory and when you do input
cubed this has to travel to the GPU to the course and to all the caches and
registers on the actual chip of this GPU and it has to calculate the all the
elements to the third and then it saves the result back to the memory and it's this uh travel time that actually causes
a lot of issues so here remember this memory bandwidth we can communicate
about 2 terabytes per second which is a lot but also we have to Traverse this
link and it's very slow so here on the GPU we're on chip and everything is super fast within the chip but going to
the memory is extremely expensive takes extremely long amount of time and so we load the input do the calculations and
load back the output and this round trip takes a lot of time and now right after we do that we
multiply by this constant so what happens then is we dispatch another kernel and then the result travels back
all the elements get multiplied by a constant and then the results travel back to the memory and then we take the
result and we add back input and so this entire thing again travels to the GPU
adds the inputs and gets written back so we're making all these round trips from
the memory to actually where the comput happens because all the tensor cores and alus and everything like that is all
stored on the chip in the GPU so we're doing a ton of round trips and pytorch uh without using torch compile doesn't
know to optimize this because it doesn't know what kind of operations you're running later you're just telling it
raise the power to the third then do this then do that and it will just do that in that sequence but torch compile
sees your entire code it will come here and it will realize wait all of these are elementwise operations and actually
what I'm going to do is I'm going to do a single trip of input to the GPU then for every single element I'm going to do
all of these operations while that memory is on the GPU or chunks of it
rather and then I'm going to write back a single time so we're not going to have these round trips and that's one example
of what's called kernel fusion and is a major way in which everything is sped up so basically if you have your benefit of
onet and you know exactly what you're going to compute you can optimize your round trips to the memory and you're not
going to pay the the memory bandwidth cost and that's fundamentally what makes some of these operations a lot faster
and what they mean by read writes here so let me erase this because we are
not using it and yeah we should be using torch compile and our code is now
significantly faster and we're doing about 125,000 tokens per second but we still
have a long way to go before we move on I wanted to supplement the discussion a little bit with a few more figures uh
because this is a complic topic but it's worth understanding on a high level uh what's happening here and I could
probably spend an entire video of like two hours on this but just the preview of that basically so this chip here that
is uh the GPU this chip is where all the calculations happen mostly but this chip
also does have some memory in it but most of the memory by far is here in the
high bandwidth memory hbm and is connected they're connected um but these
are two separate chips basically now here this is a zoom in of kind of
this cartoon diagram of a GPU and what we're seeing here is number one you see
this hbm I I realize it's probably very small for you but on the sides here it says hbm and so that that's the links to
the hbm now the hbm is again off chip on the chip there are a large number of
these streaming multiprocessors uh every one of these is an SM there's 120 of them in total and
this is where the a lot of the calculations happen and this is a zoom in of a single individual as it has
these four quadrants and see for example tensor core this is where a lot of the Matrix multiply stuff happens but
there's all these other units to do all different kinds of calculations for fp64 fp32 and for integers and so on now so
we have all this uh logic here to do the calculations but in addition to that on the chip there is memory sprinkled
throughout the chip so L2 cache is some amount of memory that lives on the chip
and then on the SMS themselves there's L1 cache I realized it's probably very small for you but this blue bar is L1
and there's also registers um and so there is memory stored here but the way
this memory is stored is very different from the way memory is stored in hbm uh this is a very different implementation
uh using um just in terms of like what the Silicon looks like it's a very different
implementation um so here you would using transistors and capacitors and here it's a very different
implementation uh with SRAM and what that looks like but long story short is
um there is um memory inside the chip but it's not a lot of memory that's the
critical point so this is some C this is a example diagram of a slightly different GPU just like here where it
shows that for example typical numbers for CPU Dam memory which is this thing here you might have one tab of this
right but it would be extremely expensive to access especially for a GPU you have to go through the CPU here now
next we have the hbm so we have tens of gigabytes of hbm memory on a typical GPU here but it's as I mentioned very
expensive to access and then on the chip itself everything is extremely fast
within the chip but we only have couple 10 megabytes of memory collectively
throughout the Chip And so there's just not enough space because the memory is very expensive on the chip and so
there's not a lot of it but it is lightning fast to access in relative terms and so basically whenever we have
these kernels um the more accurate picture of what's Happening Here is that
we take these inputs which live by default on the global memory and now we need to perform some calculation so we
start streaming the data from the um Global memory to the uh chip we perform
the calculations on the chip and then stream it back and store it back to the global memory right and so if we are if
we don't have torch compile we are streaming the data through the chip doing the calculations and saving to the memory and we're doing those round trips
many many times but uh if it's torch compiled then we start streaming the memory as before
but then while we're on the chip we're we're we have a chunk of the uh data
that we're trying to process so that chunk now lives on the chip while it's on the chip it's extremely fast to
operate on so if we have kernel Fusion we can do all the operations right there in an element-wise fashion and those are
very cheap and then we do a single round trip back to the global memory so
operator Fusion basically allows you to keep your chunk of data on the Chip And do lots of calculations on it before you
write it back and that gives huge savings and that's why torch compile ends up being a lot faster or that's one
of the major reasons uh so again just a very brief intro to the memory hierarchy and
roughly what torch compile does for you now torch compile is amazing but there are operations torch compile will not
flash attention, 96ms
find and an amazing example of that is Flash attention to which we turn next so
flash attention comes from this paper from uh Stanford in 2022 and it's this incredible algorithm
for performing attention so um and running it a lot faster so flash
attention will come here and we will take out these four lines and Flash attention implements
these four lines really really quickly and how does it do that well flash attention is a kernel Fusion operation
so you see here we have um in this diagram they're showing P torch and you
have these four operations uh they're including Dropout but we are not using Dropout here so we just have these four
lines of code here and instead of those we are fusing them into a single fused
kernel of flash attention so it's an it's a it's a kernel Fusion algorithm
but it's a kernel Fusion that torch compile cannot find and the reason that it cannot find it is
that it um requires an algorithmic rewrite of how attention is actually implemented here in this case and what's
remarkable about it is that uh flash attention actually if you just count the number of flops flash attention does
more flops than this attention here but flash attention is actually
significantly faster in fact they site 7. six times faster potentially and
that's because it is very mindful of the memory hierarchy as I described it just
now and so it's very mindful about what's in high bandwidth memory what's in the shared memory and it is very
careful with how it orchestrates the computation such that we have fewer reads and writes to the high bandwidth
memory and so even though we're doing more flops the expensive part is they load and store into hbm and that's what
they avoid and so in particular they do not ever materialize this end byend
attention Matrix this ATT here a flash attention is designed such that this
Matrix never gets materialized at any point and it never gets read or written to the hbm and this is a very large
Matrix right so um because this is where all the queries and keys interact and we're sort of getting
um for each head for each batch element we're getting a t BYT Matrix of
attention which is a Million numbers even for a single head at a single batch index at like so so basically this is a
ton of memory and and this is never materialized and the way that this is achieved is that basically the
fundamental algorithmic rewrite here relies on this online softmax trick which was proposed previously and I'll
show you the paper in a bit and the online softmax trick coming from a previous paper um shows how you can
incrementally evaluate a soft Max without having to sort of realize all of
the inputs to the softmax to do the normalization and you do that by having these intermediate variables M and L and
there's an update to them that allows you to evaluate the softmax in an online manner um now flash attention actually
so recently flash attention 2 came out as well so I have that paper up here as well uh that has additional gains to how
it calculates flash attention and the original paper that this is based on basically is this online normalizer
calculation for softmax and remarkably it came out of Nvidia and it came out of it like really early 2018 so this is 4
years before flash attention and this paper says that we propose a
way to compute the classical softmax with fewer memory accesses and hypothesize that this reduction in memory accesses should improve softmax
performance on actual hardware and so they are extremely correct in this
hypothesis but it's really fascinating to me that they're from Nvidia and that they had this realization but they
didn't actually take it to the actual flash attention that had to come four years later from Stanford so I don't
fully understand the historical how this happened historically um but they do basically propose this online update to
the softmax uh right here and this is fundamentally what they reuse here to
calculate the softmax in a streaming Manner and then they realize they can actually fuse all the other operations
with the online sofx calculation into a single fused kernel flash attention and that's what we are about to use so great
example I think of being aware of um memory hierarchy the fact that flops don't matter uh the entire memory access
pattern matters and that torch compile is amazing but there are many optimizations that are still available to us that potentially torch compile
cannot find maybe maybe one day it could but right now it seems like a lot to ask
so here's what we're going to do we're going to use Flash attention and the way to do that basically in pytorch is we
are going to comment out these four lines and we're going to replace them with a single line and here we are
calling this compound operation in pytorch called scale that product attention and uh pytorch will call flash
attention when you use it in this way I'm not actually 100% sure why torch
compile doesn't realize that these four lines should just call flash attention in this exact way we have to do it again
for it which in my opinion is a little bit odd but um here we are so you have
to use this compound up and uh let's wait for a few moments before torch comp
compile gets around to it and then let's remember that we achieved 6.05 661 I
have it here that's the loss we were expecting to see and we took 130 milliseconds uh before this change so
we're expecting to see the exact same result by iteration 49 but we expect to see faster runtime because Flash
attention is just a an algorithmic rewrite and it's a faster kernel but it doesn't actually change any of the computation and we should have the exact
same optimization so okay so we're a lot faster we're at about 95 milliseconds
and we achiev 6.58 okay so they're basically identical
up to a floating Point fudge Factor so it's the identical computation but it's
significantly faster going from 130 to roughly 90 96 and so this is um 96 divide
130ish so this is maybe 27 is% Improvement um so uh really interesting
and that is Flash retention okay we are now getting to one of my favorite optimizations and it is simultaneously
nice/ugly numbers. vocab size 50257 → 50304, 93ms
the dumbest and the most brilliant optimization and it's always a little bit surprising to me um anyway so
basically I mentioned a few minutes ago that there are some numbers that are nice and some numbers that are ugly so
64 is a beautiful nice number 128 is even nicer 256 is beautiful what makes
these numbers beautiful is that there are many powers of two inside them you can divide by two many times and uh
examples of ugly numbers are like 13 and 17 and something like that prime numbers numbers that are not even and so on and
so pretty much you always want to use nice numbers in all of your code that deals with neural networks or Cuda
because everything in Cuda Works in sort of like powers of two and lots of kernels are written in terms of powers
of Two And there are lots of blocks of sizes 16 and uh 64 and so on so
everything is written in those terms and you always have special case handling for all kinds of uh logic that U when
your inputs are not made of nice numbers so let's see what that looks like basically scan your code and look for
ugly numbers is roughly theistic so three times is kind of ugly um I'm not
100% sure maybe this can be improved but this is uh this is ugly and not ideal um four times is nice so that's uh
that's nice 1024 is very nice that's a power of two
12 is a little bit suspicious um not too many powers of two 768 is great 50, 257
is a really really ugly number um it's first of all it's odd so uh and there's
no not too many powers of two in there so this is a very ugly number and it's highly suspicious and then when we
scroll down all these numbers are nice and then here we have mostly nice numbers except for 25 so in this
configuration of gpt2 XL a number of heads is 25 uh that's a really ugly number that's an odd number and um
actually this did cause a lot of headaches for us recently when we're trying to optimize some kernels uh to run this fast um and required a bunch of
special case handling so basically these numbers are we have some ugly numbers
and some of them are easier to fix than others and in particular the voap size being 50257 that's a very ugly number
very suspicious and we want to fix it now when you when you fix these things uh one of the easy ways to do that is
you basically um increase the number until it's the nearest power of two that
you like so here's a much nicer number it's 50304 and why is that because 50304 can
be divided by 8 or by 16 or by 32
64 it can even be divided by 128 I think yeah so it's a very nice number um so
what we're going to do here is the GPT config and you see that we initialized B cap size to
50257 Let's override just that um element to be
50304 okay so everything else stays the same we're just increasing our vocabulary size so we're adding it's
almost like we're adding fake tokens uh so that book up size has powers of two inside it now actually what I'm doing
here by the way is I'm increasing the amount of computation that our network will be doing if you just count the the flops on like do the math of how many
flops we're doing we're going to be doing more flops and we still have to think through whether this doesn't break
anything but if I just run this uh let's see what we get uh currently this ran in
maybe 96.5 milliseconds per step I'm just kind
of like eyeballing it and let's see what kind of a result we're going to get uh while this is compiling let's
think through whether our code actually works okay when we increase the vocap size like this let's look at where vocap
size is actually used so we swing up to the inet and we see that it's used inside the embedding
table of course so all the way at the bottom of the Transformer and it's used at the classifier layer all the way at the top of the Transformer so in two
places and let's take a look and we're running at 93 so 93 milliseconds instead
of 96.5 so we are seeing a roughly yeah 4%
Improvement here uh by doing more calculations and the reason for this is
we fixed we've made an ugly number into a nice number let's I'm going to come
into the explanation for that a little bit again but for now let's just convince ourselves that we're not breaking anything when we do this so
first of all we've made the the wte the embedding table for the tokens we've made it larger it's almost like we
introduced more tokens at the bottom and these tokens are never used because the
gbt tokenizer only has tokens up to $50,000 256 and so we'll never index into the
rows that we've added so we're wasting a little bit of space here by creating memory that's never going to be accessed
never going to be used Etc now that's not fully correct because this wte
weight ends up being shared and ends up being used in the classifier here at the end so what is that doing to the
classifier right here well what what that's doing is we're predicting additional Dimensions at the classifier
now and we're predicting probabilities for tokens that will of course never be present in the training set um and so
therefore the network has to learn that these probabilities uh have to be driven to zero and so the logits that the
network produces have to drive those dimensions of the output to negative Infinity but it that's no different from
all the other tokens that are already in our data set um or rather that are not in our data set so Shakespeare only
probably uses let's say a th000 tokens out of 50,000 to 57 tokens so most of the tokens are already being driven to
zero probability by the optimization we' just introduced a few more tokens now that in a similar manner will never be
used and have to be driven to zero in probability um so functionally though
nothing breaks we're using a bit more extra um memory but otherwise this is a
harmless operation as far as I can tell but and we're adding calculation but it's running faster and it's running
faster because as I mentioned in Cuda so many kernels use uh block tiles and
these block towels are usually nice numbers uh so powers of two so calculations are done in like chunks of
64 or chunks of 32 and when your um when your desired calculation doesn't neatly
fit into those block tiles um there are all kinds of boundary kernels that can
kick in to like do the last part so basically in a lot of kernels they will
chunk at up your input and they will do the nice part first and then they have a whole second second phase where they
come back to any that like uh remains uh and then they process the remaining part
and the kernels for that could be very inefficient and so you're basically um spinning up all this extra compute and
is extremely inefficient so you might as well pad your inputs and um make it fit
nicely and usually that empiric lens up actually running faster um so this is
another example of a 4% Improvement that we've added and this is something that
also torch compile did not find for us you would hope that torch compile at some point could figure an optimization
like this out uh but for now uh this is it and I also have to point out that we're using pytorch nightly so that's
why we're only seeing 4% if you're using pytorch 2.3.1 or earlier you would
actually see something like 30% Improvement just from this change from changing it to from 50,000 to 57 to
50304 so again one of my favorite examples also of having to understand
the under the hood and how it all works and to know what kinds of things to Tinker with to push the performance of your code okay so at this point we have
SECTION 3: hyperpamaters, AdamW, gradient clipping
improved the performance by about 11x right because we started at about 1,000 milliseconds per step and we're now down
to like 93 milliseconds so that's uh quite good and we're uh doing a much
better job of utilizing our GPU resources so I'm going to now turn to more algorithmic changes uh and
improvements to the actual optimization itself and what we would like to do is we would like to follow the hyper parameters that are mentioned in the GP
G2 or gpt2 gpt3 paper now sadly gpt2 is
uh doesn't actually say too much it's very nice of them that they released the model weights and the code but the paper
itself is extremely vague as to the optimization details uh the code itself that they released as well the code
we've been looking at this is just the inference code so there's no training code here and very few hyp parameters so
this doesn't also tell us too much so for that we have to turn to the gpt3 paper and um in the depending of the
gpt3 paper um they have a lot more hyper parameters here for us to use and the
gpt3 paper in general is a lot more detailed as to uh all of the you know
small details that go into the model training but gpt3 U models were never released so gbt2 we have the weights but
no details and gpt3 we have lots of details but no weights so um but roughly
speaking gpt2 and gpt3 architectures are very very similar and um basically there
are very few changes the context length was expanded from 1024 to 2048 and that's kind of like the major change uh
and some of the hyper parameters around the Transformer have changed but otherwise they're pretty much the same model it's just that gpt3 was trained
for a lot longer on a bigger data set and uh has a lot more thorough evaluations uh and the gpt3 model is 175
billion instead of 1.6 billion um in the gpt2 so long story short we're going to
go to gp3 paper to follow along some the hyper parameters so to train all the
versions of gpt3 we use atom with beta 1 beta 2 of9 and .95 so let's swing over
here and make sure that the betas parameter which you can see here defaults to 0.9 and
999 is actually set to 0.9 and .95 and then the Epsilon parameter uh
you can see is the default is 1 in8 and this is also one in8 let's just uh put
it in so that works expit uh now next up they say we clip
the gra Global Norm of the gradient at 1.0 so what this is referring to is that
once we calculate the gradients right after l. backward um we basically have
the gradients at all the parameter tensors and what people like to do is
basically uh clip them to have some kind of a maximum Norm so in pytor this is fairly easy to do uh it's one line of
code here that we have to insert right after we calcul Cal the gradients and what this utility function is doing is
um it's calculating the global Norm of the parameters so every single par um
gradient on all the parameters you square it and you add it all up and you take a big square root of that and
that's the norm of the parameter V Vector basically it's the it's the
length of it if you if you'd like to look at it that way and we are basically making sure that its length is no more
than 1.0 and we're going to clip it and the reason that people like to use this is that uh sometimes you can get
unlucky during your optimization maybe it's a bad data batch or something like that and if you get very unlucky in the
batch you might get really high loss and really high loss could lead to a really high gradient and this could basically
uh shock your model and shock the optimization so people like to use a gradient Norm clipping uh to prevent the
model from um basically getting too big of shocks in terms of the gradient magnet ude and uh the upper bound it in
this way it's a bit of a hacky solution it's about like a patch on top of like deeper issues uh but uh people still do
it fairly frequently now the clip grad Norm Returns the norm of the gradient
which I like to always visualize uh because um it is useful information and
sometimes you can look at the norm of the gradient and if it's well behaved things are good if it's climbing things
are bad and they're destabilizing during training sometimes you could get a spike in the norm and that means there's some
kind of an issue or an instability so the norm here will be a
norm uh and let's do a uh 4f or something like
that and I believe this is just a float and so we should be able to uh print
that uh so that's Global gradient clipping now they go into the details of
the learning rate uh scheduler so they don't just use a fixed learning rate like we do here for 3 E4 but there's
actually basically a cosine DK learning rate schedule um it's got a warm-up and
it's got a cosine DEC to 10% over some Horizon
um and so we're going to implement uh this in a second I just like to see Norm
printed here okay there we go so what happened here is the norm is actually really high in the beginning 30 or so
and you see that as we continue training it kind of like stabilizes um at values below one um and
this is not that crazy uncommon for the norm to be high in the very first few stages basically What's Happening Here
is the model is completely random and so there's a ton of learning happening very early in the network but that learning
is kind of like um you know it's mostly learning the biases of the output tokens
and so it's a bit of an unstable time uh but the network usually stabilizes in a very few iterations so this looks very
relatively reasonable to me except usually I would expect this looks a little bit funky that we go from 28 to 6
to 2 and then to 10 um it's not completely insane but it's just kind of
a little bit funky um okay so let's now get to the learning rate schuer so the learning
learning rate scheduler: warmup + cosine decay
rate schedule that's used here in gpt3 is what's called a cosine Decay learning
schedule with warmup and the way this looks is that the learning rate is basically starts right at around zero
linearly rank s up over some amount of time and then comes down with this cosine sort of form and comes down to
some kind of a minimum learning rate that's up to you so here the minimum learning rate is zero but uh here in the
paper they said that they use cosine Decay for learning rate down to 10% of its value over the first 260 billion
tokens and then training continues 10% after and there's a linear warmup over
the first 375 million tokens so that's about the learn R so let's now implement
this uh so I already implemented it here and the way this works is let me scroll
down first here I changed our training Loop a little bit so this was a 4i in Max steps I just change it to step now
so that we have the notion of a step is a single optimization step in the in the
for Loop and then here I get the LR for this step of the optimization using a
new function I call get LR and then in pytorch to set the learning rate I think this is is the way to set the learning
rate it's a little bit gnarly um because you have to basically there's a notion of different par parameter groups that
could exist in the optimizer and so you actually have to iterate over them even though we currently have a single param
group only um and you have to set the LR in this for Loop kind of style is is my
impression right now so we have this look of LR we set the learning rate and then on the bottom I'm also printing it
uh so that's all the changes I made to this Loop and then of course the get LR is my scheduler now it's worth pointing
out that pytorch actually has learning rate schedulers and you can use them and I believe there's a cosine learning rate
schedule in pytorch I just don't really love using that code because honestly
it's like five lines of code and I fully understand what's happening inside these lines so I don't love to use
abstractions where they're kind of in screwable and then I don't know what they're doing so personal style so the
max learning rate here is let's say 3 E4 but we're going to see that in gpt3
here they have a table of what the maximum learning rate is for every model
size so um for for this one basically 12
12 layer 768 gpt3 so the gpt3 small is roughly like a GPT
2124m we see that here they use a learning rate of 6 E4 so we could actually go higher um in fact we may
want to try to follow that and just set the max LR here at six uh then the that's the maximum learning
rate the minum learning rate is uh 10% of that per description in the paper
some number of steps that we're going to warm up over and then the maximum steps of the optimization which I now use also
in the for Loop down here and then you can go over this code if you like it's not U it's not terribly inside Flor
interesting I'm just uh modulating based on the iteration number which learning rate uh there should be so this is the
warm-up region um this is the region after the optimization and then this is the region
sort of in between and this is where I calculate the cosine learning rate schedule and you can step through this
in detail if you'd like uh but this is basically implementing this curve and I ran this already and this is
what that looks like um so when we now run we start at
um some very low number now note that we don't start exactly at zero because that would be not useful to update with a
learning rate of zero that's why there's an it+ one so that on the zeroth iteration we are not using exactly zero
we're using something very very low then we linearly warm up to maximum learning rate which in this case was 34 when I
ran it but now would be 6 E4 and then it starts to decay all the way down to um 3
E5 which was at the time 10% of the original learning rate now one thing we are not following exactly is that they
mentioned that um let me see if I can find it
again we're not exactly following what they did because uh they mentioned that their
training Horizon is 300 billion tokens and they come down to 10% of the initial learning rate of at 260 billion and then
they train after 260 with 10% so basically their Decay time is less than
the max steps time whereas for us they're exactly equal so it's not exactly faithful but it's um it's an
okay um this is okay for us and for our purposes right now and um we're just
going to use this ourselves I don't think it makes too too big of a difference honestly I should point out that what learning rate schedule you use
is totally up to you there's many different types um coign learning rate has been popularized a lot by gpt2 and
gpt3 but people have come up with all kinds of uh other learning rate schedules um and this is kind of like an
active area of uh research as to which one is the most effective at train these networks okay next up the paper talks
batch size schedule, weight decay, FusedAdamW, 90ms
about the gradual batch size increase so there's a ramp on the batch size that is
linear and you start with very small batch size and you ramp up to a big batch size over time uh we're going to
actually skip this and we're not going to work with it and the reason I don't love to use it is that it complicates a
lot of the arithmetic because you are changing the number of tokens that you're processing at every single step of the optimization and I like to keep
that math very very simple also my understanding is that that this is not like a major um Improvement and also my
understanding is that this is not like an algorithmic optimization Improvement it's more of a systems and speed
Improvement and roughly speaking this is because uh in the early stages of the
optimization uh again the model is in a very atypical setting and mostly what
you're learning is that um you're mostly learning to ignore the tokens uh that don't come up in your training set very
often you're learning very simple biases and and that kind of a thing and so
every single example that you put through your network is basically just telling you use these tokens and don't
use these tokens and so the gradients from every single example are actually extremely highly correlated they all
look roughly the same in the in the OR original parts of the optimization because they're all just telling you
that these tokens don't appear and these tokens do appear and so because the gradients are all very similar and
they're highly correlated then why are you doing batch sizes of like Millions when if you do a batch size of 32k
you're basically getting the exact same gradient early on in the training and then later in the optimization once
you've learned all the simple stuff that's where the actual work starts and that's where the gradients become more decorrelated per examples and that's
where they actually offer you sort of statistical power in some sense um so
we're going to skip this just because it kind of complicates things and we're going to go to uh data are sampled without
replacement during training um so until an Epoch boundary is reached so without
replacement means that they're not sampling from some fixed pool and then uh take a sequence train on it but then
also like return the sequence to the pool they are exhausting a pool so when they draw a sequence it's it's gone
until the next Epoch of training uh so we're already doing that because our data loader um iterates over chunks of
data so there's no replacement they don't become eligible to be drawn again until the next P so we're basically
already doing that um all models use a weight decay of
0.1 to provide a small amount of regularization so let's Implement a weight Decay and you see here that I've
already kind of made the changes and in particular instead of creating the optimizer right here um I I'm creating a
new configure optimizers function inside the model and I'm passing in some of the hyper parameters instead so let's look
at the configure optimizers which is supposed to return the optimizer
object okay so it looks complicated but it's actually really simple and it's just um we're just being very careful
and there's a few settings here to go through the most important thing with respect to this line is that you see
there's a weight Decay parameter here and I'm passing that into um well I'm passing that into
something called optim groups that eventually ends up going into the addom W Optimizer um and the weight Decay
that's by default used in Addam W here is 0.01 so it's it's u 10 times lower
than what's used in gpt3 paper here um so the weight dek basically ends up
making its way into the ADD and W through the optimizer groups now what else is going on here in this uh function so the two things that are
happening here that are important is that I'm splitting up the parameters into those that should be weight decayed
and those that should not be weight decayed so in particular it is common to not weight decay uh biases and any other
sort of one-dimensional tensors so the one-dimensional tensors are in the no Decay prams and these are also things
like uh layer Norm scales and biases it doesn't really make sense to weight Decay those you mostly want to weight
Decay uh the weights that participate in Matrix multiplications and you want to potentially weight Decay the
embeddings and uh We've covered in previous video why it makes sense to Decay the weights because you can sort
of the it as a regularization because when you're pulling down all the weights you're forcing the optimization to use
more of the weights um and you're not allowing any one of the weights individually to be way too large um
you're forcing you're forcing the network to kind of like distribute the work across more channels because there's sort of like a pull of gravity
on the weights themselves um so that's why we are separating it in those ways here we're
only decaying the embeddings and the mmal participating ways uh we're printing the number of uh
parameters that we decaying and not most of the parameters will be decayed and then one more thing that we're doing here is I'm doing another optimization
here and previous add and W did not have this option but later parts of pytorch
introduced it and that's why I'm guarding it with an inspect do signature which is basically checking if this
fused um quar is present inside atom W
and then if it is present I'm going to end up using it and passing it in here
because some earlier versions do not have fused equals so here's adamw fused
equals it did not used to exist and it was added later and there's some docks here for what's happening and basically
they say that by default they do not use fused because it is relatively new and we want to give it sufficient big time
so by default they don't use fused but fused is a lot faster when it is available and when you're running on
Cuda and what that does is in instead of iterating in a for Loop over all the
parameter tensors and updating them that would launch a lot of kernels right and so a fused just means that it's a um all
those kernels are fused into a single kernel you get rid of a lot of overhead and you a single time on all the
parameters call a uh kernel that updates them and so it's just basically a kernel
Fusion for the atom W update instead of iterating over all the tensors so that's the configure
optimizers function that I like to use and we can rerun and we're not going to see any major differences from what we
saw before but we are going to see some prints uh coming from here so let's just take a look at what they look
like so we see that number of Decay tensors is 50 and it's most of the parameters and number of non- deay
tensors is 98 and these are the biases and the layer Norm parameters mostly and that's there's only 100,000 of those so
most of it is decayed and then we are using the fused implementation of ATM W which will be a lot faster so if you
have it available I would advise you to use it I'm not actually 100% sure why they don't default to it it seems fairly
benign and harmless and also because we are using the fused implementation I think this is
why we have dropped um notice that the running time used to be 93 milliseconds
per step and we're now down to 90 milliseconds per step because of using the fused atom W Optimizer so in a
single commit here we are introducing fused atom getting improvements on the
time and we're adding or changing the weight Decay but we're only weight decaying the two dimensional parameters
the embeddings and the matrices that participate in linear so that is this
and we can take this out and uh yeah that is it for this line one more quick
gradient accumulation
note before we continue here I just want to point out that the relationship between weight Decay learning rate batch
size the atom parameters beta 1 beta 2 the Epsilon and so on these are very complicated uh mathematical
relationships in the optimization literature and um for the most part I'm
in this video I'm just trying to copy paste the settings that open AI used but this is a complicated topic uh quite
deep and um yeah in this video I just want to copy the parameters because it's a whole different video to really talk
about that in detail and give it a proper Justice instead of just high level intuitions uh now the next thing that I
want to move on to is that uh this paragraph here by the way we're going to turn back around to when we improve our
data loader for now I want to swing back around to this
table where you will notice that um for different models we of course have
different U hyper parameters for the Transformer that dictate the size of the Transformer Network we also have a
different learning rate so we're seeing the pattern that the bigger networks are trained with slightly lower learning rates and we also see this batch size
where in in the small networks they use a smaller batch size and in the bigger networks they use a bigger batch size
now the problem with for us is we can't just use 0.5 million batch size because
uh if I just try to come in here and I try to set uh this uh B where is my
b um b
equals where where do I call the DAT okay b equal 16 if I try to set um
well well we have to be careful it's not 0.5 million because this is the badge size in the number of tokens every
single one of our rows is24 tokens so 0.5 E6 1 million divide 1024 this would
need about a 488 match size so the problem is I can't come in here and set this to 488 uh
because my GPU would explode um this would not fit for sure and so but we
still want to use this batch size because again as I mentioned the batch size is correlated with all the other
optimization hyper parameters and the learning rates and so on so we want to have a faithful representation of all
the hyper parameters and therefore we need to uh use a bat size of .5 million
roughly but the question is how do we use .5 million if we only have a small GPU well for that we need to use what's
called gradient accumulation uh so we're going to turn to that next and it allows us to simulate in a Serial way any
arbitrary batch size that we set and so we can do a batch size of .5 million we
just have to run longer and we have to process multiple sequences and basically
add up all the gradients from them to simulate a batch size of .5 million so let's turn to that next okay so I
started the implementation right here just by adding these lines of code and basically what I did is first I set the
total batch size that we desire so this is exactly .5 million and I used a nice
number a power of two uh because 2 to the 19 is 524 288 so it's roughly .5
million it's a nice number now our micro batch size as we call it now is 16 so
this is going to be we still have B BYT in the SE that go into the Transformer
and do forward backward but we're not going to do an update right we're going to do many forward backwards we're going
to and those gradients are all going to plus equals on the parameter gradients they're all going to add up so we're
going to do forward backward grad akum steps number of times and then we're going to do a single update once all
that is accumulated so in particular our micro batch size is just now controlling how
many tokens how many rows we're processing in a single go over a forward backward so um here we are doing 16 *
124 we're doing 16 384 um tokens per forward backward and
we are supposed to be doing 2 to the 19 whoops what am I doing 2 to the
19 in total so the grat Aon will be
32 uh so therefore gr AUM here will work out to 32 and we have to do 32 forward
backward um and then a single update now we see that we have about 100 milliseconds for a singer forward
backward so doing 32 of them will be will make every step roughly 3 seconds
just napkin math so that's grum steps but now we actually have to Implement that so we're
going to swing over to our training Loop because now this part
here and this part here the forward and the backward we have to now repeat this 32 times before we do everything else
that follows so let's uh see how we can Implement that so let's come over here and actually we do have to load a new
batch every single time so let me move that over here and now this is where we have the inner loop so for micro step in
range graum steps we do this and remember that l.
backward always deposits gradients so we're doing inside losta backward there's always a plus equals on the
gradients so in every single L of backward gradients will add up on the gradient
tensors um so we lost that backward and then we get all the gradients over there
and then we normalize and everything else should just follow um so we're very
close but actually there's like subtle and deep issue here and this is actually
incorrect so invite I invite you to think about why this is not yet sufficient um and uh let me fix it then
okay so I brought back the jupyter notebook so we can think about this carefully in a simple toy setting and
see what's happening so let's create a very simple neural nut that takes a 16 Vector of 16 numbers and returns a
single number and then here I'm creating some random uh examples X and some targets uh
y Y and then we are using the mean squared loss uh here to calculate the
loss so basically what this is is four individual examples and we're just doing
Simple regression with the mean squared loss over those four examples now when we calculate the loss
and we lost that backward and look at the gradient this is the gradient that we achieve now the loss objective here
notice that in MSE loss the default for the loss function is reduction is mean
so we're we're calculating the average mean loss um the the mean loss here over
the four examples so this is the exact loss objective and this is the average
the one over four because there are four independent examples here and then we have the four examples and their mean
squared error the squared error and then this makes it the mean squared error so
therefore uh we are we calculate the squared error and then we normalize it to make it the mean over the examples
and there's four examples here so now when we come to the gradient accumulation version of it this uh this
here is the gradient accumulation version of it where we have grad acum steps of four and I reset the gradient
we've grum steps of four and now I'm evaluating all the examples individually instead and calling L that backward on
them many times and then we're looking at the gradient that we achieve from that so basically now we forward our
function calculate the exact same loss do a backward and we do that four times
and when we look at the gradient uh you'll notice that the gradients don't match so here we uh did a single batch
of four and here we did uh four gradient accumulation steps of batch size one and
the gradients are not the same and basically the the reason that they're not the same is exactly because this
mean squared error gets lost this one quarter in this loss gets lost because what happens here is the loss of
objective for every one of the loops is just a mean squ error um which in this
case because there's only a single example is just this term here so that was the loss in the zeroth eration same
in the first third and so on and then when you do the loss. backward we're accumulating gradients and what happens
is that accumulation in the gradient is basically equivalent to doing a sum in
the loss so our loss actually here is this
without the factor of one quarter outside of it so we're missing the normalizer and therefore our gradients
are off and so the way to fix this or one of them is basically we can actually come here and we can say loss equals
loss divide 4 and what happens now is that we're introducing we're we're scaling our loss
we're introducing a one quarter in front of all of these
places so all the individual losses are now scaled by one quarter and and then when we backward all of these accumulate
with a sum but now there's a one quarter inside every one of these components and now our losses will be
equivalent so when I run this you see that the U gradients are now identical
so long story short with this simple example uh when you step through it you can see that basically the reason that
this is not correct is because in the same way as here in the MSE loss the
loss that we're calculating here in the model is using a reduction of mean as
well uh so where's the loss after that cross entropy and by default the reduction uh
here in Cross entropy is also I don't know why they don't show it but it's the mean uh the mean uh loss at all the B
BYT elements right so there's a reduction by mean in
there and if we're just doing this gradient accumulation here we're missing that and so the way to fix this is to
simply compensate for the number of gradient accumulation steps and we can in the same way divide this loss so in
particular here the number of steps that we're doing is loss equals loss divide
gradient accumulation steps so even uh co-pilot s gets the modification but in
the same way exactly we are scaling down the loss so that when we do loss that backward which basically corresponds to
a sum in the objective we are summing up the already normalized um loss and and therefore
when we sum up the losses divided by grum steps we are recovering the additional normalizer uh and so now
these two will be now this will be equivalent to the original uh sort of optimization because the gradient will
come out the same okay so I had to do a few more touch-ups and I launched launched the optimization here so in
particular one thing we want to do because we want to print things nicely is well first of all we need to create
like an accumulator over the loss we can't just print the loss because we'd be printing only the final loss at the
final micro step so instead we have loss ofon which I initialize at zero and then I accumulate a uh the loss into it and
I'm using detach so that um uh I'm detaching the tensor uh from the graph
and I'm just trying to keep track of the values so I'm making these Leaf nodes when I add them so that's lakum and then
we're printing that here instead of loss and then in addition to that I had to account for the grum steps inside the
tokens processed because now the tokens processed per step is B * T * gradient
accumulation so long story short here we have the optimization it looks uh
reasonable right we're starting at a good spot we calculated the grum steps to be 32 and uh we're getting about 3 seconds
here right um and so this looks pretty good now if
you'd like to verify that uh your optimization and the implementation here is correct and your working on a side
well now because we have the total patch size and the gradient accumulation steps our setting of B is purely a performance
optimization kind of setting so if you have a big GPU you can actually increase this to 32 and you'll probably go a bit
faster if you have a very small GPU you can try eight or four but in any case you should be getting the exact same
optimization and the same answers up to like a floating Point error because the gradient accumulation kicks in and um
and can um handle everything serially as an Neary so uh that's it for gradient
accumulation I think okay so now is the time to bring out the heavy weapons uh you've noticed that so far we've only
distributed data parallel (DDP)
been using a single GPU for training but actually I am paying for eight gpus here
and so uh we should be putting all of them to work and in particular they are going to collaborate and uh you know
optimize over tokens at the same time and communicate so that um uh they're
all kind of collaborating on the optimization for this we are going to be using the distributed data parallel from
pytorch there's also a legacy data parallel which I recommend you not use and that's kind of like you know Legacy
distributed data parallel Works in a very simple way we have eight gpus so we're going to uh launch eight processes
and each process is going to be assigned to GPU and for each process the training
Loop and everything we've worked on so far is going to look pretty much the same H GPU as far as it's concerned is
just working on exactly what we've built so far but now Secret L there's eight of them and they're all going to be
processing slightly different parts of the data and we're going to add one more
part where once they all calculate their gradients there's one more part where we do a average of those
gradients and so that's how they're going to be collaborating on uh the computational workload here so to use
all eight of them we're not going to be launching our script anymore with just um pytorch train
gbt2 piy we're going to be running it with a special command called torrun in pytorch we'll see that in a bit and
torrun uh when it runs our python script we'll actually make sure to run eight eight of them in parallel and it creates
these environmental variables where each of these processes can look up which uh
basically which one of the processes it is so for example torron will set rank
local Rank and World size environmental variables and so this is a bad way to
detect whether uh DDP is running so if we're using torch run if DDP is
running then uh we have to make sure that K is available because I don't know that you can run this on CPU anymore or
that that makes sense to do um this is some um setup code here the important
part is that there's a world size which for us will be eight that's the total number of processes running there's a
rank which is um each process will basically run the ex exact same code at
the exact same time roughly but all the process the only difference between these processes is that they all have a
different dtp rank so the um gpu0 will have DDP rank of zero GPU 1 will have uh
rank of one Etc so otherwise they're all running the exact same script it's just
that DDP rank will be a slightly different integer and that is the way for us to coordinate that they don't for
example run on the same data we want to we want them to run on different parts of the data and so on
now local rank is something that is only used in a multi- node setting we only have a single node with ag gpus and so
local rank is the rank of the GPU on a single node so from 0 to seven as an
example but for us we're mostly going to be running on a single box so the things we care about are Rank and World size
this is eight and this will be whatever it is depending on the GPU uh that uh that this particular instantiation of
the script runs on now here we make sure that according to
the local rank we are setting the device to be Cuda colon and colon indicates
which GPU to use if there are more than one gpus so depending on the local rank
of this process it's going to use just the appropriate GPU so there's no collisions on which GPU is being used by
which process and finally there's a Boolean variable that I like to create which is the DDP rank equ equal Z so the master
process is arbitrarily process number zero and it does a lot of the printing logging checkpointing Etc and the other
processes are thought of mostly as a compute processes that are assisting and so Master process zero will have some
additional work to do all the other processes will uh will mostly just be doing forward backwards and if we're not using DDP and
none of these variables are set we revert back to single GPU training so that means that we only have rank zero
the world size is just one uh and and we are the master process and we try to autodetect the device and this is world
as normal so so far all we've done is we've initialized DDP and uh in the case where we're
running with torrun which we'll see in a bit there's going to be eight copies running in parallel each one of them
will have a different Rank and now we have to make sure that everything happens uh correctly afterwards so the
tricky thing with running multiple processes is you always have to imagine that there's going to be eight processes
running in parallel so as you read the code now you have to imagine there's eight you know eight python interpreters
running down these lines of code and the only difference between them is that they have a different DDP rank so they
all come here they all pick the exact same seed they all make all of these calculations completely unaware of the
other copies running roughly speaking right so they all make the exact same calculations and now we have to adjust
these calculations to take into account that there's actually like a certain world size and certain ranks so in
particular these micro batches and sequence lengths these are all just per GPU right so now there's going to be num
processes of them running in parallel so we have to adjust this right because the
grum steps now is going to be total B size divide B * T time U DDP R
size because each um process will will
do B * T and there's this many of them and so in addition to that we we
want to make sure that this fits nicely into total batch size which for us it will because 16 * 124 * 8 8 gpus is
131 uh K and so 524288 this means that our gratum will
be four with the current settings right so there's going to be 16 * 124 process
on each GPU and then there's a GP pus so we're going to be doing 131,000 tokens in a single forward
backward on the 8 gpus so we want to make sure that this
fits nicely so that we can derive a nice gradient accumulation steps and uh yeah let's just adjust the
comments here times uh DDP World size okay so each GPU calculates this now
this is where we start to get run into issues right so we are each process is going to come by a print and they're all
going to print so we're going to have eight copies of these prints so one way to deal with this is exactly this master
process variable that we have so if Master process then guard this and
that's just so that we just print this a single time because otherwise all the processes would have computed the exact same variables and there's no need to
print this eight times um before getting into the data loader and we're going to have to
refactor it obviously maybe at this point is uh we should do some prints and
uh just take it out for a spin and exit at this point so import
sis and S start exit and print IM
GPU um DDP
rank IM GPU DDP Rank and that um print
by so uh so now let's try to run this and just see how this works so let's
take it for a spin just so we see what it looks like so normally we use to launch python train gpd2 P like this now
we're going to run with torch run and this is what it looks like so torch run Standalone number of processes for
example is eight for us because we have eight gpus uh and then change of2 Pi so
this is what the command would look like and torch run again we'll run eight of these so let's just see what happens so
first it gets a little busy so there's a lot going on here so first of all there's
some warnings from distributed and I don't actually know that these mean anything I think this is just like the
code is setting up and the processes are coming online and we're seeing some preliminary failure to collect while the
processes come up I'm not 100% sure about that but we start to then get into
actual prints so all the processes went down and then the first print actually comes from
process 5 uh just by chance and then it printed so process 5 basically got here
first it said I'm process on GPU 5 buy and then this these prints come from the
master process so process 5 just finished first for whatever reason it just depends on
how the operating system scheduled the processes to run uh then gpu0 ended then GPU 3 and two and then uh probably
process 5 or something like that has uh exited and and DDP really doesn't like
that because we didn't properly dispose of uh the multi-gpus um setting and so
process group has not been destroyed before we destruct uh so it really doesn't like that and in an actual
application we would want to call destroy process group uh so that we clean up DDP properly and so it doesn't
like that too much and then the rest of the gpus finish and that's it so
basically we can't guarantee when these processes are running it's totally but they are running in parallel we
don't want them to be printing um and next up let's erase
this next up we want to make sure that when we create data loader light we need to now make it aware of this
multi-process um setting because we don't want all the processes to be loading the exact same data we want
every process to get its own chunk of data so that they're all working on different parts of the data set of course so let's adjust that so one
particular particularly simple and a naive way to do this is we have to make sure that we pass in the rank and the
size to the data loader and then when we come up here we see that we now take Rank and processes
and we save them now the current position will not be zero uh because
what we want is we want to stride out all the processes so one way to do this
is we basically take S.B times salt. T and then multiply it by the process
rank so proc process rank 0 will start at zero but process rank one now starts
at B * T process rank two is starts at 2 * B * D Etc so that is the
initialization now we still they still do this identically but now when we
advance we don't Advance by B * T we advance by B * T times number of
processes right so basically um the total number of tokens that we're um
consuming is B * T * number processes and they all go off to a different Rank
and the position has to advance by the entire chunk and then here B * T time uh s. num
processes + one would be to exceed number of tokens then we're going to Loop and when we Loop we want to of
course Loop in the exact same way so we sort of like reset back uh so this is
the simplest change that I can uh find for kind of a very simple distributed data Lo light and um you can notice that
if process rank is zero and non processes is one then uh the whole thing will be identical to what we had before
but now we can have actually multiple processes uh running and this should work fine um so that's the data loader okay
so next up once they've all initialized the data loader they come here and they all create a GPT model uh so we create
eight GPT models on eight processes but because the seeds are fixed here they all create the same identical model they
all move it to the device of their Rank and they all compile the model and because the models are identical there
are eight identical compilations happening in parallel but that's okay now none of this uh changes because that
is on a per step basis and we're currently working kind of within step because we need to um just uh all the
all the changes we're making are kind of like a within step changes now the important thing here is
when we construct the M model we actually have a bit of work to to do here get loits is deprecated so uh
create model we need to actually wrap the model into the distributed data parallel
container so um this is how we wrap the model into the DDP container and these
are the docs for DDP and they're quite extensive and there's a lot of caveats and a lot of things to be careful with
because everything complexifies times 10 when multiple processes are involved but
roughly speaking this device IDs I believe has to be passed in now unfortunately the docs for what device
IDs is is is extremely unclear uh so when you actually like come here this
comment for what device IDs is is roughly nonsensical um but I'm pretty sure it's
supposed to be the DDP local rank so not the DDP rank the local rank uh so this
is what you pass in here this wraps the model and in particular what DDP does for you is in a forward pass it actually
behaves identically so um my understanding of it is nothing should be changed in the forward pass but in the
backward pass as you are doing the backward pass um in the simpl setting
once the backp passes over on each independent GPU each independent GPU has
the gradient for all the parameters and what DDP does for you is once the backward pass is over it will call
what's called all reduce and it basically does an average across all the
uh ranks of their gradients and and then it will deposit that average on every
single rank so every sing Single rank will end up with the average on it and so basically that's the communication it
just synchronizes and averages the gradients and that's what DDP offers you now DDP actually is a little bit more um
it is a little bit more involved than that because as you are doing the backward pass through the layers of the Transformer it actually can dispatch
Communications for the gradient while the backward pass is still happening so there's overlap of the uh communication
of the gradient and the synchronization of them and uh the backward pass and uh this is just more efficient and um uh to
do it that way so that's what DDP does for you um forward is unchanged and
backward is mostly unchanged and we're tacking on this average as we'll see in a bit okay so now let's go to the uh
optimization nothing here changes let's go to the optimization here the inner loop and think through the
synchronization of uh these gradients in the DP so basically by default what happens as I mentioned is when you do l.
backward here it will do the backward pass and then it will synchronize the gradients um the problem here is because
of the gradient accumulation steps Loop here we don't actually want to do the
synchronization after every single La step backward because we are just depositing gradients and we're doing
that serially and we just want them adding up and we don't want to synchronize every single time that would be extremely wasteful so basically we
want to add them up and then on the the very last uh it's only on the very last step when micro when micro step becomes
gratak steps minus one only at that last step do we want to actually do the
alberu uh to average up the gradients so to do that we come here and um the
official sanctioned way by the way is to do this no sync context manager so
pytorch says this is a context manager to disable gradient synchronization across DDP processes So within this
context gradient will be accumulated and basically when you do no sync there will be no communication so
they are telling us to do with DDP no sync uh do the gradient accumulation accumulate grats and then they are
asking us to do DDP again with another input and that backward and I just really don't love this I I just really
don't like it uh the fact that you have to copy paste your code here and use a context manager and this is just super
ugly so when I went to this source code here you can see that when you enter
you simply toggle this variable this require backward grat sync and this is
uh being toggled around and changed and this is the variable that basically uh
if you step through it is being toggled to determine if the gradient is going to be synchronized so I actually just kind
of like to use that directly uh so instead what I like to do is the
following right here before the L back backward if we are using the DDP then um
then basically we only want to synchronize we only want this variable to be true when it is the final
iteration in all the other iterations inside the micr steps we want to be false so I just toggle it like this so
required backward graph sync should only turn on when the micro step is the last step and so I'm toggling this variable
directly and I hope that that impacts last St backwards and this is a naughty thing to do
because you know they could probably change the DDP and this variable will go away but for now I believe this this
works and it allows me to avoid the use of context managers and code duplication I'm just toggling the variable and then
Lop backward will not synchronize most of the steps and it will synchronize the very last step and so once this is over
uh and we come out every single um rank will suddenly magically have the average
of all the gradients that were stored on all the ranks so now we have to think
through whether that is what we want and also um if this suffices and whether how
it works with the loss and what is loss AUM so let's think through through that now and the problem I'm getting at is
that we've averaged the gradients which is great but the loss AUM has not been impacted yet and the and this is outside
of the DDP container so that is not being averaged um and so here when when we are printing Los AUM well presumably
we're only going to be printing on the master process uh rank zero and it's just going to be printing the losses
that it saw on its process but instead we want it to print the loss over all
the processes and the average of that loss because we did average of gradients so we want the average of loss as well
so simply here after this uh this is the code that I've used in the past um and
instead of LF we want Lum so if
DDP again then this is a p torch distributed I import it where do I
import it uh oh gosh so this file is starting
to get out of control huh so if uh so import torch. distributed as dist
so dist. ALU and we're doing the average on Lum
and so this lakum tensor exists on all the ranks when we call all use of average it creates the average of those
numbers and it deposits that average on all the ranks so all the ranks after this um call will now contain L AUM uh
averaged up and so when we print here on the master process the L AUM is identical in all the other ranks as well
so here if Master process oops we want to print like this okay and
finally we have to be careful because we're not processing even more tokens so times DDP World size
that's number of tokens that we've processed up above
and everything else should be fine uh the only other thing to be careful with is as I mentioned you want to destroy
the process group so that we are nice to nickel and it's not going to uh to uh to DDP and it's not going to complain to us
uh when we exit here so that should be it let's try to take it for a spin okay so I launched
the script and it should be uh printing here imminently we're now training with 8 gpus at the same time so the gradient
accumulation steps is not 32 it is now divide 8 and it's just four uh so um
otherwise this is what the optimization now looks like and wow we're going really fast so we're processing 1.5
million tokens uh per second now so these are some serious numbers and the
tiny shakespare data set is so tiny that we're just doing like so many Epoch over it most likely but this is roughly what
looks like um one thing that I had to fix by the way is that this was model.
configure optimizers which Now doesn't work because model now is a DDP model so instead this has to become raw
model. configure optimizers where raw model is something I create here so
right after I wrap the model into DDP uh I have to create the raw model which in
the case of DDP is a model. module is where it stores the raw and then module
of gpt2 as we have it which contains the uh configure optimizers function that we want to call so that's one thing that I
have to fix otherwise this seems to run now one thing you'll notice is that when you actually compare this run and the
numbers in it to the just running a single GPU you'll notice that this is single GPU run with 32 gratum the
numbers won't exactly match up and uh that's kind of a boring reason for why that happens uh the reason for
that is that in the data loader we're basically just iterating through batches and slightly different way because now
we're looking for an entire page of data and if that page uh for all the gpus if
that chunk exceeds the number of tokens we just Loop and so actually the single GPU and the H GPU process will end up um
resetting in a slightly different Manner and so our batches are slightly different and so we get slightly
different numbers but one way to convince yourself that this is okay it just make the total batch size much
smaller and the b and a t and then um so I think I used uh 4 * 124 * 8 so I
used 32768 as a total patch size and then um so I made sure that the single
GPU will do eight creting accumulation steps and then the multi-gpu and then you're reducing the boundary effects of
the data loader and you'll see that the numbers match up so long story short we're now going really really fast the
optimization is mostly consistent with gpt2 and three hyper parameters and uh
we have outgrown our tiny Shakespeare file and we want to upgrade it so let's move to next to that next so let's now
datasets used in GPT-2, GPT-3, FineWeb (EDU)
take a look at what data sets were used by gpt2 and gpt3 so gbt2 used this web
Text data set that was never released um there's an attempt at reproducing it called open web text uh so basically
roughly speaking what they say here in the paper is that they scraped all outbound links from Reddit and then uh
with at least three Karma and that was kind of like their starting point and they collected all the web P all the web pages and all the text in them and so
this was 45 million links and this ended up being 40 GB of text so uh so that's
roughly what gpt2 says about its data set so it's basically outbound links from Reddit now when we go over to gpt3
there's a training data set section and that's where they start to talk about um common coll which is a lot more uh used
actually I think even gpt2 talked about common coll um but basically it's not a
very high quality data set all by itself because it is extremely noisy this is a completely random subset of the internet
and it's much worse than you think so people go into Great Lengths to filter common craw because there's good stuff
in it but most of it is just like ad spam random tables and numbers and stock tickers and uh it's just total mess
so that's why people like to train on these data mixtures that they curate and
uh are careful with so a large chunk of these data mixtures typically will be common C like for example 50% of the
tokens will be comic but then here in gpt3 they're also using web text to from before so that's Reddit outbound but
they're also adding for example books and they're adding Wikipedia there's many other things you can decide to add
now this data set for gpt3 was also never released so today some of the data sets that I'm familiar with that are
quite good and would be representative of something along these lines are number one the red pajama data set or
more specifically for example the slim pajama subset of the red pajama data set which is a cleaned and D duplicated
version of it and just to give you a sense again it's a bunch of common crawl um C4 which is also as far as I know
more common craw but processed differently and then we have GitHub books archive Wikipedia stack exchange
these are the kinds of data sets that would go into these data mixtures now specifically the one that I like that
came out recently is called Fine web data set uh so this is an attempt to basically collect really high quality
common coll data and filter it in this case to 15 trillion tokens and then in
addition to that more recently huggingface released this fine web edu subset which is 1.3 trillion of
educational and 5.4 trillion of high educational content so basically they're
trying to filter common C to very high quality educational subsets and uh this
is the one that we will use there's a long uh web page here on fine web and
they go into a ton of detail about how they process the data which is really fascinating reading by the way and I would definitely recommend if you're
interested into Data mixtures and so on and how data gets processed at these scales a look at this uh page and more
specifically we'll be working with the fine web edu I think and it's basically educational content from the
internet uh they show that training on educational content in in their metrics
um uh works really really well and we're going to use this sample 10 billion
tokens subsample of it because we're not going to be training on trillions of tokens uh we're just going to train on
uh 10 billion sample of the fine web edu because empirically in my previous few
experiments this actually suffices to really get close to gpt2 Performance and it's um simple enough to work with and
so let's work with the sample 10 uh BT so our goal will be to download it
process it and make sure that our data loader can work with it so let's get to that okay so I introduced another um
file here that will basically download Fine web edu from huging face data sets
it will pre-process and pre- tokenize all of the data and it will save data shards to a uh folder on um local disk
and so while this is running uh just wanted to briefly mention that you can
kind of look through the data set viewer here just to get a sense of what's in here and it's kind of interesting I mean it's a it basically looks like it's
working fairly well like it's talking about nuclear energy in France it's talking
about Mexican America some mac PJs Etc so actually it
seems like their filters are working pretty well uh the filters here by the way were applied automatically using um
llama 370b I believe and so uh basically llms are judging which content is
educational and that ends up making it through the filter uh so that's pretty cool now in terms of the script itself
I'm not going to go through the full script because it's not as interesting and not as llm Centric but when you run
this basically number one we're going to load the data set uh which this is all huging face code running this you're
going to need to uh pip install data sets um so it's downloading the data set
then it is tokenizing all of the documents inside this data set now when we tokenize the documents you'll notice
that um to tokenize a single document uh we first
start the tokens with the end of text token and this is a special token in the gpt2 tokenizer as you know so
50256 is the ID of the end of text and this is what begins a document even
though it's called end of text but this is uh the first token that begins a document then we extend with all of the
tokens of that document then we create a numpy array out of that we make sure
that all the tokens are between oh okay let me debug this
okay so apologies for that uh it just had to do with me using a float division in Python it must be integer division so
that this is an INT and everything is nice um okay but basically the
tokenization here is relatively straightforward returns tokens in mp. un6 uh we're using .16 to save a little
bit of space because 2 to the 16us 1 is 65,000 so the gpt2 max token ID is well
below that and then here there's a bunch of multiprocessing code and it's honestly not that exciting so I'm not
going to step through it but we're loading the data set we're tokenizing it and we're saving everything to shards
and the shards are numpy files uh so just storing a numpy array and uh which
is very very similar to torch tensors and the first Shard 0000 is a
Val a validation Shard and all the other shards are uh training shards and as I
mentioned they all have 100 million tokens in them exactly um and and that
just makes it easier to work with as to Shard the files because if we just have a single massive file sometimes they can
be hard to work with on the disk and so sharting it is just kind of um nicer from that
perspective and uh yeah so we'll just let this run this will be probably um
30ish minutes or so and then we're going to come back to actually train on this data and we're going to be actually doing some legit pre-training in this
case this is a good data set we're doing lots of tokens per second we have 8 gpus
the code is ready and so we're actually going to be doing a serious training run so let's get P it back in a bit okay so
we're back so uh if we LS edu fine web we see that there's now 100 charts in it
um and that makes sense because each chart is 100 million tokens so 100 charts of that is 10 billion tokens in
total now swinging over to the main file I made some adjustments to our data loader again and that's because we're
not running with uh Shakespeare anymore we want to use the fine web shards and
so you'll see some code here that additionally basically can load these shards uh we load the um un6 numpy file
we convert it to a torch. long tensor which is what a lot of the layers up top expect by default and then here we're
just enumerating all the shards I also added a split to data load of light so
we can uh load the split train but also the split Val uh the zero split and then we can load the shards
and then here we also have not just the current position now but also the current Shard so we have a position
inside A Shard and then when we uh run out of tokens in A Single Shard we first
Advance The Shard and loop if we need to and then we get the tokens and readjust the position so this data loader will
now iterate all the shards as well so I Chang that and then the other thing that
I did while uh the data was processing is our train loader now has split train
of course and down here I set up some I set up some numbers so we are doing 2 to the
9 uh tokens per uh per um per step and
we want to do roughly 10 billion tokens um because that's how many unique tokens
we have so if we did 10 billion tokens then divide that by 29 we see that this
is 1973 steps so that's where that's from and then the GPT three paper says
that they warm up the learning rate over 375 million tokens so I came here and
375 E6 tokens divide uh 2 to the 19 is 715 steps so that's why warm-up
steps is set to 715 so this will exactly match um the warm-up schedule that gpt3
used and I think 715 by the way is very uh mild and this could be made significantly more aggressive probably
even like 100 is good enough um but it's okay let's leave it for now so that we have the exact hyper parameters
of gpt3 so I fix that and then um that's
pretty much it we can we can run so we have our script here and we can
launch and actually sorry let me do one more
thing excuse me for my GPU I can actually fit more
batch size and I believe I can fat I can fit 60 4 on my GPU as a micro bash size
so let me try that I could be misremembering but that
means 64 * 124 per GPU and then we have a gpus so that means we would not even
be doing gradient accumulation if this fits because uh this just multi multiplies out to uh the full total bat
size so no gradient accumulation and that would run pretty quickly if that fits
let's go let's go I mean if this works then this is basically a serious pre-training run um we're not logging
we're not evaluating the validation split we're not running any evaluations yet so it's not we haven't crossed our
te's and dotted our eyes but uh if we let this run for a while we're going to actually get a pretty good model and the
model that might even be on par with or better than gpt2 124 M okay so it looks
like everything is going great we're processing 1.5 million tokens per second uh everything here looks good
we're doing 330 milliseconds per iteration and we have to do a total
of uh where are we printing that 1973 so 19073 times 0.33
is this many seconds this many minutes so this will run for 1.7
hours uh so one and a half hour run uh like this and uh we don't even have to
use gradient accumulation which is nice and you might not have that luxury in your GPU in that case just start decreasing the batch size until things
fit but keep it to nice numbers um so that's pretty exciting
we're currently warming up the learning rate so you see that it's still very low one4 so this will ramp up over the next
few steps all the way to 6 e Nega uh 4
here very cool so now what I'd like to do is uh let's cross the T and do our eyes let's evaluate on the validation
split and let's try to figure out how we can run evals how we can do logging how we can visualize our losses and all the
good stuff so let's get to that before we actually do the run okay so I've adjusted the code so that we're
validation data split, validation loss, sampling revive
evaluating on the validation split so creating the Val loader just by passing in Split equals Val that will basically
create a data loader just for the uh validation Shard um the other thing I did is in the
data loader I introduced a new function reset which is called at init and it basically resets the data loader and
that is very useful because when we come to the main training Loop now so this is
the code that I've added and basically every 100th iteration including the zeroth iteration we put the model into
evaluation mode we reset the Val loader and then um no gradients involved we're
going to basically accumulate the gradients over say 20 steps and then average it all up
and print out the validation loss and so that basically is the exact same logic
as the training Loop roughly but there's no loss that backward it's only inference we're just measuring the loss
we're adding it up everything else otherwise applies and is exactly as we've seen it before and so this will
print the validation laws um every 100th iteration including on the very first
iteration uh so that's nice that will tell us some amount some a little bit about how much we're overfitting that
said like uh we have roughly Infinity data so we're mostly expecting our train and Val loss to be about the same but
the other reason I'm kind of interested in this is because we can take the GPT 2124m as openi released it we can
initialize from it and we can basically see what kind of loss it achieves on the validation loss as well and that gives
us kind of an indication as to uh how much that model would generalize to 124 M but it's not an sorry to fine web edu
validation split that said it's not a super fair comparison to gpt2 because it was trained on a very different data
distribution but it's still kind of like an interesting data point and in any case you would always want to have a
validation split in a training run like this so that you can make sure that you are not um overfitting and this is
especially a concern if we were to make more Epoch in our training data um so
for example right now we're just doing a single Epoch but if we get to a point where we want to train on 10 epochs or something like that we would be really
careful with maybe we are memorizing that data too much if we have a big enough model and our validation split
would be one way to tell whether that is happening okay and in addition to that if you remember at bottom of our script
we had all of this orphaned code for sampling from way back when so I deleted that code and I moved it up um to here
so once in a while we simply value validation once in a while we sample we generate
samples and then uh we do that only every 100 steps and we train on every
single step so that's how I have a structure right now and I've been running this for 10,000 iterations so
here are some samples on neration 1,000 um hello I'm a language model and I'm
not able to get more creative I'm a language model and languages file you're learning about
here is or is the beginning of a computer okay so this is all like pretty uh this
is still a garble uh but we're only at ration 1,000 and we've only just barely reached maximum learning rate uh so this
is still learning uh we're about to get some more samples coming up in
1,00 okay um okay this is you know the model is
still is still a young baby okay so uh basically all of this sampling code that
I've put here everything should be familiar with to you and came from before the only thing that I did is I created a generator object in pytorch so
that I have a direct control over the sampling of the random numbers don't because I don't want to impact the RNG
state of the random number generator that is the global one used for training I want this to be completely outside of
the training Loop and so I'm using a special sampling RNG and then I make
sure to seed it that every single rank has a different seed and then I pass in
here where we sort of consumer in the numbers in multinomial where the sampling happens I make sure to pass in
the generator object there otherwise this is identical uh now the other thing is um you'll notice that we're running a
bit slower that's because I actually had to disable torch. compile to get this to sample and um so we're running a bit
slower so for some reason it works with no torch compile but when I torch compile my model I get a really scary
error from pytorch and I have no idea how to resolve it right now so probably by the time you see this code released
or something like that maybe it's fixed but for now I'm just going to do end false um and I'm going to bring back
toor compile and you're not going to get samples and I I think I'll fix this
later uh by the way um I will be releasing all this code and actually I've been very careful about making get
commits every time we add something and so I'm going to release the entire repo that starts completely from scratch all
the way to uh now and after this as well and so everything should be exactly documented in the git commit history um
um and so I think that will be nice so hopefully by the time you go to GitHub uh this is removed and it's working and
I will have fixed the bug okay so I have the optimization running here and it's stepping and we're on step 6,000 or so
evaluation: HellaSwag, starting the run
so we're about 30% through training now while this is training I would like to introduce one evaluation that we're
going to use to supplement the validation set and that is the H swag eval so hos swag comes from this paper
back in 2019 so it's a 5-year-old eval now and the way H swag works is there is basically a sentence completion data set
so it's a multiple choice for every one of these questions we have uh basically a shared context like a woman is outside
with a bucket and a dog the dog is running around trying to avoid bath she
a Rises the bucket off with soap and blow dry the dog's head B uses a hose to
keep it from getting soapy C gets the dog wet and it runs away again or D gets
into a bathtub with the dog and so basically the idea is that these
multiple choice are constructed so that one of them is a natural continuation of
the um sentence and the others are not and uh the others might not make
sense like uses the host to keep it from getting soaped that makes no sense and so what happens is that models that are
not trained very well are not able to tell these apart but models that have a lot of World Knowledge and can tell uh
which um and can tell a lot about the world will be able to create these completions and these sentences are
sourced from activity net and from Wiki how and at the bottom of the uh
paper there's kind of like a cool chart of the kinds of domains in Wiki house so
there's a lot of sentences from computers and electronics and Homes and Garden and it has kind of a broad
coverage of the kinds of things you need to know about the world in order to find the most likely completion and um the
identity of that of that completion one more thing that's kind of interesting about H swag is the way it was
constructed is that the incorrect um options are deliberately um
adversarially sourced so they're not just random sentences they're actually sentences generated by language models
and they're generated in such a way that language models basically find them difficult but humans find them easy and
so they mentioned that humans have a 95% accuracy on this set but at the time the state-of-the-art language models had
only 48% and so at the time this was a good Benchmark now you can read the
details of this paper to to learn more um the thing to point out though is that this is 5 years ago and since then what
happened to H swag is that it's been totally just uh um solved and so now the language models
here are 96% so basically the 4% the last 4% is probably errors in the data
set or the questions are really really hard and so basically this data set is kind of crushed with respect to language
models but back then the best language model was only at about 50% uh but this
is how far things got but still the the reason people like H swag and it's not
used by the way in gpt2 but in gpt3 there is H swag eval and lots of people
use H swag and so for gpt3 we have results
here that are cited so we know what percent accuracies gpt3 um attains at all these
different model checkpoints for H swag eval and the reason people like it is because H swag is a smooth eval and it
is an eval that offers quote unquote early signal uh so early signal means that even small language models are
going to start at the random chance of 25% but they're going to slowly improve and you're going to see 25 26 27 Etc and
uh you can see slow Improvement even when the models are very small and it's very early so it's smooth it has early
signal and um it's been around for a long time so that's why people kind of
like this eval uh now the way that we're going to evaluate this is as
follows as I mentioned we have a shared context and this is kind of like a multiple choice task but instead of
giving the model a multiple choice question and asking it for A B C or D uh we can't do that because these models
when they are so small as we are seeing here the models can't actually do multiple choice they don't understand
the concept of associating a label to one of the options of multiple choice uh they don't understand that so we have to
give it to them in a native form and the native form is a token completion so
here's what we do we construct a batch of four rows and uh T tokens whatever
that t happens to be then the shared context that is basically the context for the for choices the tokens of that
are shared across all of the rows and then we have the four options so we kind of like lay them out and then only one
of the options is correct in this case label three option three and so um this
is the correct option and option one two and for are incorrect now these options might be of
different lengths so what we do is we sort of like take the longest length and that's the size of the batch B BYT and
then some of these uh here are going to be pded Dimensions so they're going to be unused and so we need the tokens we
need the correct label and we need a mask that tells us which tokens are active and the mask is then zero for
these uh padded areas so that's how we construct these batches and then in
order to get the language model to predict A B C or D the way this works is basically we're just going to look at
the tokens their probabilities and we're going to pick the option that gets the
lowest or the highest average probability for the token so for the
tokens because that is the most likely completion according to the language model so we're just going to look at the
um probabilities here and average them up across the options and pick the one
with the highest probability roughly speaking so this is how we're going to do H swag
um and this is I believe also how uh gpt3 did it um this is how gpt3 did it
as far as I know but you should note that some of the other evals where you might see H swag may not do it this way
they may do it in a multiple choice format where you sort of uh give the the context a single time and then the four
completions and so the model is able to see all the four options before it picks the best possible option and that's
actually an easier task for a model because you get to see the other options when you're picking your choice um but
unfortunately models at our size can't do that only models at a bigger size are able to do that and so our models are
actually slightly handicapped in this way that they are not going to see the other options they're only going to see
one option at a time and they just have to assign probabilities and the correct option has to win out in this metric all
right so let's now implement this very briefly and incorporate it into our script okay so what I've done here is
I've introduced a new file called hell swag. py that you can take a look into and I'm not going to to step through all
of it because uh this is not exactly like deep code deep code it's kind of
like a little bit tedious honestly because what's happening is I'm downloading hsac from GitHub and I'm
rendering all of its examples and there are a total of 10,000 examples I am rendering them into this format um and
so here at the end of this render example function you can see that I'm
returning the tokens uh the tokens of this um 4xt
uh array of Tokens The Mask which tells us which parts are the options and
everything else is zero and the label that is the correct label and so that
allows us to then iterate the examples and render them and I have an evaluate function here which can load a um gpt2
from huging face and it runs the eval here um and it basically just calculates
uh just as I described it predicts the option that has the lowest or the highest prob ility and the way to do
that actually is we can basically evaluate the cross entropy loss so we're basically evaluating the loss of
predicting the next token in a sequence and then we're looking at the row that has the lowest average loss and that's
the uh option that we pick as the prediction and then we do some stats and
prints and stuff like that so that is a way to evaluate L swag now if you go up here I'm showing that for GPT 2124m if
you run this script you're going to see that H swag gets
29.5% um so that's the performance we get here now remember that random Chan is 25% so we haven't gone too far and
gpt2 XL which is the biggest the gpt2 gets all the way up to 49% roughly so uh
these are pretty low values considering that today's state-ofthe-art is more like 95% uh so these are definitely
older models by now and then there's one more thing called Uther harness which is a very piece of infrastructure for
running evals for language models and they get slightly different numbers and I'm not 100% sure what the discrepancy
is for these um it could be that they actually do the multiple choice uh instead of just the completions and that
could be the um uh the discrepancy but I'm not 100% sure about that i' have to take a look but for now our script
reports 2955 and so that is the number that we'd like to beat if we are training a GPD 2124m from scratch and
ourselves um so now I'm going to go into actually
incorporating this eval into our main training script and um and basically
because we want to evaluate it in a periodic manner so that we can track H swag and how it evolves over time and
see when when and if we cross uh this 2955 um sort of region so let's now walk
through some of the changes to train gpt2 thatp the first thing I did here is I actually made use compile optional
kind of and I disabled it by default and the problem with that is the problem
with compile is that unfortunately it does make our code faster but it actually breaks the evaluation code and
the sampling code it gives me a very gnarly message and I don't know why so hopefully by the time you get to the
codebase when I put it up on GitHub uh we're going to fix that by then but for now I'm running without torch compile
which is why you see this be a bit slower so we're running without torch compile I also create cre a log
directory log where we can place our log.txt which will record the train loss
validation loss and the H swag accuracies so a very simple text file and we're going to uh open for writing
so that it sort of starts empty and then we're going to append to it I created a simple variable that um
helps tell us when we have a last step and then basically periodically inside this Loop every 250th iteration or at
the last step we're going to evaluate the validation loss and then every 250th
iteration um we are going to evaluate H swag but only if we are not using
compile because compile breaks it so I'm going to come back to this code for evaluating H swag in a second and then
every 250th iteration as well we're also going to sample from the model and so you should recognize this as our ancient
code from way back when we started the video and we're just sampling from the model
and then finally here um these are if we're not after we validate sample and
evaluate hell swag we actually do a training step here and so this is one step of uh training and you should be
pretty familiar with all of what this does and at the end here once we get our training laws we write it to the file so
the only thing that changed that I really added is this entire section for H swag eval and the way this works is
I'm trying to get all the gpus to collaborate on the H swag and so we're iterating all the examples and then each
process only picks the examples that assigned to it so we sort of take I and
moded by the world size and we have to make it equal to rank otherwise we continue and then we render an example
put it on the GPU we get the low jits then I create a helper function that helps us basically predict the option
with the lowest loss so this comes here the prediction and then if it's correct we sort of keep count and then if
multiple processes were collaborating on all this then we need to synchronize their stats and so the way one way to do
that is to package up our statistics here into tensors which we can then call
this. alberon and sum and then here we sort of um unwrap
them from tensors so that we just have ins and then here the master process will print and log the hellis swag
accuracy so that's kind of the that's kind of it
and that's what I'm running right here so you see this optimization here and uh we just had a generation and this is
Step 10,000 out of about 20,000 right so we are halfway done and these are the
kinds of samples that uh we are getting at this stage so let's take a look hello I'm a language model so I'd like to use
it to generate some kinds of output hello I'm a language model and I'm a developer for a lot of
companies Al language model uh let's see if I can find fun
one
um I don't know you can go through this yourself but certainly the predictions are getting less and less random uh it
seems like the model is a little bit more self-aware and using language uh that is a bit
more uh specific to it being language model hello I'm a language model and
like how the language is used to communicate I'm a language model and I'm going to be speaking English and German
okay I don't know so let's just wait until this optimization finishes and uh we'll see what kind of samples we get
and we're also going to look at the train Val and the hway accuracy and see
how we're doing with respect to gpt2 okay good morning so focusing For a
SECTION 4: results in the morning! GPT-2, GPT-3 repro
Moment On The jupyter Notebook here on the right I created a new cell that basically allows us to visualize the the
train Val and Hela and um the hel score and you can step through this it
basically like parses the log file that we are writing and um a lot of this is just like boring ma plot lip code but
basically this is what our optimization looks like so we ran for
19,731 billion tokens which is whoops oh my gosh which is one Epoch of the sample
10B of webd on the left we have the loss and the in blue we have the training
loss in Orange we have the validation loss and red as a horizontal line we
have the opening IG gpt2 124 M model checkpoint when it's just evaluated on
the validation set of um of this fine web edu uh so you can see that we are
surpassing this orange is below the red so we're surpassing the validation set of this data set and like I mentioned
the data set distribution is very different from what gpt2 trained on so this is not an exactly fair comparison
but it's a good cross check uh to uh to look at now we would ideally like
something that is withheld and comparable and somewhat standard um and
so for us that is helis swag and so on here we see the H swag progress we made from 25% all the way here in red we see
the open gpt2 124 M model in red so it
achieves this h bag here and the the gpt3 model 124 M which was trained on
300 billion tokens achieves green so that's over here so you see that we
basically surpassed the gbt2 24m uh model right here uh which is uh really
nice now interestingly we were able to do so with only training on 10 billion tokens while gpt2 was trained on 100
billion tokens so uh for some reason we were able to get away with significantly fewer tokens for training there are many
possibilities to as to why we could match or surpass this accuracy um with
only 10 million training so number one um it could be that opening gbt2 was
trained on a much wider data distribution so in particular fine web edu is all English it's not multilingual
and there's not that much math and code um and so math and code and multilingual
could have been stealing capacity from the original gpt2 model and um basically
that could be partially the reason why uh this is not working out there's many other reasons um so for example the H
swag eval is fairly old uh maybe 5 years or so it is possible that aspects of H
swag in some way or even identically have made it into the training Set uh of fine web we don't know for sure but if
that was the case then we are basically looking at the training curve instead of the validation curve so long story short this is not a perfect eval and there's
some caveats here uh but at least we have some confidence that that we're not doing something completely wrong and
um and uh it's probably the case that when people try to create these data sets they try to make sure that test
sets that are very common are not part of the training set for example uh when hugging face created the fine web BDU
they use H swag as an eval so I would hope that they make sure that they D duplicate and that there's no hella swag
in the training set but we can't be sure uh the other thing I wanted to address briefly is look at this loss curve this
looks really this looks really wrong here I don't actually know 100% what this is and I suspect it's because the
uh 10 billion sample of fine web edu was not properly shuffled um and there's
some issue here uh with the data that I don't fully understand yet and there's some weird periodicity to it um and
because we are in a very lazy way sort of serializing all the tokens and just iterating all them from scratch without
doing any permutation or any random sampling ourselves I think we're inheriting some of the ordering that
they have in the data set so uh this is not ideal but hopefully by the time you
get to this repo uh some of these things by the way will hopefully be fixed and I
will release this build n GPT repo and right now it looks a little ugly and
preliminary uh so hopefully by the time you get here it's nicer but down here I'm going to show aada and I'm going to
talk about about some of the things that happened after the video and I expect that we will have fixed uh the small
issue uh but for now basically this shows that uh our training is not uh completely wrong and it shows that uh
we're able to surpass the accuracy with only 10x the token budget um and
possibly it could be also that the data set may have improved so uh the original
uh gpt2 data set was web text it's possible that not a lot of care and attention went into the data set this
was very early in llms whereas now there's a lot more scrutiny on good practices around uh D duplication
filtering uh quality filtering and so on and it's possible that the data that we're training on is just of higher quality per token and that could be
giving us a boost as well so a number of cave has to think about but for now uh we're pretty happy with this um and yeah
now the next thing I was interested in is as you see it's a morning now so there was an overnight and I wanted to
basically see how far I could push the result so uh to do an overnight run I
basically did instead of one Epoch which took roughly two hours I just did a times four so that that would take eight
hours while I was sleeping and so we did four Epoch or roughly 40 billion uh tokens of training and I was trying to
see how far we could get um and so this was the only change and I reran the script and when I point uh and read the
log file at uh at the 40b uh this is what the curve look like okay so to narrate this number one
we are seeing this issue here here with the periodicity through the different Epoch and something really weird with
the fine web edu data set and that is to be determined uh but otherwise we are
seeing that the H swag actually went up by a lot and we almost we almost made it
uh to the GPT 324m accuracy uh up here uh but not quite so uh it's too bad that
I didn't sleep slightly longer um and uh I think if this was an uh five Epoch run
we may have gotten here now one thing to point out is that if you're doing multi Epoch runs uh we're not actually being
very careful in our data loader and we're not um I this data loader goes
through the data in exactly the same format and exactly the same order and
this is kind of suboptimal and you would want to look into extensions where you actually permute the data uh randomly
you permute the documents around in Every Single Shard on every single new Epoch um and po even permute the
shards and that would go a long way into decreasing the pricity and it's also better for the optimization so that
you're not seeing things ident in the identical format and you're introducing some of the some uh Randomness in how
the documents follow each other because you have to remember that in every single row these documents follow each
other and then there's the end of text token and then the next document so the documents are currently glued together
in the exact same identical manner but we actually want to break break up the documents and shuffle them around
because the order of the documents shouldn't matter and they shouldn't um basically we want to break up that
dependence because it's a kind of a spous correlation and so our data lad is not currently doing that and that's one
Improvement uh you could think of making um the other thing to point out
is we're almost matching gpt3 accuracy with only 40 billion tokens gpt3 trained on 300 billion tokens so again we're
seeing about a 10x um Improvement here with respect to learning efficiency uh
the other thing I wanted to and I don't actually know exactly what to attribute this to other than some of the things
that I already mentioned previously for the previous run uh the other thing I wanted to briefly mention is uh the max
LR here I saw some people already play with this a little bit in a previous related repository um and it turns out
that you can actually almost like three xas so it's possible that the maximum learning rate can be a lot higher and
for some reason the gpt3 hyper parameters that we are inheriting are actually extremely conservative and you can actually get away with a Higher
Learning rate and it would train faster so a lot of these hyper parameters um are quite tunable and feel free to play
with them and they're probably not set precisely correctly and um it's possible
that you can get away with doing this basically and if you wanted to exactly be faithful to gpt3 you would also want
to make the following difference you'd want to come here and the sequence length of gpt3 is 2x it's 20 48 instead
of 1,24 so you would come here change this to 248 for T and then if you want
the exact same number of tokens uh half a million per iteration or per step you
want to then decrease this to 32 so they still multiply to half a mil so that
would give your model sequence length equal to that of gpt3 and in that case basically the
um the models would be roughly identical as far as I'm as far as I'm aware
because again gpt2 and gpt3 are very very similar models now we can also look at some of the samples here from the
model that was trained overnight so this is the optimization and you see that here
we stepped all the way to 76290 also or so and these are the hos
mag we achieved was 33.2 4 and these are some of the samples from the model and
you can see that if you read through this and pause the video briefly you can see that they are a lot more coherent uh
so um and they're actually addressing the fact that it's a language model almost
so uh hello I'm a language model and I try to be as accurate as
possible um I'm a language model not a programming language I know how to communicate uh I
use Python um I don't know if you pause this and
look at it and then compare it to the one to the model that was only trained for 10 billion uh you will see that
these are a lot more coherent and you can play with this uh yourself one more thing I added to The Code by the way is this chunk of code
here so basically right after we evaluate the validation loss if we are the master process in addition to
logging the validation loss every 5,000 steps we're also going to save the checkpoint which is really just the
state dictionary of the model and so checkpointing is nice just because uh you can save the model and later you can
uh use it in some way if you wanted to resume the optimiz ation then in addition to saving the model we have to
also save the optimizer State dict because remember that the optimizer has a few additional buffers because of adom
so it's got the m and V and uh you need to also resume the optimizer properly
you have to be careful with your RNG seeds uh random number generators and so on so if you wanted to exactly be able
to resume optimization you have to think through the state of the of the training process but if you just want to save the
model this is how you would do it and one one nice reason why you might want to do this is because you may want to
evaluate the model a lot more carefully so here we are only kind of like winging
the hell swag eval but you may want to use something um nicer like for example
the Luther uh Luther evaluation hardness evaluation hardness hardness um so this
is a way to also evaluate language models and um so it's possible that um
you may want to use basically different infrastructure to more thoroughly evaluate the models on different um
evaluations and compare it to the opening gbt2 model on many other um tasks like for example that involve math
code or different languages and so on so this is a nice functionality to have as well
um and then the other thing I wanted to mention is that everything we've built here this is only the pre-training step
so um the GPT here is a it dreams documents it just predicts the next to
you can't talk to it like you can talk to chat GPT uh chat GPT if you wanted to talk to the model we have to fine-tune
it into the chat format and it's not actually like that complicated if you're looking at supervised fine-tuning or sft
really what that means is we're just swapping out a data set into a data set that is a lot more conversational and there's a user assistant user assistant
kind of structure and we just fine-tune on it and then we um we basically fill in the user tokens and we sample the
assistant tokens it's not a lot more deeper than that uh but basically we swap out the data set and continue
training uh but for now we're going to stop at uh pre-training one more thing that I wanted to briefly show you is
shoutout to llm.c, equivalent but faster code in raw C/CUDA
that of course what we've built up today was building towards nanog GPT which is this repository from earlier uh but also
there's actually another nanog GPT implementation and it's hiding in a more recent project that I've been working on
called llm Doc and lm. C is a pure Cuda
implementation of gpt2 or gpt3 training and it just directly uses uh Cuda and is
written as Cuda now the nanog gbt here acts as reference code in pytorch to the
C implementation so we're trying to exactly match up the two but we're hoping that the C Cuda is faster and of
course currently that seems to be the case um because it is a direct optimized implementation so train gpt2 Pi in LL
M.C is basically the nanog GPT and when you scroll through this file you'll find
a lot of things that very much look like um things that we've built up in this
lecture and then when you look at train gpt2 docu uh this is the C Cuda
implementation so there's a lot of MPI nickel GPU Cuda
cc++ and you have to be familiar with that but uh um when this is built up we
can actually run the two side by side and they're going to produce the exact same results but lm. C actually runs
faster so let's see that so on the left I have pytorch a nanog GPT looking thing
on the right I have the llmc call and here I'm going to launch the two both of these are going to be
running on a single GPU and here I'm putting the lm. C on GPU 1 and this one will grab uh gpu0 by default and
then we can see here that lm. c compiled and then allocate space and
it's stepping so basically uh meanwhile P torch is still
compiling because torch compile is a bit slower here than the lm. C nbcc Cuda
compile and so this program has already started running and uh we're still waiting here for torch compile now of
course uh this is a very specific implementation to gpt2 and 3 a pytorch is a very general neural network
framework so they're not exactly comparable but if you're only interested in training gpt2 and 3 lm. C is very
fast it takes less space it's faster to start and it's faster per
step and so P started to Stepping here and as you can see we're running at about 223,000 tokens per second here and
about 185,000 tokens per second here um
so quite a bit slower but I don't have full confidence that I exactly squeezed
out all the juice from the pytorch implementation but the important thing here is notice that if I Aline up the
steps you will see that the losses and Norms that are printed between these two are
identical so on the left we have the pie torch and on the right this C implementation and they're the same
except this one runs faster uh so that's kind of I wanted to show you also briefly lm. C and this is a parallel
implementation and it's also something that you may want to uh play with or look at and um it's kind of interesting
summary, phew, build-nanogpt github repo
okay so at this point I should probably start wrapping up the video because I think it's getting way longer than I anticipated uh but we did Cover a lot of
ground and we built everything from scratch so as a brief summary we were looking at the gpt2 and GPT 3
papers we were looking at how you set up these training runs uh and all the considerations involved we wrote
everything from scratch and then we saw that over the duration of either a 2-hour training run or an overnight run
we can actually match the 124 million parameter checkpoints of gbt2 and gpt3
uh to a very large extent um in principle the code that we wrote would be able to train even bigger
models if you have the patients or the Computing resources uh and so you could potentially think about training some of
the bigger checkpoints as well um there are a few remaining issues to address
what's happening with the loss here which I suspect has to do with the fine web edu data sampling uh why can't we
turn on Torch compile uh it currently breaks generation and H swag what's up with that in the data loader we should
probably be permuting our data when we reach boundaries so there's a few more issues like that and I expect to be
documenting some of those over time in the uh build n GPT repository here which
I'm going to be releasing with this video if you have any questions or like to talk about anything that we covered
please go to discussions tab uh so we can talk here uh or please go to issues or pull request pull requests um
depending on what you'd like to contribute or also have a look at the uh Zero to Hero Discord and uh I'm going to
be hanging out here on N GPT um otherwise for now I'm pretty happy
about where we got um and I hope you enjoyed the video and I will see you later

